{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import io, color\n",
    "from skimage import exposure\n",
    "from time import time\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import metrics\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),  (x_test, y_test) = mnist.load_data()\n",
    "img_rows, img_cols = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape para meter la dimensión del canal\n",
    "x_train_ = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test_ = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "                           \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "# normalizamos\n",
    "x_train_ = x_train_.astype('float32')/255\n",
    "x_test_ = x_test_.astype('float32')/255\n",
    "\n",
    "nb_classes = 10\n",
    "y_train_ = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test_ = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                200736    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 201,386\n",
      "Trainable params: 201,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# limpiar sesion (higiene en nombres)\n",
    "backend.clear_session()\n",
    "\n",
    "nb_filters = 32 # queremos que aprenda 32 filtros distintos\n",
    "kernel_size = (3,3)\n",
    "pool_size=(2,2)\n",
    "nb_classes = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = nb_filters, input_shape = input_shape, kernel_size = kernel_size, padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense( 10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 16s 338us/step - loss: 0.5329 - categorical_accuracy: 0.8554 - val_loss: 0.2894 - val_categorical_accuracy: 0.9157\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 17s 353us/step - loss: 0.2741 - categorical_accuracy: 0.9185 - val_loss: 0.2307 - val_categorical_accuracy: 0.9323\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 17s 355us/step - loss: 0.2280 - categorical_accuracy: 0.9323 - val_loss: 0.2055 - val_categorical_accuracy: 0.9407\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 17s 345us/step - loss: 0.1966 - categorical_accuracy: 0.9416 - val_loss: 0.1852 - val_categorical_accuracy: 0.9451\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 17s 351us/step - loss: 0.1736 - categorical_accuracy: 0.9483 - val_loss: 0.1638 - val_categorical_accuracy: 0.9518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1216bf28>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir = 'logs/',\n",
    "    histogram_freq = 1\n",
    ")\n",
    "\n",
    "model.fit(x_train_, y_train_, epochs=5, verbose=1, callbacks=[tensorboard], validation_split=.2)\n",
    "# callback para que visualice en tensor boaard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con la red entrenando invocamos tensorboard:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir = ./logs\n",
    "```\n",
    "\n",
    "Que nos devuelve la url del visor (localhost:6006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otra arquitectura con API funcional\n",
    "\n",
    "Vamos a aplicar convoluciones con kernels de distinto tamaño en paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_33 (Conv2D)                (None, 28, 28, 32)   320         input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_55 (Conv2D)                (None, 28, 28, 32)   832         input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_77 (Conv2D)                (None, 28, 28, 32)   1600        input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mxpool_33 (MaxPooling2D)        (None, 14, 14, 32)   0           conv_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mxpool_55 (MaxPooling2D)        (None, 14, 14, 32)   0           conv_55[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mxpool_77 (MaxPooling2D)        (None, 14, 14, 32)   0           conv_77[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat (Add)                    (None, 14, 14, 32)   0           mxpool_33[0][0]                  \n",
      "                                                                 mxpool_55[0][0]                  \n",
      "                                                                 mxpool_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 6272)         0           concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 32)           200736      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 10)           330         dense1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 203,818\n",
      "Trainable params: 203,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "backend.clear_session()\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir = 'logs/',\n",
    "    histogram_freq = 1\n",
    ")\n",
    "\n",
    "#input layer\n",
    "input_layer = layers.Input(shape = input_shape, dtype='float32', name='input_layer')\n",
    "\n",
    "# kernels de  distinto tamaño\n",
    "# 3,3\n",
    "conv_33_inst = layers.Conv2D(filters = nb_filters, input_shape = input_shape, kernel_size = (3,3), padding='same', activation = 'relu', name='conv_33')\n",
    "mxpool_33_inst = layers.MaxPooling2D(pool_size=pool_size, name='mxpool_33')\n",
    "conv_33 = conv_33_inst(input_layer)\n",
    "mxpool_33 = mxpool_33_inst(conv_33)\n",
    "\n",
    "# 5,5\n",
    "conv_55_inst = layers.Conv2D(filters = nb_filters, input_shape = input_shape, kernel_size = (5,5), padding='same', activation = 'relu', name='conv_55')\n",
    "mxpool_55_inst = layers.MaxPooling2D(pool_size=pool_size, name='mxpool_55')\n",
    "conv_55 = conv_55_inst(input_layer)\n",
    "mxpool_55 = mxpool_55_inst(conv_55)\n",
    "\n",
    "# 7,7 A estas alturas ya debería haber hecho un bucle for\n",
    "conv_77_inst = layers.Conv2D(filters = nb_filters, input_shape = input_shape, kernel_size = (7,7), padding='same', activation = 'relu', name='conv_77')\n",
    "mxpool_77_inst = layers.MaxPooling2D(pool_size=pool_size, name='mxpool_77')\n",
    "conv_77 = conv_77_inst(input_layer)\n",
    "mxpool_77 = mxpool_77_inst(conv_77)\n",
    "\n",
    "\n",
    "# concatenate \n",
    "#concat_layer = layers.concatenate([mxpool_33, mxpool_55, mxpool_77], name = 'concat')\n",
    "# (pero podríamos sumar)\n",
    "concat_layer = layers.add([mxpool_33, mxpool_55, mxpool_77], name = 'concat')\n",
    "# flatten\n",
    "flatten_layer_inst = layers.Flatten(name='flatten')\n",
    "flatten_layer = flatten_layer_inst(concat_layer)\n",
    "\n",
    "# Densas de salida\n",
    "dense_1_inst = layers.Dense(32,activation='relu', name = 'dense1')\n",
    "dense_1 = dense_1_inst(flatten_layer)\n",
    "\n",
    "out_inst = layers.Dense(10, activation='softmax', name='out')\n",
    "out = out_inst(dense_1)\n",
    "\n",
    "model = models.Model(input_layer, out)\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 55s 1ms/step - loss: 0.4323 - categorical_accuracy: 0.8734 - val_loss: 0.2287 - val_categorical_accuracy: 0.9337\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.2007 - categorical_accuracy: 0.9404 - val_loss: 0.1546 - val_categorical_accuracy: 0.9558\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.1366 - categorical_accuracy: 0.9600 - val_loss: 0.1143 - val_categorical_accuracy: 0.9682\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 55s 1ms/step - loss: 0.1047 - categorical_accuracy: 0.9694 - val_loss: 0.1038 - val_categorical_accuracy: 0.9706\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 56s 1ms/step - loss: 0.0858 - categorical_accuracy: 0.9753 - val_loss: 0.0836 - val_categorical_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2854e0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=[metrics.categorical_accuracy])\n",
    "model.fit(x_train_, y_train_, epochs=5, verbose=1, callbacks=[tensorboard], validation_split=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN's con autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 3,217\n",
      "Trainable params: 3,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# limpiar sesion (higiene en nombres)\n",
    "backend.clear_session()\n",
    "pool_size=(2,2)\n",
    "nb_classes = 10\n",
    "\n",
    "\n",
    "input_img = Input(shape=(28,28,1))\n",
    "x = layers.Conv2D(filters = 16, kernel_size = (3,3), padding='same', activation = 'relu')(input_img)\n",
    "x = layers.MaxPooling2D(pool_size=pool_size)(x)\n",
    "x = layers.Conv2D(filters = 8, kernel_size = (3,3), padding='same', activation = 'relu')(x)\n",
    "encoded = layers.MaxPooling2D(pool_size=pool_size)(x)\n",
    "\n",
    "x = layers.Conv2D(filters=8, kernel_size=(3,3), padding='same', activation='relu')(encoded)\n",
    "x = layers.UpSampling2D(size=(2,2))(x)\n",
    "x = layers.Conv2D(filters = 16, kernel_size = (3,3), padding='same', activation = 'relu')(x)\n",
    "x = layers.UpSampling2D(size=(2,2))(x)\n",
    "\n",
    "decoded = layers.Conv2D(filters=1, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer = 'adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected conv2d_5 to have 4 dimensions, but got array with shape (60000, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5f9d943b9791>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dl_win\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dl_win\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dl_win\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected conv2d_5 to have 4 dimensions, but got array with shape (60000, 10)"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(x_train_, y_train_, epochs=5, validation_split=.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text and CNN's\n",
    "\n",
    "Dataset : https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepath_dict = {\n",
    "    'yelp':'../data/sentiment labelled sentences/yelp_labelled.txt',\n",
    "    'amazon':'../data/sentiment labelled sentences/amazon_cells_labelled.txt',\n",
    "    'imdb':'../data/sentiment labelled sentences/imdb_labelled.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>yelp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label source\n",
       "0                           Wow... Loved this place.      1   yelp\n",
       "1                                 Crust is not good.      0   yelp\n",
       "2          Not tasty and the texture was just nasty.      0   yelp\n",
       "3  Stopped by during the late May bank holiday of...      1   yelp\n",
       "4  The selection on the menu was great and so wer...      1   yelp"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_list = []\n",
    "for source, filepath in filepath_dict.items():\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n",
    "    df['source'] = source  # Add another column filled with the source name\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df.head()\n",
    "sentences_train = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a mapear cada palabra a  un número, de forma que cada frase es un un vector. La dimensión del vector viene dado por la frase más larga, de modo que  se hace un padding para rellenar las dimensiones que faltan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5a46ddfd9082>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m \u001b[1;31m# mapeo de palabras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences_train' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer # mapeo de palabras\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "x_train = tokenizer.text_to_sequences(sequences_train)\n",
    "x_test = tokenizer.text_to_sequences(sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_win)",
   "language": "python",
   "name": "dl_win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
