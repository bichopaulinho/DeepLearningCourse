{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conventional neural networks are not well designed to model the uncertainty associated with the predictions they make. For that, one way is to go full Bayesian. \n",
    "\n",
    "In this class we will introduce the concept of Bayesian Neural Networks (BNN) by studing a standard supervised use case: a toy regression problem. For that, we will first implement linear regression and learn point estimates for the parameters w and b. Then we’ll see how to incorporate uncertainty into our estimates by using MCMC to implement Bayesian linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian neural network is a neural network with a prior distribution on its weights. This means that, in contrast with any convential (non-Bayesian) neural network, BNNs are interested not only in the optimal values of the network's parameters -- weights and biases -- but also in the distribution associated with them. Thanks to these distributions we could have a certain level of confidence about the network predictions.\n",
    "\n",
    "The  idea  behind  Bayesian  neural  networks is then to cast the task of training a network as a problem of inference, which is solved using Bayes’ theorem. The latter theorem is used to assign a probability density to each point in the parameter space of the neural network, as it is featured below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Formula\n",
    "\n",
    "Estimating the distributions associated with the network parameters is hard. These are generally referred to as posterior densities, and are estimated using the Bayes rule.\n",
    "\n",
    "$$p \\left(w \\mid x,y \\right) = \\frac{p \\left( x,y \\mid w \\right) p(w)}{\\int p \\left( x, y \\mid w\\right) p(w)dw}$$\n",
    "\n",
    "The main problem lies in the denominator — also known as model evidence. It requires integrating over all possible values of the parameters (i.e., all weights and biases space), and it is often not doable in practice.\n",
    "\n",
    "Instead, pseudo-numerical approaches can be chosen where the solution to those integrals is approximated. The most common approaches are: \n",
    "\n",
    "1.- Approximating the integral with MCMC\n",
    "\n",
    "2.- Using variational inference \n",
    "\n",
    "3.-Using MC dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook we will explore the MCMC approach with the help of the tensorflow_probability library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's dive deep into the basic principles with the regression example.\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning.\n",
    "\n",
    "Considering $\\mathcal{D} = \\left( \\boldsymbol{X},\\boldsymbol{Y} \\right) = \\left\\{ (\\boldsymbol{x}_i,y_i) \\right\\}_{i=1}^{N}$. Suppose there exist $f(x)$ so that \n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "We want to find $f(x) = \\phi_w (x)$ where $w$ are the parameters of the later function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first implement linear regression and learn point estimates for the parameters w and b. Then we’ll see how to incorporate uncertainty into our estimates to implement Bayesian linear regression.\n",
    "\n",
    "But, first of all, let’s import the modules we’ll need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a toy dataset of 100 data points with one feature and w=3.5 and b=-9.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "points = 100  # number of data points\n",
    "low = -1.\n",
    "high = 1.\n",
    "X_train = np.random.uniform(low=low,high=high,size=(points))\n",
    "\n",
    "def f(x):\n",
    "    a_0 = -9.3\n",
    "    a_1 = 3.5\n",
    "    return a_0+x*a_1\n",
    "\n",
    "y_train = f(X_train)\n",
    "\n",
    "plt.plot(X_train,y_train,'o',\n",
    "         c='#072146',label='real generated data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add some constant noise in the predicted dimension:\n",
    "\n",
    "$$y = f(x) + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "std = 0.4\n",
    "epsilon = np.random.normal(0,std,size=(points))\n",
    "\n",
    "y_train = f(X_train)+epsilon\n",
    "\n",
    "plt.plot(X_train,y_train,'o',\n",
    "         c='#072146',label='real generated data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the problem state as $$y = \\phi_w (x) + \\epsilon$$\n",
    "\n",
    "$$ p(y \\mid x, w, \\epsilon) = \\mathcal{N}(y \\mid \\phi_w(x), \\epsilon^2)$$\n",
    "\n",
    "where $\\phi_w (x) = w_1 x + w_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(y_{i}\\mid x_{i},w, \\epsilon)=\\frac{1}{\\sqrt{2\\pi\\epsilon^{2}}}\\exp\\left(-\\frac{\\left(y_{i}-\\phi_{w}(x_{i})\\right)^{2}}{2\\epsilon^{2}}\\right)$$\n",
    "\n",
    "$$\\Rightarrow-\\ln\\left(p(y_{i}\\mid x_{i},w)\\right)\\Big|_{\\{w,\\epsilon\\}}=-\\ln(\\epsilon)-\\frac{\\left(y_{i}-\\phi_{w}(x_{i})\\right)^{2}}{2\\epsilon^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define using keras (K) the second term of the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_normal_pdf(y, output_psi, epsilon):\n",
    "    return -K.log(epsilon)-(K.square(y-output_psi)/(2.*K.square(epsilon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particularize the above function with the real y and the parameters' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression(y_true, parameters):\n",
    "    mu = parameters[:,:-1]\n",
    "    sigma = parameters[:,-1:]\n",
    "    return -K.sum(log_normal_pdf(y_true[:,:1],\n",
    "                                 mu,sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom keras' layer to deal with scalars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Scalar(Layer):\n",
    "    def __init__(self, units = 1, initializer=K.zeros, \n",
    "                 activation = None, **kwargs):\n",
    "        super(Scalar, self).__init__(**kwargs)\n",
    "        self.scalar_initializer = initializer\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        \n",
    "    def build(self, bs_input):\n",
    "        self.scalar = self.add_weight(\"sigma_hom\", \n",
    "                                      shape=[self.units],  \n",
    "                                      initializer=self.scalar_initializer)\n",
    "        self.params = [self.scalar]\n",
    "    \n",
    "    def get_output(self, train=False):\n",
    "        output = self.scalar*K.ones_like(self.get_input()[:,:1])\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def call(self, bs_input):\n",
    "        output = self.scalar*K.ones_like(bs_input[:,:1])\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a function which retuns the ELU value of x + 1 given x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import ELU\n",
    "\n",
    "def elu_plus1(x, a=1.):\n",
    "    return ELU(alpha=a)(x)+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = Input(name='input', shape=(1,), dtype='float32')\n",
    "\n",
    "phi = Dense(units=1, activation=\"linear\", name='w',\n",
    "              kernel_initializer='ones')(i)\n",
    "\n",
    "epsilon = Scalar(activation=elu_plus1)(i)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[i],\n",
    "    outputs=[concatenate([phi,epsilon],  \n",
    "                         axis=1, \n",
    "                         name='main_output')])\n",
    "\n",
    "opt = Adam(lr=0.1)\n",
    "model.compile(optimizer=opt, loss={'main_output':regression})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1119.7524 - val_loss: 233.2484\n",
      "Epoch 2/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 664.4039 - val_loss: 153.7306\n",
      "Epoch 3/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 446.5157 - val_loss: 112.6658\n",
      "Epoch 4/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 337.6080 - val_loss: 88.6804\n",
      "Epoch 5/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 269.2354 - val_loss: 73.5421\n",
      "Epoch 6/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 225.5292 - val_loss: 63.3354\n",
      "Epoch 7/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 195.0624 - val_loss: 56.1400\n",
      "Epoch 8/1000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 174.2650 - val_loss: 50.8321\n",
      "Epoch 9/1000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 158.6245 - val_loss: 46.7819\n",
      "Epoch 10/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 146.5927 - val_loss: 43.5953\n",
      "Epoch 11/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 136.7990 - val_loss: 41.0263\n",
      "Epoch 12/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 129.4897 - val_loss: 38.9015\n",
      "Epoch 13/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 122.5065 - val_loss: 37.1153\n",
      "Epoch 14/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 117.1252 - val_loss: 35.5817\n",
      "Epoch 15/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 112.2356 - val_loss: 34.2407\n",
      "Epoch 16/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 108.3443 - val_loss: 33.0571\n",
      "Epoch 17/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 104.6263 - val_loss: 31.9972\n",
      "Epoch 18/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 101.3729 - val_loss: 31.0369\n",
      "Epoch 19/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 98.1316 - val_loss: 30.1613\n",
      "Epoch 20/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 95.3471 - val_loss: 29.3541\n",
      "Epoch 21/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 92.8423 - val_loss: 28.6054\n",
      "Epoch 22/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 90.5444 - val_loss: 27.9050\n",
      "Epoch 23/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 88.3550 - val_loss: 27.2508\n",
      "Epoch 24/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 86.2552 - val_loss: 26.6350\n",
      "Epoch 25/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 84.2041 - val_loss: 26.0541\n",
      "Epoch 26/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 82.4452 - val_loss: 25.5033\n",
      "Epoch 27/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 80.5954 - val_loss: 24.9825\n",
      "Epoch 28/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 78.9729 - val_loss: 24.4871\n",
      "Epoch 29/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 77.4922 - val_loss: 24.0146\n",
      "Epoch 30/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 76.0328 - val_loss: 23.5637\n",
      "Epoch 31/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 74.4803 - val_loss: 23.1334\n",
      "Epoch 32/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 73.2550 - val_loss: 22.7229\n",
      "Epoch 33/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 71.8585 - val_loss: 22.3293\n",
      "Epoch 34/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 70.6159 - val_loss: 21.9526\n",
      "Epoch 35/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 69.4413 - val_loss: 21.5915\n",
      "Epoch 36/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 68.1901 - val_loss: 21.2451\n",
      "Epoch 37/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 67.1019 - val_loss: 20.9125\n",
      "Epoch 38/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 66.0516 - val_loss: 20.5931\n",
      "Epoch 39/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 65.0016 - val_loss: 20.2859\n",
      "Epoch 40/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 64.1313 - val_loss: 19.9893\n",
      "Epoch 41/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 62.9880 - val_loss: 19.7045\n",
      "Epoch 42/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 62.2668 - val_loss: 19.4302\n",
      "Epoch 43/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 61.2703 - val_loss: 19.1653\n",
      "Epoch 44/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 60.4123 - val_loss: 18.9103\n",
      "Epoch 45/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 59.6589 - val_loss: 18.6643\n",
      "Epoch 46/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 58.8857 - val_loss: 18.4263\n",
      "Epoch 47/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 58.0697 - val_loss: 18.1971\n",
      "Epoch 48/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 57.2941 - val_loss: 17.9759\n",
      "Epoch 49/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 56.6125 - val_loss: 17.7618\n",
      "Epoch 50/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 55.9012 - val_loss: 17.5548\n",
      "Epoch 51/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 55.3552 - val_loss: 17.3543\n",
      "Epoch 52/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 54.6569 - val_loss: 17.1604\n",
      "Epoch 53/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 54.0543 - val_loss: 16.9724\n",
      "Epoch 54/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 53.3251 - val_loss: 16.7911\n",
      "Epoch 55/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 52.7881 - val_loss: 16.6148\n",
      "Epoch 56/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 52.2350 - val_loss: 16.4439\n",
      "Epoch 57/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 51.7271 - val_loss: 16.2783\n",
      "Epoch 58/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 51.1891 - val_loss: 16.1175\n",
      "Epoch 59/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 50.6432 - val_loss: 15.9619\n",
      "Epoch 60/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 50.1633 - val_loss: 15.8103\n",
      "Epoch 61/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 49.6491 - val_loss: 15.6630\n",
      "Epoch 62/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 49.1368 - val_loss: 15.5205\n",
      "Epoch 63/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 48.6442 - val_loss: 15.3819\n",
      "Epoch 64/1000\n",
      "90/90 [==============================] - 0s 161us/step - loss: 48.2494 - val_loss: 15.2472\n",
      "Epoch 65/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 47.7991 - val_loss: 15.1160\n",
      "Epoch 66/1000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 47.3551 - val_loss: 14.9882\n",
      "Epoch 67/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 46.9134 - val_loss: 14.8641\n",
      "Epoch 68/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 46.5167 - val_loss: 14.7435\n",
      "Epoch 69/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 46.1399 - val_loss: 14.6258\n",
      "Epoch 70/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 45.7282 - val_loss: 14.5110\n",
      "Epoch 71/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 45.3781 - val_loss: 14.3990\n",
      "Epoch 72/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 44.9982 - val_loss: 14.2903\n",
      "Epoch 73/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 44.6728 - val_loss: 14.1841\n",
      "Epoch 74/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 44.3605 - val_loss: 14.0802\n",
      "Epoch 75/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 44.0288 - val_loss: 13.9792\n",
      "Epoch 76/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 43.6846 - val_loss: 13.8804\n",
      "Epoch 77/1000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 43.3040 - val_loss: 13.7841\n",
      "Epoch 78/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 42.9750 - val_loss: 13.6899\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 91us/step - loss: 42.6929 - val_loss: 13.5977\n",
      "Epoch 80/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 42.3952 - val_loss: 13.5074\n",
      "Epoch 81/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 42.0564 - val_loss: 13.4195\n",
      "Epoch 82/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 41.7841 - val_loss: 13.3334\n",
      "Epoch 83/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 41.5385 - val_loss: 13.2493\n",
      "Epoch 84/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 41.2546 - val_loss: 13.1669\n",
      "Epoch 85/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 40.9440 - val_loss: 13.0860\n",
      "Epoch 86/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 40.7240 - val_loss: 13.0067\n",
      "Epoch 87/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 40.4056 - val_loss: 12.9288\n",
      "Epoch 88/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 40.1826 - val_loss: 12.8523\n",
      "Epoch 89/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 39.9018 - val_loss: 12.7774\n",
      "Epoch 90/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 39.6742 - val_loss: 12.7039\n",
      "Epoch 91/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 39.4108 - val_loss: 12.6319\n",
      "Epoch 92/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 39.1633 - val_loss: 12.5610\n",
      "Epoch 93/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 38.9766 - val_loss: 12.4911\n",
      "Epoch 94/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 38.7222 - val_loss: 12.4230\n",
      "Epoch 95/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 38.5092 - val_loss: 12.3552\n",
      "Epoch 96/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 38.2762 - val_loss: 12.2887\n",
      "Epoch 97/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 38.0406 - val_loss: 12.2230\n",
      "Epoch 98/1000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 37.8325 - val_loss: 12.1584\n",
      "Epoch 99/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 37.6222 - val_loss: 12.0943\n",
      "Epoch 100/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 37.4141 - val_loss: 12.0312\n",
      "Epoch 101/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 37.1816 - val_loss: 11.9693\n",
      "Epoch 102/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 36.9534 - val_loss: 11.9075\n",
      "Epoch 103/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 36.7930 - val_loss: 11.8464\n",
      "Epoch 104/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 36.5728 - val_loss: 11.7861\n",
      "Epoch 105/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 36.3659 - val_loss: 11.7263\n",
      "Epoch 106/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 36.1695 - val_loss: 11.6669\n",
      "Epoch 107/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 35.9650 - val_loss: 11.6081\n",
      "Epoch 108/1000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 35.7844 - val_loss: 11.5499\n",
      "Epoch 109/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 35.5720 - val_loss: 11.4921\n",
      "Epoch 110/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 35.3993 - val_loss: 11.4345\n",
      "Epoch 111/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 35.1803 - val_loss: 11.3771\n",
      "Epoch 112/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 34.9918 - val_loss: 11.3201\n",
      "Epoch 113/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 34.8109 - val_loss: 11.2633\n",
      "Epoch 114/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 34.6286 - val_loss: 11.2068\n",
      "Epoch 115/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 34.4281 - val_loss: 11.1504\n",
      "Epoch 116/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 34.2405 - val_loss: 11.0938\n",
      "Epoch 117/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 34.0623 - val_loss: 11.0375\n",
      "Epoch 118/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 33.8877 - val_loss: 10.9812\n",
      "Epoch 119/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 33.6745 - val_loss: 10.9248\n",
      "Epoch 120/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 33.4883 - val_loss: 10.8682\n",
      "Epoch 121/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 33.3096 - val_loss: 10.8118\n",
      "Epoch 122/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 33.1240 - val_loss: 10.7547\n",
      "Epoch 123/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 32.9510 - val_loss: 10.6978\n",
      "Epoch 124/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 32.7584 - val_loss: 10.6404\n",
      "Epoch 125/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 32.5624 - val_loss: 10.5830\n",
      "Epoch 126/1000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 32.3885 - val_loss: 10.5253\n",
      "Epoch 127/1000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 32.1883 - val_loss: 10.4673\n",
      "Epoch 128/1000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 32.0032 - val_loss: 10.4089\n",
      "Epoch 129/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 31.8165 - val_loss: 10.3499\n",
      "Epoch 130/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 31.6143 - val_loss: 10.2904\n",
      "Epoch 131/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 31.4211 - val_loss: 10.2304\n",
      "Epoch 132/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 31.2248 - val_loss: 10.1696\n",
      "Epoch 133/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 31.0285 - val_loss: 10.1085\n",
      "Epoch 134/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 30.8409 - val_loss: 10.0466\n",
      "Epoch 135/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 30.6269 - val_loss: 9.9840\n",
      "Epoch 136/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 30.4294 - val_loss: 9.9207\n",
      "Epoch 137/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 30.2267 - val_loss: 9.8565\n",
      "Epoch 138/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 30.0228 - val_loss: 9.7915\n",
      "Epoch 139/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 29.8181 - val_loss: 9.7258\n",
      "Epoch 140/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 29.6014 - val_loss: 9.6591\n",
      "Epoch 141/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 29.3830 - val_loss: 9.5912\n",
      "Epoch 142/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 29.1627 - val_loss: 9.5226\n",
      "Epoch 143/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 28.9478 - val_loss: 9.4528\n",
      "Epoch 144/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 28.7346 - val_loss: 9.3819\n",
      "Epoch 145/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 28.5125 - val_loss: 9.3098\n",
      "Epoch 146/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 28.2757 - val_loss: 9.2366\n",
      "Epoch 147/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 28.0409 - val_loss: 9.1619\n",
      "Epoch 148/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 27.8094 - val_loss: 9.0859\n",
      "Epoch 149/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 27.5723 - val_loss: 9.0083\n",
      "Epoch 150/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 27.3217 - val_loss: 8.9295\n",
      "Epoch 151/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 27.0728 - val_loss: 8.8490\n",
      "Epoch 152/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 26.8135 - val_loss: 8.7669\n",
      "Epoch 153/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 26.5663 - val_loss: 8.6836\n",
      "Epoch 154/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 26.3010 - val_loss: 8.5982\n",
      "Epoch 155/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 26.0328 - val_loss: 8.5108\n",
      "Epoch 156/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 25.7602 - val_loss: 8.4219\n",
      "Epoch 157/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 25.4809 - val_loss: 8.3314\n",
      "Epoch 158/1000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 25.2080 - val_loss: 8.2383\n",
      "Epoch 159/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 24.9146 - val_loss: 8.1431\n",
      "Epoch 160/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 24.6180 - val_loss: 8.0458\n",
      "Epoch 161/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 24.3084 - val_loss: 7.9460\n",
      "Epoch 162/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 24.0034 - val_loss: 7.8445\n",
      "Epoch 163/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 23.6825 - val_loss: 7.7402\n",
      "Epoch 164/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 23.3579 - val_loss: 7.6336\n",
      "Epoch 165/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 23.0311 - val_loss: 7.5241\n",
      "Epoch 166/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 22.6703 - val_loss: 7.4114\n",
      "Epoch 167/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 22.3277 - val_loss: 7.2958\n",
      "Epoch 168/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 21.9833 - val_loss: 7.1770\n",
      "Epoch 169/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 21.6262 - val_loss: 7.0550\n",
      "Epoch 170/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 21.2376 - val_loss: 6.9291\n",
      "Epoch 171/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 20.8497 - val_loss: 6.8001\n",
      "Epoch 172/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 20.4540 - val_loss: 6.6689\n",
      "Epoch 173/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 20.0489 - val_loss: 6.5325\n",
      "Epoch 174/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 19.6178 - val_loss: 6.3914\n",
      "Epoch 175/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 19.1932 - val_loss: 6.2466\n",
      "Epoch 176/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 18.7324 - val_loss: 6.0968\n",
      "Epoch 177/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 18.2916 - val_loss: 5.9417\n",
      "Epoch 178/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 17.8074 - val_loss: 5.7815\n",
      "Epoch 179/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 17.3015 - val_loss: 5.6158\n",
      "Epoch 180/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.8049 - val_loss: 5.4457\n",
      "Epoch 181/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 16.2782 - val_loss: 5.2686\n",
      "Epoch 182/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.7323 - val_loss: 5.0845\n",
      "Epoch 183/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.1720 - val_loss: 4.8945\n",
      "Epoch 184/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 14.5832 - val_loss: 4.6980\n",
      "Epoch 185/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.9784 - val_loss: 4.4922\n",
      "Epoch 186/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 13.3365 - val_loss: 4.2766\n",
      "Epoch 187/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.6758 - val_loss: 4.0491\n",
      "Epoch 188/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.9721 - val_loss: 3.8130\n",
      "Epoch 189/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.2593 - val_loss: 3.5690\n",
      "Epoch 190/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.4952 - val_loss: 3.3113\n",
      "Epoch 191/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6892 - val_loss: 3.0405\n",
      "Epoch 192/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.8831 - val_loss: 2.7588\n",
      "Epoch 193/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.0162 - val_loss: 2.4673\n",
      "Epoch 194/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.0757 - val_loss: 2.1475\n",
      "Epoch 195/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1105 - val_loss: 1.8154\n",
      "Epoch 196/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.0968 - val_loss: 1.4682\n",
      "Epoch 197/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 3.9826 - val_loss: 1.0803\n",
      "Epoch 198/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 2.7972 - val_loss: 0.6792\n",
      "Epoch 199/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.5645 - val_loss: 0.2518\n",
      "Epoch 200/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.2841 - val_loss: -0.1425\n",
      "Epoch 201/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -0.9707 - val_loss: -0.5551\n",
      "Epoch 202/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -2.2170 - val_loss: -0.9386\n",
      "Epoch 203/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -3.4011 - val_loss: -1.3310\n",
      "Epoch 204/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -4.6317 - val_loss: -1.6536\n",
      "Epoch 205/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -5.6790 - val_loss: -1.9889\n",
      "Epoch 206/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -6.7430 - val_loss: -2.3683\n",
      "Epoch 207/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -7.6927 - val_loss: -2.7142\n",
      "Epoch 208/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -8.6759 - val_loss: -2.9687\n",
      "Epoch 209/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -9.5880 - val_loss: -3.1832\n",
      "Epoch 210/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -10.3808 - val_loss: -3.4712\n",
      "Epoch 211/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -11.0167 - val_loss: -3.5578\n",
      "Epoch 212/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -11.6340 - val_loss: -3.9432\n",
      "Epoch 213/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -12.2986 - val_loss: -4.0415\n",
      "Epoch 214/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -12.8164 - val_loss: -3.9648\n",
      "Epoch 215/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -13.1609 - val_loss: -3.9474\n",
      "Epoch 216/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -13.3144 - val_loss: -4.5072\n",
      "Epoch 217/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -13.6626 - val_loss: -4.4154\n",
      "Epoch 218/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -13.7929 - val_loss: -4.3432\n",
      "Epoch 219/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.1812 - val_loss: -4.7319\n",
      "Epoch 220/1000\n",
      "90/90 [==============================] - 0s 125us/step - loss: -14.1097 - val_loss: -4.6849\n",
      "Epoch 221/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.5310 - val_loss: -4.4899\n",
      "Epoch 222/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.2561 - val_loss: -4.2305\n",
      "Epoch 223/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.3276 - val_loss: -4.9017\n",
      "Epoch 224/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.2590 - val_loss: -4.4246\n",
      "Epoch 225/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.7896 - val_loss: -4.3055\n",
      "Epoch 226/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.6496 - val_loss: -4.4921\n",
      "Epoch 227/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.5706 - val_loss: -4.7194\n",
      "Epoch 228/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.5999 - val_loss: -4.8603\n",
      "Epoch 229/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: -14.6413 - val_loss: -4.1635\n",
      "Epoch 230/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.4771 - val_loss: -4.4622\n",
      "Epoch 231/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.8387 - val_loss: -4.7989\n",
      "Epoch 232/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -15.0990 - val_loss: -4.7986\n",
      "Epoch 233/1000\n",
      "90/90 [==============================] - 0s 80us/step - loss: -14.3011 - val_loss: -4.1610\n",
      "Epoch 234/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.2762 - val_loss: -4.1768\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 99us/step - loss: -14.4583 - val_loss: -4.8504\n",
      "Epoch 236/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.6853 - val_loss: -4.7229\n",
      "Epoch 237/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.7711 - val_loss: -4.5344\n",
      "Epoch 238/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.9683 - val_loss: -4.3012\n",
      "Epoch 239/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.8410 - val_loss: -4.2017\n",
      "Epoch 240/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.3043 - val_loss: -4.5108\n",
      "Epoch 241/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.8253 - val_loss: -5.0215\n",
      "Epoch 242/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.2877 - val_loss: -4.4627\n",
      "Epoch 243/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6319 - val_loss: -4.8449\n",
      "Epoch 244/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.7576 - val_loss: -4.2298\n",
      "Epoch 245/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.6211 - val_loss: -4.3117\n",
      "Epoch 246/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.9541 - val_loss: -4.5361\n",
      "Epoch 247/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.9234 - val_loss: -4.6090\n",
      "Epoch 248/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.6884 - val_loss: -4.5750\n",
      "Epoch 249/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.6851 - val_loss: -4.7171\n",
      "Epoch 250/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.8790 - val_loss: -4.5075\n",
      "Epoch 251/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.4669 - val_loss: -4.5773\n",
      "Epoch 252/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.8178 - val_loss: -4.8055\n",
      "Epoch 253/1000\n",
      "90/90 [==============================] - 0s 117us/step - loss: -14.6601 - val_loss: -4.7243\n",
      "Epoch 254/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.7336 - val_loss: -4.3592\n",
      "Epoch 255/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -14.4307 - val_loss: -4.2477\n",
      "Epoch 256/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -15.0381 - val_loss: -4.2569\n",
      "Epoch 257/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.8089 - val_loss: -4.5328\n",
      "Epoch 258/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.6925 - val_loss: -5.0004\n",
      "Epoch 259/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.8502 - val_loss: -4.7040\n",
      "Epoch 260/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.5405 - val_loss: -4.5721\n",
      "Epoch 261/1000\n",
      "90/90 [==============================] - 0s 79us/step - loss: -15.0941 - val_loss: -4.5696\n",
      "Epoch 262/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.6009 - val_loss: -4.3216\n",
      "Epoch 263/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.6079 - val_loss: -4.6071\n",
      "Epoch 264/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.7106 - val_loss: -4.3415\n",
      "Epoch 265/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.7879 - val_loss: -4.4609\n",
      "Epoch 266/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.7867 - val_loss: -4.9946\n",
      "Epoch 267/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.7461 - val_loss: -4.8556\n",
      "Epoch 268/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.4832 - val_loss: -4.7248\n",
      "Epoch 269/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -15.1054 - val_loss: -4.0835\n",
      "Epoch 270/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.5998 - val_loss: -3.8227\n",
      "Epoch 271/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.6352 - val_loss: -4.3359\n",
      "Epoch 272/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.7576 - val_loss: -4.9631\n",
      "Epoch 273/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.5727 - val_loss: -5.3548\n",
      "Epoch 274/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.3305 - val_loss: -4.5920\n",
      "Epoch 275/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.4651 - val_loss: -4.0885\n",
      "Epoch 276/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.6964 - val_loss: -4.2589\n",
      "Epoch 277/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.4495 - val_loss: -4.7753\n",
      "Epoch 278/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.7317 - val_loss: -4.7583\n",
      "Epoch 279/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -15.1557 - val_loss: -4.4414\n",
      "Epoch 280/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.7365 - val_loss: -4.4443\n",
      "Epoch 281/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.6047 - val_loss: -4.7488\n",
      "Epoch 282/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.7665 - val_loss: -4.6661\n",
      "Epoch 283/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.9844 - val_loss: -4.2437\n",
      "Epoch 284/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.5136 - val_loss: -4.2369\n",
      "Epoch 285/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.3053 - val_loss: -4.5231\n",
      "Epoch 286/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.4269 - val_loss: -4.5845\n",
      "Epoch 287/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7166 - val_loss: -4.7686\n",
      "Epoch 288/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.3428 - val_loss: -4.8980\n",
      "Epoch 289/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.3427 - val_loss: -4.0848\n",
      "Epoch 290/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.6986 - val_loss: -4.1905\n",
      "Epoch 291/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6159 - val_loss: -4.8224\n",
      "Epoch 292/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.7438 - val_loss: -4.6778\n",
      "Epoch 293/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -15.1019 - val_loss: -4.7133\n",
      "Epoch 294/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -15.0812 - val_loss: -4.4344\n",
      "Epoch 295/1000\n",
      "90/90 [==============================] - 0s 80us/step - loss: -14.7698 - val_loss: -4.5455\n",
      "Epoch 296/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.9633 - val_loss: -4.4339\n",
      "Epoch 297/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.8619 - val_loss: -4.4284\n",
      "Epoch 298/1000\n",
      "90/90 [==============================] - 0s 133us/step - loss: -14.5497 - val_loss: -4.3592\n",
      "Epoch 299/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.7758 - val_loss: -5.0781\n",
      "Epoch 300/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.9110 - val_loss: -5.0398\n",
      "Epoch 301/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.4530 - val_loss: -4.1360\n",
      "Epoch 302/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.6464 - val_loss: -3.8628\n",
      "Epoch 303/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.8596 - val_loss: -3.9243\n",
      "Epoch 304/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.8116 - val_loss: -4.8058\n",
      "Epoch 305/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.3673 - val_loss: -5.0903\n",
      "Epoch 306/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.6966 - val_loss: -4.7933\n",
      "Epoch 307/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.5740 - val_loss: -4.2519\n",
      "Epoch 308/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.6582 - val_loss: -4.0038\n",
      "Epoch 309/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.2188 - val_loss: -4.4150\n",
      "Epoch 310/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.5117 - val_loss: -5.0051\n",
      "Epoch 311/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.3646 - val_loss: -5.0533\n",
      "Epoch 312/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8845 - val_loss: -4.3144\n",
      "Epoch 313/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.8262 - val_loss: -3.9983\n",
      "Epoch 314/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.5317 - val_loss: -4.3628\n",
      "Epoch 315/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7089 - val_loss: -4.7678\n",
      "Epoch 316/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -15.0052 - val_loss: -4.6659\n",
      "Epoch 317/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.8105 - val_loss: -4.5762\n",
      "Epoch 318/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.7605 - val_loss: -4.2697\n",
      "Epoch 319/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7518 - val_loss: -4.4065\n",
      "Epoch 320/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.6649 - val_loss: -4.9046\n",
      "Epoch 321/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7368 - val_loss: -4.9637\n",
      "Epoch 322/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.7691 - val_loss: -4.5880\n",
      "Epoch 323/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.7778 - val_loss: -4.5024\n",
      "Epoch 324/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.3888 - val_loss: -3.9083\n",
      "Epoch 325/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.4346 - val_loss: -4.8789\n",
      "Epoch 326/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.7621 - val_loss: -4.7005\n",
      "Epoch 327/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6813 - val_loss: -4.5584\n",
      "Epoch 328/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.5928 - val_loss: -4.3367\n",
      "Epoch 329/1000\n",
      "90/90 [==============================] - 0s 129us/step - loss: -14.5875 - val_loss: -4.5784\n",
      "Epoch 330/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.9575 - val_loss: -4.6022\n",
      "Epoch 331/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.8034 - val_loss: -4.5209\n",
      "Epoch 332/1000\n",
      "90/90 [==============================] - 0s 127us/step - loss: -14.6093 - val_loss: -4.6081\n",
      "Epoch 333/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.4025 - val_loss: -5.0235\n",
      "Epoch 334/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.6333 - val_loss: -4.4834\n",
      "Epoch 335/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.6322 - val_loss: -4.2227\n",
      "Epoch 336/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6825 - val_loss: -4.4549\n",
      "Epoch 337/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.7670 - val_loss: -4.5659\n",
      "Epoch 338/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.5693 - val_loss: -4.6059\n",
      "Epoch 339/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.8984 - val_loss: -4.5428\n",
      "Epoch 340/1000\n",
      "90/90 [==============================] - 0s 127us/step - loss: -14.8357 - val_loss: -4.2576\n",
      "Epoch 341/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.6714 - val_loss: -4.4855\n",
      "Epoch 342/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.9014 - val_loss: -4.7138\n",
      "Epoch 343/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.7871 - val_loss: -4.7318\n",
      "Epoch 344/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.3554 - val_loss: -4.0723\n",
      "Epoch 345/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -14.5297 - val_loss: -4.3477\n",
      "Epoch 346/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.5534 - val_loss: -4.8356\n",
      "Epoch 347/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.5716 - val_loss: -5.0904\n",
      "Epoch 348/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.9706 - val_loss: -4.7326\n",
      "Epoch 349/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7346 - val_loss: -4.0862\n",
      "Epoch 350/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.8267 - val_loss: -3.9258\n",
      "Epoch 351/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.6407 - val_loss: -4.2235\n",
      "Epoch 352/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.5831 - val_loss: -4.7564\n",
      "Epoch 353/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.8592 - val_loss: -4.9739\n",
      "Epoch 354/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -14.8040 - val_loss: -4.7506\n",
      "Epoch 355/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.3084 - val_loss: -4.1799\n",
      "Epoch 356/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.1198 - val_loss: -3.8657\n",
      "Epoch 357/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.1829 - val_loss: -4.7797\n",
      "Epoch 358/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.5335 - val_loss: -4.7131\n",
      "Epoch 359/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.5778 - val_loss: -4.5264\n",
      "Epoch 360/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.4003 - val_loss: -4.4982\n",
      "Epoch 361/1000\n",
      "90/90 [==============================] - 0s 77us/step - loss: -14.8195 - val_loss: -4.3284\n",
      "Epoch 362/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7966 - val_loss: -4.5620\n",
      "Epoch 363/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.5737 - val_loss: -4.8122\n",
      "Epoch 364/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -15.1525 - val_loss: -4.6638\n",
      "Epoch 365/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.7236 - val_loss: -4.3377\n",
      "Epoch 366/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.4959 - val_loss: -3.8871\n",
      "Epoch 367/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.5101 - val_loss: -4.7004\n",
      "Epoch 368/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -14.5151 - val_loss: -5.0803\n",
      "Epoch 369/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.6676 - val_loss: -4.6067\n",
      "Epoch 370/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.3188 - val_loss: -4.0761\n",
      "Epoch 371/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.5431 - val_loss: -4.4786\n",
      "Epoch 372/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6571 - val_loss: -4.7029\n",
      "Epoch 373/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.8274 - val_loss: -4.6484\n",
      "Epoch 374/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7953 - val_loss: -4.5300\n",
      "Epoch 375/1000\n",
      "90/90 [==============================] - 0s 76us/step - loss: -14.7195 - val_loss: -4.7202\n",
      "Epoch 376/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: -14.2775 - val_loss: -4.1957\n",
      "Epoch 377/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.4799 - val_loss: -4.2005\n",
      "Epoch 378/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.4962 - val_loss: -5.0719\n",
      "Epoch 379/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.8144 - val_loss: -4.9378\n",
      "Epoch 380/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.5668 - val_loss: -4.4637\n",
      "Epoch 381/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.9099 - val_loss: -4.4650\n",
      "Epoch 382/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6169 - val_loss: -4.4230\n",
      "Epoch 383/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.3092 - val_loss: -4.1163\n",
      "Epoch 384/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6011 - val_loss: -4.1909\n",
      "Epoch 385/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.8381 - val_loss: -4.7774\n",
      "Epoch 386/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.8776 - val_loss: -5.0194\n",
      "Epoch 387/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.8562 - val_loss: -5.1518\n",
      "Epoch 388/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.7265 - val_loss: -4.1031\n",
      "Epoch 389/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 90us/step - loss: -14.4381 - val_loss: -3.5885\n",
      "Epoch 390/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.6412 - val_loss: -3.9676\n",
      "Epoch 391/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.3959 - val_loss: -5.1850\n",
      "Epoch 392/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.3409 - val_loss: -5.2940\n",
      "Epoch 393/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.8311 - val_loss: -4.6209\n",
      "Epoch 394/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.4731 - val_loss: -3.6848\n",
      "Epoch 395/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.2352 - val_loss: -4.0483\n",
      "Epoch 396/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7800 - val_loss: -4.6872\n",
      "Epoch 397/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7717 - val_loss: -4.9768\n",
      "Epoch 398/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.7514 - val_loss: -4.8897\n",
      "Epoch 399/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.7606 - val_loss: -4.4233\n",
      "Epoch 400/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -15.1018 - val_loss: -4.2531\n",
      "Epoch 401/1000\n",
      "90/90 [==============================] - 0s 81us/step - loss: -14.4820 - val_loss: -4.3253\n",
      "Epoch 402/1000\n",
      "90/90 [==============================] - 0s 127us/step - loss: -14.7473 - val_loss: -4.5984\n",
      "Epoch 403/1000\n",
      "90/90 [==============================] - 0s 81us/step - loss: -14.8747 - val_loss: -4.9552\n",
      "Epoch 404/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6533 - val_loss: -4.7917\n",
      "Epoch 405/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.7008 - val_loss: -4.4956\n",
      "Epoch 406/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.8838 - val_loss: -4.1461\n",
      "Epoch 407/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.5856 - val_loss: -4.2584\n",
      "Epoch 408/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.8916 - val_loss: -4.3857\n",
      "Epoch 409/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.5562 - val_loss: -4.6291\n",
      "Epoch 410/1000\n",
      "90/90 [==============================] - 0s 81us/step - loss: -14.6081 - val_loss: -4.8326\n",
      "Epoch 411/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6566 - val_loss: -4.8872\n",
      "Epoch 412/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.6332 - val_loss: -4.7281\n",
      "Epoch 413/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8617 - val_loss: -4.3221\n",
      "Epoch 414/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.3022 - val_loss: -3.6915\n",
      "Epoch 415/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.7324 - val_loss: -4.4596\n",
      "Epoch 416/1000\n",
      "90/90 [==============================] - 0s 80us/step - loss: -14.2214 - val_loss: -5.4004\n",
      "Epoch 417/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.8128 - val_loss: -4.9502\n",
      "Epoch 418/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: -14.6515 - val_loss: -4.1196\n",
      "Epoch 419/1000\n",
      "90/90 [==============================] - 0s 80us/step - loss: -14.8470 - val_loss: -3.6666\n",
      "Epoch 420/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -15.1463 - val_loss: -4.5507\n",
      "Epoch 421/1000\n",
      "90/90 [==============================] - 0s 81us/step - loss: -14.5789 - val_loss: -5.0446\n",
      "Epoch 422/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.3945 - val_loss: -4.8249\n",
      "Epoch 423/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.7523 - val_loss: -4.5173\n",
      "Epoch 424/1000\n",
      "90/90 [==============================] - 0s 76us/step - loss: -14.7165 - val_loss: -4.4644\n",
      "Epoch 425/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.9035 - val_loss: -4.2762\n",
      "Epoch 426/1000\n",
      "90/90 [==============================] - 0s 75us/step - loss: -14.8619 - val_loss: -4.3186\n",
      "Epoch 427/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -15.0164 - val_loss: -4.8521\n",
      "Epoch 428/1000\n",
      "90/90 [==============================] - 0s 123us/step - loss: -14.7904 - val_loss: -4.9011\n",
      "Epoch 429/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.5901 - val_loss: -4.2848\n",
      "Epoch 430/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.6593 - val_loss: -4.1171\n",
      "Epoch 431/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.6539 - val_loss: -4.5745\n",
      "Epoch 432/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.8578 - val_loss: -4.8720\n",
      "Epoch 433/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.9488 - val_loss: -4.9099\n",
      "Epoch 434/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.4419 - val_loss: -4.2066\n",
      "Epoch 435/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.5709 - val_loss: -4.0277\n",
      "Epoch 436/1000\n",
      "90/90 [==============================] - 0s 75us/step - loss: -14.8543 - val_loss: -4.6862\n",
      "Epoch 437/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.5335 - val_loss: -4.7299\n",
      "Epoch 438/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.8212 - val_loss: -4.7860\n",
      "Epoch 439/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.8243 - val_loss: -4.5857\n",
      "Epoch 440/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.3903 - val_loss: -4.4023\n",
      "Epoch 441/1000\n",
      "90/90 [==============================] - 0s 149us/step - loss: -14.7009 - val_loss: -4.4123\n",
      "Epoch 442/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: -14.6389 - val_loss: -4.3206\n",
      "Epoch 443/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.3505 - val_loss: -4.7507\n",
      "Epoch 444/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.4521 - val_loss: -4.9352\n",
      "Epoch 445/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.4532 - val_loss: -4.5473\n",
      "Epoch 446/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8725 - val_loss: -4.0780\n",
      "Epoch 447/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.6636 - val_loss: -4.6647\n",
      "Epoch 448/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.6033 - val_loss: -4.6758\n",
      "Epoch 449/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.4980 - val_loss: -4.2649\n",
      "Epoch 450/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.9263 - val_loss: -4.4808\n",
      "Epoch 451/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.7894 - val_loss: -4.8169\n",
      "Epoch 452/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -15.0490 - val_loss: -4.7715\n",
      "Epoch 453/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: -14.8208 - val_loss: -4.3538\n",
      "Epoch 454/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.7635 - val_loss: -4.5107\n",
      "Epoch 455/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.6563 - val_loss: -4.0840\n",
      "Epoch 456/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.5992 - val_loss: -4.6476\n",
      "Epoch 457/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.6729 - val_loss: -4.7121\n",
      "Epoch 458/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: -14.9256 - val_loss: -4.7574\n",
      "Epoch 459/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.6394 - val_loss: -4.4608\n",
      "Epoch 460/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.4538 - val_loss: -4.4063\n",
      "Epoch 461/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -15.2852 - val_loss: -4.7779\n",
      "Epoch 462/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: -14.6969 - val_loss: -4.6394\n",
      "Epoch 463/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.4216 - val_loss: -4.3886\n",
      "Epoch 464/1000\n",
      "90/90 [==============================] - 0s 79us/step - loss: -14.6314 - val_loss: -4.3562\n",
      "Epoch 465/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.3480 - val_loss: -4.9255\n",
      "Epoch 466/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.6824 - val_loss: -4.5031\n",
      "Epoch 467/1000\n",
      "90/90 [==============================] - 0s 137us/step - loss: -14.8672 - val_loss: -4.5038\n",
      "Epoch 468/1000\n",
      "90/90 [==============================] - 0s 131us/step - loss: -14.9276 - val_loss: -4.3083\n",
      "Epoch 469/1000\n",
      "90/90 [==============================] - 0s 147us/step - loss: -14.7609 - val_loss: -4.2351\n",
      "Epoch 470/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: -14.5466 - val_loss: -4.9460\n",
      "Epoch 471/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -14.8530 - val_loss: -4.8529\n",
      "Epoch 472/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.4155 - val_loss: -4.4361\n",
      "Epoch 473/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.8270 - val_loss: -4.2977\n",
      "Epoch 474/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.6639 - val_loss: -4.2559\n",
      "Epoch 475/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: -14.3538 - val_loss: -4.8071\n",
      "Epoch 476/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -14.6766 - val_loss: -4.7030\n",
      "Epoch 477/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: -14.7837 - val_loss: -4.5051\n",
      "Epoch 478/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.4741 - val_loss: -4.3416\n",
      "Epoch 479/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.8815 - val_loss: -4.3153\n",
      "Epoch 480/1000\n",
      "90/90 [==============================] - 0s 79us/step - loss: -14.6799 - val_loss: -4.3917\n",
      "Epoch 481/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7932 - val_loss: -4.6528\n",
      "Epoch 482/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.5486 - val_loss: -4.6661\n",
      "Epoch 483/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.8548 - val_loss: -4.8558\n",
      "Epoch 484/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -15.1197 - val_loss: -4.5827\n",
      "Epoch 485/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.3594 - val_loss: -4.1329\n",
      "Epoch 486/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.7989 - val_loss: -4.6982\n",
      "Epoch 487/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -15.1129 - val_loss: -4.6471\n",
      "Epoch 488/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.4617 - val_loss: -3.9419\n",
      "Epoch 489/1000\n",
      "90/90 [==============================] - 0s 120us/step - loss: -14.5608 - val_loss: -4.3175\n",
      "Epoch 490/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7521 - val_loss: -4.9231\n",
      "Epoch 491/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.7655 - val_loss: -4.7733\n",
      "Epoch 492/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.8370 - val_loss: -4.3836\n",
      "Epoch 493/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.8286 - val_loss: -4.3733\n",
      "Epoch 494/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.5606 - val_loss: -4.5535\n",
      "Epoch 495/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.8609 - val_loss: -4.7248\n",
      "Epoch 496/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.8970 - val_loss: -4.6369\n",
      "Epoch 497/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7871 - val_loss: -4.4352\n",
      "Epoch 498/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.5196 - val_loss: -4.8209\n",
      "Epoch 499/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8186 - val_loss: -4.7382\n",
      "Epoch 500/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.5343 - val_loss: -4.4603\n",
      "Epoch 501/1000\n",
      "90/90 [==============================] - 0s 80us/step - loss: -14.8090 - val_loss: -4.0620\n",
      "Epoch 502/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.6985 - val_loss: -4.5154\n",
      "Epoch 503/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.8897 - val_loss: -4.8155\n",
      "Epoch 504/1000\n",
      "90/90 [==============================] - 0s 135us/step - loss: -14.6502 - val_loss: -4.6045\n",
      "Epoch 505/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8424 - val_loss: -4.6053\n",
      "Epoch 506/1000\n",
      "90/90 [==============================] - 0s 135us/step - loss: -14.7253 - val_loss: -4.3664\n",
      "Epoch 507/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -14.8042 - val_loss: -4.6237\n",
      "Epoch 508/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.7336 - val_loss: -4.5188\n",
      "Epoch 509/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.7802 - val_loss: -4.5451\n",
      "Epoch 510/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.6493 - val_loss: -4.4852\n",
      "Epoch 511/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.7619 - val_loss: -4.5729\n",
      "Epoch 512/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.9976 - val_loss: -4.6268\n",
      "Epoch 513/1000\n",
      "90/90 [==============================] - 0s 121us/step - loss: -14.7860 - val_loss: -4.5496\n",
      "Epoch 514/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: -14.7120 - val_loss: -4.9619\n",
      "Epoch 515/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.4570 - val_loss: -4.4594\n",
      "Epoch 516/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.5669 - val_loss: -4.1476\n",
      "Epoch 517/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.6235 - val_loss: -4.1305\n",
      "Epoch 518/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.9076 - val_loss: -4.6503\n",
      "Epoch 519/1000\n",
      "90/90 [==============================] - 0s 126us/step - loss: -14.7233 - val_loss: -4.9506\n",
      "Epoch 520/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.4482 - val_loss: -4.8502\n",
      "Epoch 521/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.3214 - val_loss: -4.3307\n",
      "Epoch 522/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.3660 - val_loss: -3.9007\n",
      "Epoch 523/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -15.0036 - val_loss: -4.3060\n",
      "Epoch 524/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.7235 - val_loss: -4.9231\n",
      "Epoch 525/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.7370 - val_loss: -4.8630\n",
      "Epoch 526/1000\n",
      "90/90 [==============================] - 0s 125us/step - loss: -14.2953 - val_loss: -4.1885\n",
      "Epoch 527/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.6614 - val_loss: -4.4774\n",
      "Epoch 528/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.8775 - val_loss: -4.3749\n",
      "Epoch 529/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.4599 - val_loss: -4.8236\n",
      "Epoch 530/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.6060 - val_loss: -4.5369\n",
      "Epoch 531/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.6598 - val_loss: -4.7014\n",
      "Epoch 532/1000\n",
      "90/90 [==============================] - 0s 126us/step - loss: -14.6824 - val_loss: -4.0826\n",
      "Epoch 533/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.9086 - val_loss: -4.5173\n",
      "Epoch 534/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.6256 - val_loss: -4.8086\n",
      "Epoch 535/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8523 - val_loss: -4.8385\n",
      "Epoch 536/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -14.6785 - val_loss: -4.2999\n",
      "Epoch 537/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.8777 - val_loss: -4.3220\n",
      "Epoch 538/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.6808 - val_loss: -4.4995\n",
      "Epoch 539/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.9791 - val_loss: -4.7659\n",
      "Epoch 540/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.6065 - val_loss: -4.7387\n",
      "Epoch 541/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.8362 - val_loss: -4.3747\n",
      "Epoch 542/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.7139 - val_loss: -4.2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 543/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8858 - val_loss: -4.7523\n",
      "Epoch 544/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.7698 - val_loss: -5.0329\n",
      "Epoch 545/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.4057 - val_loss: -4.8678\n",
      "Epoch 546/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.3857 - val_loss: -4.2367\n",
      "Epoch 547/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.2497 - val_loss: -3.7331\n",
      "Epoch 548/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.5965 - val_loss: -4.4763\n",
      "Epoch 549/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7873 - val_loss: -4.9687\n",
      "Epoch 550/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.5532 - val_loss: -4.8198\n",
      "Epoch 551/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -14.8207 - val_loss: -4.2927\n",
      "Epoch 552/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.9251 - val_loss: -4.4632\n",
      "Epoch 553/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.6920 - val_loss: -4.7217\n",
      "Epoch 554/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.8954 - val_loss: -4.3438\n",
      "Epoch 555/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.8284 - val_loss: -4.5981\n",
      "Epoch 556/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.9971 - val_loss: -4.6291\n",
      "Epoch 557/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.1551 - val_loss: -4.4987\n",
      "Epoch 558/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -15.0307 - val_loss: -4.3850\n",
      "Epoch 559/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.5762 - val_loss: -4.6666\n",
      "Epoch 560/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.5567 - val_loss: -4.8678\n",
      "Epoch 561/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.5935 - val_loss: -4.6917\n",
      "Epoch 562/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.6985 - val_loss: -4.0558\n",
      "Epoch 563/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.5331 - val_loss: -3.8109\n",
      "Epoch 564/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.8699 - val_loss: -4.8592\n",
      "Epoch 565/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8951 - val_loss: -5.0278\n",
      "Epoch 566/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.3029 - val_loss: -4.7553\n",
      "Epoch 567/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.6448 - val_loss: -3.9337\n",
      "Epoch 568/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.5248 - val_loss: -3.9032\n",
      "Epoch 569/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.9366 - val_loss: -4.4272\n",
      "Epoch 570/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.6587 - val_loss: -5.2018\n",
      "Epoch 571/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.2791 - val_loss: -4.9964\n",
      "Epoch 572/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.9951 - val_loss: -4.3005\n",
      "Epoch 573/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -14.8167 - val_loss: -3.9522\n",
      "Epoch 574/1000\n",
      "90/90 [==============================] - 0s 79us/step - loss: -14.6725 - val_loss: -4.3892\n",
      "Epoch 575/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.8141 - val_loss: -4.6623\n",
      "Epoch 576/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.6689 - val_loss: -4.7613\n",
      "Epoch 577/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.3042 - val_loss: -4.2061\n",
      "Epoch 578/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.6505 - val_loss: -4.2881\n",
      "Epoch 579/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: -14.7888 - val_loss: -4.8413\n",
      "Epoch 580/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.5380 - val_loss: -4.4662\n",
      "Epoch 581/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.7640 - val_loss: -4.5984\n",
      "Epoch 582/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.5707 - val_loss: -4.7073\n",
      "Epoch 583/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -15.1683 - val_loss: -4.6021\n",
      "Epoch 584/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.8238 - val_loss: -4.4793\n",
      "Epoch 585/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.7581 - val_loss: -4.4124\n",
      "Epoch 586/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -15.1396 - val_loss: -4.5458\n",
      "Epoch 587/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.5504 - val_loss: -4.1999\n",
      "Epoch 588/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.9452 - val_loss: -4.1517\n",
      "Epoch 589/1000\n",
      "90/90 [==============================] - 0s 135us/step - loss: -14.8303 - val_loss: -4.4867\n",
      "Epoch 590/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.7180 - val_loss: -5.0543\n",
      "Epoch 591/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: -14.6170 - val_loss: -4.5319\n",
      "Epoch 592/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.5572 - val_loss: -4.4542\n",
      "Epoch 593/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.7712 - val_loss: -4.3512\n",
      "Epoch 594/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -15.0202 - val_loss: -4.6232\n",
      "Epoch 595/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.3567 - val_loss: -5.0205\n",
      "Epoch 596/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.6498 - val_loss: -4.8803\n",
      "Epoch 597/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -14.4108 - val_loss: -4.1346\n",
      "Epoch 598/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.6905 - val_loss: -4.1458\n",
      "Epoch 599/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.7810 - val_loss: -4.2991\n",
      "Epoch 600/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.6432 - val_loss: -5.0065\n",
      "Epoch 601/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.4682 - val_loss: -4.5295\n",
      "Epoch 602/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.5792 - val_loss: -4.5217\n",
      "Epoch 603/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.7848 - val_loss: -4.6860\n",
      "Epoch 604/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.6912 - val_loss: -4.2530\n",
      "Epoch 605/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.3709 - val_loss: -4.0494\n",
      "Epoch 606/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.8395 - val_loss: -4.5282\n",
      "Epoch 607/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.2614 - val_loss: -4.9140\n",
      "Epoch 608/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -14.5496 - val_loss: -4.5659\n",
      "Epoch 609/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.5942 - val_loss: -4.5423\n",
      "Epoch 610/1000\n",
      "90/90 [==============================] - 0s 124us/step - loss: -14.9309 - val_loss: -4.5655\n",
      "Epoch 611/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.4140 - val_loss: -4.8074\n",
      "Epoch 612/1000\n",
      "90/90 [==============================] - 0s 123us/step - loss: -14.6553 - val_loss: -4.3357\n",
      "Epoch 613/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.7590 - val_loss: -4.4092\n",
      "Epoch 614/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -14.4681 - val_loss: -4.5987\n",
      "Epoch 615/1000\n",
      "90/90 [==============================] - 0s 131us/step - loss: -14.8486 - val_loss: -4.6241\n",
      "Epoch 616/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.8250 - val_loss: -4.6720\n",
      "Epoch 617/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.8277 - val_loss: -4.6713\n",
      "Epoch 618/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.3297 - val_loss: -4.2775\n",
      "Epoch 619/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.5278 - val_loss: -4.3514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 620/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.7023 - val_loss: -4.6706\n",
      "Epoch 621/1000\n",
      "90/90 [==============================] - 0s 137us/step - loss: -14.8595 - val_loss: -4.8994\n",
      "Epoch 622/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.5136 - val_loss: -4.6410\n",
      "Epoch 623/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.7485 - val_loss: -4.5119\n",
      "Epoch 624/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: -14.9004 - val_loss: -4.2704\n",
      "Epoch 625/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -14.4703 - val_loss: -4.4712\n",
      "Epoch 626/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.7822 - val_loss: -4.4874\n",
      "Epoch 627/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: -14.5028 - val_loss: -4.7082\n",
      "Epoch 628/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.7456 - val_loss: -4.6826\n",
      "Epoch 629/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.4749 - val_loss: -4.3061\n",
      "Epoch 630/1000\n",
      "90/90 [==============================] - 0s 154us/step - loss: -14.6994 - val_loss: -4.3765\n",
      "Epoch 631/1000\n",
      "90/90 [==============================] - 0s 138us/step - loss: -14.6924 - val_loss: -4.5190\n",
      "Epoch 632/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.6583 - val_loss: -4.8235\n",
      "Epoch 633/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.3870 - val_loss: -4.9216\n",
      "Epoch 634/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.6703 - val_loss: -4.9820\n",
      "Epoch 635/1000\n",
      "90/90 [==============================] - 0s 74us/step - loss: -14.6200 - val_loss: -4.6920\n",
      "Epoch 636/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.9288 - val_loss: -3.7583\n",
      "Epoch 637/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.4049 - val_loss: -4.1061\n",
      "Epoch 638/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.7007 - val_loss: -4.5566\n",
      "Epoch 639/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.7060 - val_loss: -5.0087\n",
      "Epoch 640/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.5470 - val_loss: -4.7925\n",
      "Epoch 641/1000\n",
      "90/90 [==============================] - 0s 117us/step - loss: -14.8347 - val_loss: -4.7211\n",
      "Epoch 642/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.6494 - val_loss: -4.1939\n",
      "Epoch 643/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.3166 - val_loss: -4.0217\n",
      "Epoch 644/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.7960 - val_loss: -4.5204\n",
      "Epoch 645/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.6710 - val_loss: -4.9917\n",
      "Epoch 646/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6383 - val_loss: -4.6720\n",
      "Epoch 647/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.6818 - val_loss: -4.5700\n",
      "Epoch 648/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7599 - val_loss: -4.4141\n",
      "Epoch 649/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.2005 - val_loss: -3.9997\n",
      "Epoch 650/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.5771 - val_loss: -4.7411\n",
      "Epoch 651/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.4497 - val_loss: -4.8348\n",
      "Epoch 652/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.5061 - val_loss: -5.2389\n",
      "Epoch 653/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.3443 - val_loss: -4.3472\n",
      "Epoch 654/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.5261 - val_loss: -3.9633\n",
      "Epoch 655/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.2919 - val_loss: -4.2980\n",
      "Epoch 656/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.8761 - val_loss: -4.7313\n",
      "Epoch 657/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.6804 - val_loss: -5.1135\n",
      "Epoch 658/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.4551 - val_loss: -4.5928\n",
      "Epoch 659/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.7628 - val_loss: -4.2284\n",
      "Epoch 660/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.8096 - val_loss: -4.0490\n",
      "Epoch 661/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -14.5112 - val_loss: -4.6244\n",
      "Epoch 662/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.7337 - val_loss: -4.9659\n",
      "Epoch 663/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7381 - val_loss: -4.6308\n",
      "Epoch 664/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.7792 - val_loss: -4.0027\n",
      "Epoch 665/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -14.7342 - val_loss: -3.9809\n",
      "Epoch 666/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8545 - val_loss: -4.7722\n",
      "Epoch 667/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.4311 - val_loss: -4.9389\n",
      "Epoch 668/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.6657 - val_loss: -4.8177\n",
      "Epoch 669/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.8901 - val_loss: -4.3233\n",
      "Epoch 670/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.4655 - val_loss: -4.1981\n",
      "Epoch 671/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.6911 - val_loss: -4.7511\n",
      "Epoch 672/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.5792 - val_loss: -4.6109\n",
      "Epoch 673/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.5903 - val_loss: -4.4791\n",
      "Epoch 674/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.3957 - val_loss: -4.3781\n",
      "Epoch 675/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.6176 - val_loss: -4.7153\n",
      "Epoch 676/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.6845 - val_loss: -4.5354\n",
      "Epoch 677/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7251 - val_loss: -4.2403\n",
      "Epoch 678/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.5678 - val_loss: -4.6088\n",
      "Epoch 679/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -15.0217 - val_loss: -4.5885\n",
      "Epoch 680/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.8361 - val_loss: -4.5131\n",
      "Epoch 681/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.7209 - val_loss: -4.2979\n",
      "Epoch 682/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.6668 - val_loss: -4.5677\n",
      "Epoch 683/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.5663 - val_loss: -4.9007\n",
      "Epoch 684/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.8525 - val_loss: -4.8951\n",
      "Epoch 685/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.5743 - val_loss: -3.9306\n",
      "Epoch 686/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.7415 - val_loss: -4.3794\n",
      "Epoch 687/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.7741 - val_loss: -4.8472\n",
      "Epoch 688/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.7246 - val_loss: -4.3702\n",
      "Epoch 689/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.4659 - val_loss: -4.3348\n",
      "Epoch 690/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.4983 - val_loss: -4.9806\n",
      "Epoch 691/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.9339 - val_loss: -5.0886\n",
      "Epoch 692/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.9342 - val_loss: -4.5187\n",
      "Epoch 693/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.9509 - val_loss: -4.2306\n",
      "Epoch 694/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.6108 - val_loss: -4.2450\n",
      "Epoch 695/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7594 - val_loss: -4.2755\n",
      "Epoch 696/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7575 - val_loss: -4.8874\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 95us/step - loss: -14.3082 - val_loss: -4.7580\n",
      "Epoch 698/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: -14.8982 - val_loss: -4.8551\n",
      "Epoch 699/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.9287 - val_loss: -4.4640\n",
      "Epoch 700/1000\n",
      "90/90 [==============================] - 0s 78us/step - loss: -14.5826 - val_loss: -3.9457\n",
      "Epoch 701/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.5931 - val_loss: -4.3159\n",
      "Epoch 702/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.7279 - val_loss: -4.5794\n",
      "Epoch 703/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.3513 - val_loss: -5.2376\n",
      "Epoch 704/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: -14.3046 - val_loss: -4.9191\n",
      "Epoch 705/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.6632 - val_loss: -3.8530\n",
      "Epoch 706/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.3103 - val_loss: -3.7720\n",
      "Epoch 707/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -15.0857 - val_loss: -4.7200\n",
      "Epoch 708/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6684 - val_loss: -4.9514\n",
      "Epoch 709/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.4587 - val_loss: -4.6707\n",
      "Epoch 710/1000\n",
      "90/90 [==============================] - 0s 122us/step - loss: -14.8027 - val_loss: -4.2679\n",
      "Epoch 711/1000\n",
      "90/90 [==============================] - 0s 125us/step - loss: -14.7997 - val_loss: -4.3900\n",
      "Epoch 712/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.6929 - val_loss: -4.7318\n",
      "Epoch 713/1000\n",
      "90/90 [==============================] - 0s 113us/step - loss: -14.8649 - val_loss: -4.8602\n",
      "Epoch 714/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8710 - val_loss: -4.4442\n",
      "Epoch 715/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.8484 - val_loss: -4.4905\n",
      "Epoch 716/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -15.1949 - val_loss: -4.1065\n",
      "Epoch 717/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.9716 - val_loss: -4.5271\n",
      "Epoch 718/1000\n",
      "90/90 [==============================] - 0s 120us/step - loss: -14.9018 - val_loss: -4.7346\n",
      "Epoch 719/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7297 - val_loss: -4.6291\n",
      "Epoch 720/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.0683 - val_loss: -4.0356\n",
      "Epoch 721/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8151 - val_loss: -4.5869\n",
      "Epoch 722/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -14.6981 - val_loss: -4.8049\n",
      "Epoch 723/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: -14.4670 - val_loss: -4.9006\n",
      "Epoch 724/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.6880 - val_loss: -4.4686\n",
      "Epoch 725/1000\n",
      "90/90 [==============================] - 0s 80us/step - loss: -14.9714 - val_loss: -4.1704\n",
      "Epoch 726/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7343 - val_loss: -4.9230\n",
      "Epoch 727/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.6293 - val_loss: -4.8973\n",
      "Epoch 728/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.6898 - val_loss: -4.5185\n",
      "Epoch 729/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.8246 - val_loss: -4.2150\n",
      "Epoch 730/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.8268 - val_loss: -4.3889\n",
      "Epoch 731/1000\n",
      "90/90 [==============================] - 0s 79us/step - loss: -14.7533 - val_loss: -4.7331\n",
      "Epoch 732/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.7463 - val_loss: -4.9961\n",
      "Epoch 733/1000\n",
      "90/90 [==============================] - 0s 144us/step - loss: -14.5448 - val_loss: -4.4638\n",
      "Epoch 734/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: -14.7087 - val_loss: -4.0304\n",
      "Epoch 735/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.3615 - val_loss: -4.8038\n",
      "Epoch 736/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -14.6520 - val_loss: -4.7991\n",
      "Epoch 737/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.4868 - val_loss: -4.1038\n",
      "Epoch 738/1000\n",
      "90/90 [==============================] - 0s 78us/step - loss: -15.0089 - val_loss: -4.4458\n",
      "Epoch 739/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.8337 - val_loss: -4.9131\n",
      "Epoch 740/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6259 - val_loss: -4.9357\n",
      "Epoch 741/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.7569 - val_loss: -4.1628\n",
      "Epoch 742/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.5855 - val_loss: -4.2288\n",
      "Epoch 743/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -15.1153 - val_loss: -4.7295\n",
      "Epoch 744/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.6024 - val_loss: -4.7766\n",
      "Epoch 745/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.2479 - val_loss: -3.9323\n",
      "Epoch 746/1000\n",
      "90/90 [==============================] - 0s 112us/step - loss: -14.8348 - val_loss: -4.3273\n",
      "Epoch 747/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.6634 - val_loss: -4.8922\n",
      "Epoch 748/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -15.1048 - val_loss: -4.9739\n",
      "Epoch 749/1000\n",
      "90/90 [==============================] - 0s 120us/step - loss: -14.6051 - val_loss: -4.0600\n",
      "Epoch 750/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.3401 - val_loss: -4.0794\n",
      "Epoch 751/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.4085 - val_loss: -4.5849\n",
      "Epoch 752/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.7016 - val_loss: -5.1447\n",
      "Epoch 753/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.9178 - val_loss: -4.8956\n",
      "Epoch 754/1000\n",
      "90/90 [==============================] - 0s 123us/step - loss: -14.7167 - val_loss: -4.1218\n",
      "Epoch 755/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.6217 - val_loss: -4.2998\n",
      "Epoch 756/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8631 - val_loss: -4.5542\n",
      "Epoch 757/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.7319 - val_loss: -4.4667\n",
      "Epoch 758/1000\n",
      "90/90 [==============================] - 0s 117us/step - loss: -14.7109 - val_loss: -4.5398\n",
      "Epoch 759/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.6920 - val_loss: -4.5596\n",
      "Epoch 760/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.5756 - val_loss: -4.0968\n",
      "Epoch 761/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.6047 - val_loss: -4.6482\n",
      "Epoch 762/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.8004 - val_loss: -5.0067\n",
      "Epoch 763/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.5322 - val_loss: -4.9072\n",
      "Epoch 764/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.5342 - val_loss: -4.1219\n",
      "Epoch 765/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.9556 - val_loss: -4.1937\n",
      "Epoch 766/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.7838 - val_loss: -4.2240\n",
      "Epoch 767/1000\n",
      "90/90 [==============================] - 0s 118us/step - loss: -14.6003 - val_loss: -4.6401\n",
      "Epoch 768/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.4071 - val_loss: -5.0135\n",
      "Epoch 769/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: -14.7525 - val_loss: -4.3957\n",
      "Epoch 770/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.7081 - val_loss: -4.2574\n",
      "Epoch 771/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.6927 - val_loss: -4.4595\n",
      "Epoch 772/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -15.0532 - val_loss: -4.7293\n",
      "Epoch 773/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.9485 - val_loss: -4.5815\n",
      "Epoch 774/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 91us/step - loss: -14.7605 - val_loss: -4.5340\n",
      "Epoch 775/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.6883 - val_loss: -4.4297\n",
      "Epoch 776/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -15.0116 - val_loss: -4.6519\n",
      "Epoch 777/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.5098 - val_loss: -4.4976\n",
      "Epoch 778/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.6453 - val_loss: -4.2144\n",
      "Epoch 779/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.9205 - val_loss: -5.0027\n",
      "Epoch 780/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7000 - val_loss: -4.8132\n",
      "Epoch 781/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.2838 - val_loss: -4.3997\n",
      "Epoch 782/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.5386 - val_loss: -3.9919\n",
      "Epoch 783/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.4849 - val_loss: -4.3149\n",
      "Epoch 784/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.6812 - val_loss: -5.0115\n",
      "Epoch 785/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.8074 - val_loss: -5.2046\n",
      "Epoch 786/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.4793 - val_loss: -4.5455\n",
      "Epoch 787/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -15.2602 - val_loss: -4.0235\n",
      "Epoch 788/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.6204 - val_loss: -3.8287\n",
      "Epoch 789/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.6101 - val_loss: -4.7559\n",
      "Epoch 790/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.5718 - val_loss: -5.0322\n",
      "Epoch 791/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.3048 - val_loss: -4.2316\n",
      "Epoch 792/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.8373 - val_loss: -4.4157\n",
      "Epoch 793/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.5773 - val_loss: -4.9066\n",
      "Epoch 794/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.6750 - val_loss: -4.7232\n",
      "Epoch 795/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.5958 - val_loss: -4.6498\n",
      "Epoch 796/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.4323 - val_loss: -4.0531\n",
      "Epoch 797/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.9104 - val_loss: -4.2507\n",
      "Epoch 798/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.6547 - val_loss: -4.7438\n",
      "Epoch 799/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.9965 - val_loss: -5.0559\n",
      "Epoch 800/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.9268 - val_loss: -4.9522\n",
      "Epoch 801/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.4078 - val_loss: -3.8566\n",
      "Epoch 802/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6166 - val_loss: -3.7645\n",
      "Epoch 803/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.9058 - val_loss: -4.4660\n",
      "Epoch 804/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -15.1617 - val_loss: -5.0102\n",
      "Epoch 805/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.4501 - val_loss: -4.6021\n",
      "Epoch 806/1000\n",
      "90/90 [==============================] - 0s 233us/step - loss: -14.8546 - val_loss: -4.4684\n",
      "Epoch 807/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.3773 - val_loss: -4.7097\n",
      "Epoch 808/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.8276 - val_loss: -4.8607\n",
      "Epoch 809/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.8015 - val_loss: -4.1791\n",
      "Epoch 810/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.6448 - val_loss: -4.3711\n",
      "Epoch 811/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.7567 - val_loss: -4.8563\n",
      "Epoch 812/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.5403 - val_loss: -4.6850\n",
      "Epoch 813/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.8178 - val_loss: -4.2434\n",
      "Epoch 814/1000\n",
      "90/90 [==============================] - 0s 117us/step - loss: -14.6423 - val_loss: -4.4533\n",
      "Epoch 815/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.6847 - val_loss: -4.6642\n",
      "Epoch 816/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -14.5605 - val_loss: -4.4523\n",
      "Epoch 817/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8139 - val_loss: -4.5572\n",
      "Epoch 818/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -15.1159 - val_loss: -4.7716\n",
      "Epoch 819/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.5207 - val_loss: -4.6518\n",
      "Epoch 820/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.9028 - val_loss: -4.3246\n",
      "Epoch 821/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.5153 - val_loss: -4.5564\n",
      "Epoch 822/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.7633 - val_loss: -4.6595\n",
      "Epoch 823/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -15.0685 - val_loss: -4.2875\n",
      "Epoch 824/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.6594 - val_loss: -4.5377\n",
      "Epoch 825/1000\n",
      "90/90 [==============================] - 0s 124us/step - loss: -14.3923 - val_loss: -4.6510\n",
      "Epoch 826/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.5073 - val_loss: -4.2929\n",
      "Epoch 827/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.6661 - val_loss: -4.2556\n",
      "Epoch 828/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.7820 - val_loss: -4.9475\n",
      "Epoch 829/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.9282 - val_loss: -4.9075\n",
      "Epoch 830/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.3373 - val_loss: -4.7544\n",
      "Epoch 831/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.3421 - val_loss: -3.8106\n",
      "Epoch 832/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.3380 - val_loss: -4.5034\n",
      "Epoch 833/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.8462 - val_loss: -4.6345\n",
      "Epoch 834/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.7406 - val_loss: -4.8726\n",
      "Epoch 835/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.7511 - val_loss: -4.4279\n",
      "Epoch 836/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.4744 - val_loss: -4.8220\n",
      "Epoch 837/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.6321 - val_loss: -4.6134\n",
      "Epoch 838/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.4243 - val_loss: -4.2161\n",
      "Epoch 839/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.5889 - val_loss: -4.2338\n",
      "Epoch 840/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.4390 - val_loss: -4.5789\n",
      "Epoch 841/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.6254 - val_loss: -4.7701\n",
      "Epoch 842/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.6933 - val_loss: -4.5180\n",
      "Epoch 843/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.9782 - val_loss: -4.7169\n",
      "Epoch 844/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.7330 - val_loss: -4.3493\n",
      "Epoch 845/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.7732 - val_loss: -4.2197\n",
      "Epoch 846/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.7699 - val_loss: -4.7936\n",
      "Epoch 847/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.7735 - val_loss: -4.6467\n",
      "Epoch 848/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -15.0236 - val_loss: -4.6332\n",
      "Epoch 849/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.9333 - val_loss: -4.4106\n",
      "Epoch 850/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.9935 - val_loss: -4.4103\n",
      "Epoch 851/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 119us/step - loss: -14.5698 - val_loss: -4.6578\n",
      "Epoch 852/1000\n",
      "90/90 [==============================] - 0s 133us/step - loss: -14.9846 - val_loss: -4.4179\n",
      "Epoch 853/1000\n",
      "90/90 [==============================] - 0s 128us/step - loss: -14.8810 - val_loss: -4.6835\n",
      "Epoch 854/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.8813 - val_loss: -4.9604\n",
      "Epoch 855/1000\n",
      "90/90 [==============================] - 0s 127us/step - loss: -14.5216 - val_loss: -4.3850\n",
      "Epoch 856/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.9512 - val_loss: -4.3371\n",
      "Epoch 857/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.9243 - val_loss: -4.4623\n",
      "Epoch 858/1000\n",
      "90/90 [==============================] - 0s 143us/step - loss: -14.7426 - val_loss: -4.4949\n",
      "Epoch 859/1000\n",
      "90/90 [==============================] - 0s 154us/step - loss: -14.7098 - val_loss: -4.7137\n",
      "Epoch 860/1000\n",
      "90/90 [==============================] - 0s 128us/step - loss: -14.8076 - val_loss: -4.4628\n",
      "Epoch 861/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.8726 - val_loss: -4.5910\n",
      "Epoch 862/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.8389 - val_loss: -4.4950\n",
      "Epoch 863/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.7288 - val_loss: -4.4132\n",
      "Epoch 864/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: -14.4936 - val_loss: -4.3918\n",
      "Epoch 865/1000\n",
      "90/90 [==============================] - 0s 130us/step - loss: -14.5203 - val_loss: -4.6602\n",
      "Epoch 866/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.5498 - val_loss: -4.7614\n",
      "Epoch 867/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.7193 - val_loss: -4.4721\n",
      "Epoch 868/1000\n",
      "90/90 [==============================] - 0s 129us/step - loss: -14.6512 - val_loss: -4.7585\n",
      "Epoch 869/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.4827 - val_loss: -4.6215\n",
      "Epoch 870/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.9250 - val_loss: -4.3400\n",
      "Epoch 871/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -15.0332 - val_loss: -4.3977\n",
      "Epoch 872/1000\n",
      "90/90 [==============================] - 0s 86us/step - loss: -14.9838 - val_loss: -4.3707\n",
      "Epoch 873/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.8370 - val_loss: -4.5484\n",
      "Epoch 874/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.6929 - val_loss: -5.1405\n",
      "Epoch 875/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.6561 - val_loss: -4.4683\n",
      "Epoch 876/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.4788 - val_loss: -3.9310\n",
      "Epoch 877/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: -14.7187 - val_loss: -4.1243\n",
      "Epoch 878/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.5789 - val_loss: -4.9336\n",
      "Epoch 879/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.9395 - val_loss: -4.7583\n",
      "Epoch 880/1000\n",
      "90/90 [==============================] - 0s 126us/step - loss: -14.2588 - val_loss: -4.1546\n",
      "Epoch 881/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.7195 - val_loss: -4.5502\n",
      "Epoch 882/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.8849 - val_loss: -4.6978\n",
      "Epoch 883/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.7673 - val_loss: -4.5992\n",
      "Epoch 884/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -15.0130 - val_loss: -4.5710\n",
      "Epoch 885/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.6947 - val_loss: -4.5999\n",
      "Epoch 886/1000\n",
      "90/90 [==============================] - 0s 100us/step - loss: -14.9115 - val_loss: -4.6386\n",
      "Epoch 887/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.7618 - val_loss: -4.7654\n",
      "Epoch 888/1000\n",
      "90/90 [==============================] - 0s 105us/step - loss: -14.2839 - val_loss: -4.0096\n",
      "Epoch 889/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6068 - val_loss: -4.4265\n",
      "Epoch 890/1000\n",
      "90/90 [==============================] - 0s 111us/step - loss: -15.0701 - val_loss: -4.7142\n",
      "Epoch 891/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.8479 - val_loss: -4.6669\n",
      "Epoch 892/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.5968 - val_loss: -4.5581\n",
      "Epoch 893/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.7840 - val_loss: -4.1028\n",
      "Epoch 894/1000\n",
      "90/90 [==============================] - 0s 117us/step - loss: -14.6214 - val_loss: -4.5123\n",
      "Epoch 895/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.6896 - val_loss: -4.8693\n",
      "Epoch 896/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.5028 - val_loss: -4.9303\n",
      "Epoch 897/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.7190 - val_loss: -4.5798\n",
      "Epoch 898/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.7086 - val_loss: -4.0462\n",
      "Epoch 899/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.5777 - val_loss: -4.2074\n",
      "Epoch 900/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.5790 - val_loss: -5.1256\n",
      "Epoch 901/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -15.0219 - val_loss: -4.7895\n",
      "Epoch 902/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.6097 - val_loss: -4.2791\n",
      "Epoch 903/1000\n",
      "90/90 [==============================] - 0s 137us/step - loss: -14.6552 - val_loss: -4.2642\n",
      "Epoch 904/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.8294 - val_loss: -4.3293\n",
      "Epoch 905/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.6374 - val_loss: -4.5046\n",
      "Epoch 906/1000\n",
      "90/90 [==============================] - 0s 103us/step - loss: -14.7389 - val_loss: -4.7873\n",
      "Epoch 907/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.5743 - val_loss: -4.5997\n",
      "Epoch 908/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.4506 - val_loss: -4.7127\n",
      "Epoch 909/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.9990 - val_loss: -4.2344\n",
      "Epoch 910/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.5726 - val_loss: -3.8467\n",
      "Epoch 911/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -15.0231 - val_loss: -4.7950\n",
      "Epoch 912/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.7559 - val_loss: -5.2857\n",
      "Epoch 913/1000\n",
      "90/90 [==============================] - 0s 163us/step - loss: -14.7510 - val_loss: -4.8515\n",
      "Epoch 914/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.9713 - val_loss: -4.2594\n",
      "Epoch 915/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.7889 - val_loss: -3.9326\n",
      "Epoch 916/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.8187 - val_loss: -4.3801\n",
      "Epoch 917/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.3618 - val_loss: -4.6259\n",
      "Epoch 918/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.7474 - val_loss: -4.7826\n",
      "Epoch 919/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.5775 - val_loss: -4.5363\n",
      "Epoch 920/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.8282 - val_loss: -4.6608\n",
      "Epoch 921/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.9467 - val_loss: -4.8613\n",
      "Epoch 922/1000\n",
      "90/90 [==============================] - 0s 109us/step - loss: -14.8352 - val_loss: -4.4645\n",
      "Epoch 923/1000\n",
      "90/90 [==============================] - 0s 130us/step - loss: -14.4658 - val_loss: -4.4295\n",
      "Epoch 924/1000\n",
      "90/90 [==============================] - 0s 125us/step - loss: -14.7107 - val_loss: -4.2272\n",
      "Epoch 925/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.7911 - val_loss: -4.4853\n",
      "Epoch 926/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.8224 - val_loss: -4.9686\n",
      "Epoch 927/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.9535 - val_loss: -4.9466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 928/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.9698 - val_loss: -4.6208\n",
      "Epoch 929/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.8779 - val_loss: -4.0477\n",
      "Epoch 930/1000\n",
      "90/90 [==============================] - 0s 94us/step - loss: -14.6201 - val_loss: -4.2029\n",
      "Epoch 931/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.5477 - val_loss: -4.4958\n",
      "Epoch 932/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.9262 - val_loss: -4.9145\n",
      "Epoch 933/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.2741 - val_loss: -5.0581\n",
      "Epoch 934/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -15.1474 - val_loss: -4.7007\n",
      "Epoch 935/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.3316 - val_loss: -3.6775\n",
      "Epoch 936/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.9634 - val_loss: -4.0623\n",
      "Epoch 937/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.8978 - val_loss: -5.0242\n",
      "Epoch 938/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.7946 - val_loss: -5.0675\n",
      "Epoch 939/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.6466 - val_loss: -4.9834\n",
      "Epoch 940/1000\n",
      "90/90 [==============================] - 0s 119us/step - loss: -14.6620 - val_loss: -4.0837\n",
      "Epoch 941/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.2750 - val_loss: -3.7005\n",
      "Epoch 942/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.7166 - val_loss: -4.6767\n",
      "Epoch 943/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8476 - val_loss: -5.0479\n",
      "Epoch 944/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.7611 - val_loss: -4.6863\n",
      "Epoch 945/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.7924 - val_loss: -4.3432\n",
      "Epoch 946/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.9212 - val_loss: -4.2935\n",
      "Epoch 947/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.7155 - val_loss: -4.6408\n",
      "Epoch 948/1000\n",
      "90/90 [==============================] - 0s 125us/step - loss: -14.8333 - val_loss: -4.6894\n",
      "Epoch 949/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.8631 - val_loss: -4.6396\n",
      "Epoch 950/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8252 - val_loss: -4.7608\n",
      "Epoch 951/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.5400 - val_loss: -4.5832\n",
      "Epoch 952/1000\n",
      "90/90 [==============================] - 0s 140us/step - loss: -14.4641 - val_loss: -4.2907\n",
      "Epoch 953/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.6982 - val_loss: -4.5620\n",
      "Epoch 954/1000\n",
      "90/90 [==============================] - 0s 115us/step - loss: -14.5234 - val_loss: -4.3489\n",
      "Epoch 955/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.6647 - val_loss: -4.6517\n",
      "Epoch 956/1000\n",
      "90/90 [==============================] - 0s 116us/step - loss: -14.6549 - val_loss: -4.7905\n",
      "Epoch 957/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -14.7216 - val_loss: -4.5452\n",
      "Epoch 958/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.1908 - val_loss: -4.3534\n",
      "Epoch 959/1000\n",
      "90/90 [==============================] - 0s 78us/step - loss: -14.8248 - val_loss: -4.8123\n",
      "Epoch 960/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.6834 - val_loss: -4.6670\n",
      "Epoch 961/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.9369 - val_loss: -4.4488\n",
      "Epoch 962/1000\n",
      "90/90 [==============================] - 0s 93us/step - loss: -15.0694 - val_loss: -4.1842\n",
      "Epoch 963/1000\n",
      "90/90 [==============================] - 0s 110us/step - loss: -14.6981 - val_loss: -4.5497\n",
      "Epoch 964/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.6913 - val_loss: -4.4087\n",
      "Epoch 965/1000\n",
      "90/90 [==============================] - 0s 90us/step - loss: -14.5886 - val_loss: -4.8783\n",
      "Epoch 966/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.7019 - val_loss: -4.6331\n",
      "Epoch 967/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.8534 - val_loss: -4.3635\n",
      "Epoch 968/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.7777 - val_loss: -4.7841\n",
      "Epoch 969/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.8380 - val_loss: -4.7899\n",
      "Epoch 970/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.9562 - val_loss: -4.5360\n",
      "Epoch 971/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -15.0730 - val_loss: -4.0518\n",
      "Epoch 972/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.4100 - val_loss: -4.0840\n",
      "Epoch 973/1000\n",
      "90/90 [==============================] - 0s 84us/step - loss: -14.8657 - val_loss: -4.5035\n",
      "Epoch 974/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.8322 - val_loss: -4.7894\n",
      "Epoch 975/1000\n",
      "90/90 [==============================] - 0s 98us/step - loss: -14.5581 - val_loss: -4.8081\n",
      "Epoch 976/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.4743 - val_loss: -4.3541\n",
      "Epoch 977/1000\n",
      "90/90 [==============================] - 0s 108us/step - loss: -14.6656 - val_loss: -4.2587\n",
      "Epoch 978/1000\n",
      "90/90 [==============================] - 0s 80us/step - loss: -14.2376 - val_loss: -4.7135\n",
      "Epoch 979/1000\n",
      "90/90 [==============================] - 0s 102us/step - loss: -14.8456 - val_loss: -4.8256\n",
      "Epoch 980/1000\n",
      "90/90 [==============================] - 0s 99us/step - loss: -14.7939 - val_loss: -4.5805\n",
      "Epoch 981/1000\n",
      "90/90 [==============================] - 0s 96us/step - loss: -14.6942 - val_loss: -4.3249\n",
      "Epoch 982/1000\n",
      "90/90 [==============================] - 0s 104us/step - loss: -14.8096 - val_loss: -4.8766\n",
      "Epoch 983/1000\n",
      "90/90 [==============================] - 0s 83us/step - loss: -14.8195 - val_loss: -4.6840\n",
      "Epoch 984/1000\n",
      "90/90 [==============================] - 0s 95us/step - loss: -14.4591 - val_loss: -3.9258\n",
      "Epoch 985/1000\n",
      "90/90 [==============================] - 0s 89us/step - loss: -14.6411 - val_loss: -4.2925\n",
      "Epoch 986/1000\n",
      "90/90 [==============================] - 0s 85us/step - loss: -14.6510 - val_loss: -4.9743\n",
      "Epoch 987/1000\n",
      "90/90 [==============================] - 0s 75us/step - loss: -14.5766 - val_loss: -5.1247\n",
      "Epoch 988/1000\n",
      "90/90 [==============================] - 0s 88us/step - loss: -14.7663 - val_loss: -4.3985\n",
      "Epoch 989/1000\n",
      "90/90 [==============================] - 0s 82us/step - loss: -14.8162 - val_loss: -4.0941\n",
      "Epoch 990/1000\n",
      "90/90 [==============================] - 0s 87us/step - loss: -14.6036 - val_loss: -4.0681\n",
      "Epoch 991/1000\n",
      "90/90 [==============================] - 0s 106us/step - loss: -14.5427 - val_loss: -4.9222\n",
      "Epoch 992/1000\n",
      "90/90 [==============================] - 0s 101us/step - loss: -14.1655 - val_loss: -5.1688\n",
      "Epoch 993/1000\n",
      "90/90 [==============================] - 0s 91us/step - loss: -14.4958 - val_loss: -4.3968\n",
      "Epoch 994/1000\n",
      "90/90 [==============================] - 0s 114us/step - loss: -14.7010 - val_loss: -3.5816\n",
      "Epoch 995/1000\n",
      "90/90 [==============================] - 0s 97us/step - loss: -14.5296 - val_loss: -4.1180\n",
      "Epoch 996/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.5539 - val_loss: -4.7439\n",
      "Epoch 997/1000\n",
      "90/90 [==============================] - 0s 107us/step - loss: -14.6139 - val_loss: -5.3106\n",
      "Epoch 998/1000\n",
      "90/90 [==============================] - 0s 92us/step - loss: -14.8012 - val_loss: -5.2210\n",
      "Epoch 999/1000\n",
      "90/90 [==============================] - 0s 4ms/step - loss: -14.7952 - val_loss: -4.1904\n",
      "Epoch 1000/1000\n",
      "90/90 [==============================] - 0s 291us/step - loss: -14.5120 - val_loss: -3.6131\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train.reshape(-1,1), \n",
    "          np.repeat(y_train.reshape(-1,1),2,axis=1), \n",
    "          epochs=1000, validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predictions and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = np.arange(low,high,0.01).reshape(-1,1)\n",
    "y_test = f(X_test) + np.random.normal(\n",
    "    0,std,size=(X_test.shape[0])).reshape(-1,1)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "plt.errorbar(X_test,prediction[:,0],\n",
    "             yerr=2*prediction[:,1],\n",
    "             color='#0A5FB4',\n",
    "             alpha=0.8,\n",
    "             label='prediction')\n",
    "\n",
    "plt.plot(X_test, y_test,'x',c='r',\n",
    "         alpha=0.8, label='real generated data')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable noise\n",
    "\n",
    "Let's now assume that our noise is not constant but a function of x:\n",
    "\n",
    "$$y = f(x) + \\epsilon(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, generate the data according to this new condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "std = 0.4\n",
    "epsilon = np.random.normal(0,std,size=(points))\n",
    "\n",
    "y_train = f(X_train)+epsilon+[np.random.normal(0,i) for i in (np.sin(X_train*-10))+1]\n",
    "\n",
    "plt.plot(X_train,y_train,'o',\n",
    "         c='#072146',label='real generated data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have $$y = \\phi_w (x) + \\psi_v (x)$$\n",
    "\n",
    "$$ p(y \\mid x, w, v) = \\mathcal{N}(y \\mid \\phi_w(x), \\psi_v^2(x))=$$\n",
    "            $$= \\frac{1}{\\sqrt{2\\pi \\psi_v(x)^2}} e^{-\\frac{(y-\\phi_w (x))^2}{2\\psi_v(x)^2}}$$\n",
    "\n",
    "i.e. We estimate a variability of $y_i$ for every data point $x_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the new regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = Input(name='input', shape=(1,), dtype='float32')\n",
    "\n",
    "phi = Dense(units=1, activation=\"linear\", \n",
    "            name='w',\n",
    "            kernel_initializer='ones')(i)\n",
    "\n",
    "psi = Dense(units=12, activation=\"sigmoid\", \n",
    "            name='v_0')(i)\n",
    "psi = Dense(units=6, activation=\"sigmoid\", \n",
    "            name='v_1')(psi)\n",
    "psi = Dense(units=1, activation=elu_plus1, \n",
    "            name='v_2')(psi)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[i],\n",
    "    outputs=[concatenate([phi,psi],  \n",
    "                         axis=1, \n",
    "                         name='main_output')])\n",
    "\n",
    "opt = Adam(lr=0.01)\n",
    "model.compile(optimizer=opt, loss={'main_output':regression})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/10000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 448.4223 - val_loss: 123.2976\n",
      "Epoch 2/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 357.7233 - val_loss: 101.4331\n",
      "Epoch 3/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 296.0807 - val_loss: 86.4069\n",
      "Epoch 4/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 254.0238 - val_loss: 75.8507\n",
      "Epoch 5/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 225.7362 - val_loss: 68.1678\n",
      "Epoch 6/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 202.3294 - val_loss: 62.4457\n",
      "Epoch 7/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 187.5131 - val_loss: 58.0441\n",
      "Epoch 8/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 174.4111 - val_loss: 54.6092\n",
      "Epoch 9/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 163.5509 - val_loss: 51.8793\n",
      "Epoch 10/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 155.9248 - val_loss: 49.6516\n",
      "Epoch 11/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 150.1617 - val_loss: 47.7964\n",
      "Epoch 12/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 144.4611 - val_loss: 46.2352\n",
      "Epoch 13/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 139.7783 - val_loss: 44.8978\n",
      "Epoch 14/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 135.9324 - val_loss: 43.7400\n",
      "Epoch 15/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 132.0504 - val_loss: 42.7254\n",
      "Epoch 16/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 129.1298 - val_loss: 41.8269\n",
      "Epoch 17/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 126.8831 - val_loss: 41.0200\n",
      "Epoch 18/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 123.9938 - val_loss: 40.2951\n",
      "Epoch 19/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 122.6187 - val_loss: 39.6326\n",
      "Epoch 20/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 120.2137 - val_loss: 39.0292\n",
      "Epoch 21/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 118.3993 - val_loss: 38.4734\n",
      "Epoch 22/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 116.2701 - val_loss: 37.9600\n",
      "Epoch 23/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 114.9230 - val_loss: 37.4821\n",
      "Epoch 24/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 113.4859 - val_loss: 37.0356\n",
      "Epoch 25/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 112.3958 - val_loss: 36.6159\n",
      "Epoch 26/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 110.8553 - val_loss: 36.2229\n",
      "Epoch 27/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 110.2532 - val_loss: 35.8503\n",
      "Epoch 28/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 108.8243 - val_loss: 35.4995\n",
      "Epoch 29/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 108.0064 - val_loss: 35.1683\n",
      "Epoch 30/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 106.5625 - val_loss: 34.8544\n",
      "Epoch 31/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 105.7525 - val_loss: 34.5563\n",
      "Epoch 32/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 105.2192 - val_loss: 34.2708\n",
      "Epoch 33/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 104.2560 - val_loss: 33.9984\n",
      "Epoch 34/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 103.3278 - val_loss: 33.7396\n",
      "Epoch 35/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 102.9070 - val_loss: 33.4909\n",
      "Epoch 36/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 101.7158 - val_loss: 33.2547\n",
      "Epoch 37/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 100.9173 - val_loss: 33.0289\n",
      "Epoch 38/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 100.0826 - val_loss: 32.8129\n",
      "Epoch 39/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 99.6709 - val_loss: 32.6046\n",
      "Epoch 40/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 99.0467 - val_loss: 32.4038\n",
      "Epoch 41/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 98.6759 - val_loss: 32.2103\n",
      "Epoch 42/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 97.5814 - val_loss: 32.0260\n",
      "Epoch 43/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 97.0641 - val_loss: 31.8481\n",
      "Epoch 44/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 96.9586 - val_loss: 31.6749\n",
      "Epoch 45/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 96.1192 - val_loss: 31.5094\n",
      "Epoch 46/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 95.6964 - val_loss: 31.3487\n",
      "Epoch 47/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 95.4673 - val_loss: 31.1931\n",
      "Epoch 48/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 94.8955 - val_loss: 31.0440\n",
      "Epoch 49/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 93.9554 - val_loss: 30.9006\n",
      "Epoch 50/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 93.7364 - val_loss: 30.7616\n",
      "Epoch 51/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 93.5064 - val_loss: 30.6261\n",
      "Epoch 52/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 93.2074 - val_loss: 30.4946\n",
      "Epoch 53/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 92.5784 - val_loss: 30.3682\n",
      "Epoch 54/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 92.5013 - val_loss: 30.2445\n",
      "Epoch 55/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 91.9702 - val_loss: 30.1250\n",
      "Epoch 56/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 91.8205 - val_loss: 30.0082\n",
      "Epoch 57/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 91.4443 - val_loss: 29.8956\n",
      "Epoch 58/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 91.0236 - val_loss: 29.7865\n",
      "Epoch 59/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 90.0962 - val_loss: 29.6816\n",
      "Epoch 60/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 90.1693 - val_loss: 29.5782\n",
      "Epoch 61/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 89.9312 - val_loss: 29.4778\n",
      "Epoch 62/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 89.7454 - val_loss: 29.3802\n",
      "Epoch 63/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 89.1219 - val_loss: 29.2855\n",
      "Epoch 64/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 89.1353 - val_loss: 29.1921\n",
      "Epoch 65/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 88.7727 - val_loss: 29.1021\n",
      "Epoch 66/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 88.3558 - val_loss: 29.0144\n",
      "Epoch 67/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 88.0927 - val_loss: 28.9286\n",
      "Epoch 68/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 87.9048 - val_loss: 28.8453\n",
      "Epoch 69/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 87.6338 - val_loss: 28.7638\n",
      "Epoch 70/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 87.4670 - val_loss: 28.6844\n",
      "Epoch 71/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 87.3161 - val_loss: 28.6062\n",
      "Epoch 72/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 86.8252 - val_loss: 28.5304\n",
      "Epoch 73/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 86.5467 - val_loss: 28.4568\n",
      "Epoch 74/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 86.2926 - val_loss: 28.3847\n",
      "Epoch 75/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 86.0303 - val_loss: 28.3144\n",
      "Epoch 76/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 86.1181 - val_loss: 28.2444\n",
      "Epoch 77/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 85.7841 - val_loss: 28.1770\n",
      "Epoch 78/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 85.6051 - val_loss: 28.1111\n",
      "Epoch 79/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 85.5141 - val_loss: 28.0461\n",
      "Epoch 80/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 85.1896 - val_loss: 27.9825\n",
      "Epoch 81/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 84.9815 - val_loss: 27.9206\n",
      "Epoch 82/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 84.9277 - val_loss: 27.8596\n",
      "Epoch 83/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 84.3870 - val_loss: 27.8007\n",
      "Epoch 84/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 84.5124 - val_loss: 27.7423\n",
      "Epoch 85/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 84.2674 - val_loss: 27.6853\n",
      "Epoch 86/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 84.1434 - val_loss: 27.6290\n",
      "Epoch 87/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 84.0972 - val_loss: 27.5737\n",
      "Epoch 88/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 83.9080 - val_loss: 27.5198\n",
      "Epoch 89/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 83.4882 - val_loss: 27.4675\n",
      "Epoch 90/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 83.5168 - val_loss: 27.4154\n",
      "Epoch 91/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 83.3217 - val_loss: 27.3646\n",
      "Epoch 92/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 83.0502 - val_loss: 27.3147\n",
      "Epoch 93/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 83.1169 - val_loss: 27.2655\n",
      "Epoch 94/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 83.0785 - val_loss: 27.2169\n",
      "Epoch 95/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 82.8431 - val_loss: 27.1693\n",
      "Epoch 96/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 82.3451 - val_loss: 27.1233\n",
      "Epoch 97/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 82.3994 - val_loss: 27.0776\n",
      "Epoch 98/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 82.2299 - val_loss: 27.0329\n",
      "Epoch 99/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 82.1687 - val_loss: 26.9887\n",
      "Epoch 100/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 82.0047 - val_loss: 26.9454\n",
      "Epoch 101/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 81.9878 - val_loss: 26.9026\n",
      "Epoch 102/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 81.5757 - val_loss: 26.8610\n",
      "Epoch 103/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 81.6298 - val_loss: 26.8191\n",
      "Epoch 104/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 81.6196 - val_loss: 26.7783\n",
      "Epoch 105/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 81.3630 - val_loss: 26.7381\n",
      "Epoch 106/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 81.3130 - val_loss: 26.6986\n",
      "Epoch 107/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 81.0732 - val_loss: 26.6601\n",
      "Epoch 108/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 80.9893 - val_loss: 26.6217\n",
      "Epoch 109/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 81.0761 - val_loss: 26.5838\n",
      "Epoch 110/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 80.7778 - val_loss: 26.5468\n",
      "Epoch 111/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 80.5228 - val_loss: 26.5104\n",
      "Epoch 112/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 80.5055 - val_loss: 26.4743\n",
      "Epoch 113/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 80.5285 - val_loss: 26.4388\n",
      "Epoch 114/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 80.4052 - val_loss: 26.4035\n",
      "Epoch 115/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 80.3085 - val_loss: 26.3688\n",
      "Epoch 116/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 80.2847 - val_loss: 26.3345\n",
      "Epoch 117/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 80.1053 - val_loss: 26.3008\n",
      "Epoch 118/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 80.0690 - val_loss: 26.2675\n",
      "Epoch 119/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 79.9374 - val_loss: 26.2349\n",
      "Epoch 120/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 79.8504 - val_loss: 26.2027\n",
      "Epoch 121/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 79.5666 - val_loss: 26.1709\n",
      "Epoch 122/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 79.5086 - val_loss: 26.1397\n",
      "Epoch 123/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 79.4789 - val_loss: 26.1085\n",
      "Epoch 124/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 79.2794 - val_loss: 26.0779\n",
      "Epoch 125/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 79.1725 - val_loss: 26.0475\n",
      "Epoch 126/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 79.0769 - val_loss: 26.0176\n",
      "Epoch 127/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 79.0032 - val_loss: 25.9880\n",
      "Epoch 128/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 79.0263 - val_loss: 25.9586\n",
      "Epoch 129/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 78.9807 - val_loss: 25.9294\n",
      "Epoch 130/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 78.7850 - val_loss: 25.9007\n",
      "Epoch 131/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 78.8059 - val_loss: 25.8724\n",
      "Epoch 132/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 78.6855 - val_loss: 25.8443\n",
      "Epoch 133/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 78.5806 - val_loss: 25.8167\n",
      "Epoch 134/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 78.3978 - val_loss: 25.7894\n",
      "Epoch 135/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 78.4028 - val_loss: 25.7623\n",
      "Epoch 136/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 78.3002 - val_loss: 25.7354\n",
      "Epoch 137/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 78.1830 - val_loss: 25.7089\n",
      "Epoch 138/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 78.0395 - val_loss: 25.6829\n",
      "Epoch 139/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 77.9987 - val_loss: 25.6569\n",
      "Epoch 140/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 77.9571 - val_loss: 25.6310\n",
      "Epoch 141/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 77.8018 - val_loss: 25.6055\n",
      "Epoch 142/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 77.9201 - val_loss: 25.5801\n",
      "Epoch 143/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 77.6639 - val_loss: 25.5551\n",
      "Epoch 144/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 77.5412 - val_loss: 25.5306\n",
      "Epoch 145/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 77.6380 - val_loss: 25.5060\n",
      "Epoch 146/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 77.3949 - val_loss: 25.4818\n",
      "Epoch 147/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 77.3789 - val_loss: 25.4577\n",
      "Epoch 148/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 77.2616 - val_loss: 25.4339\n",
      "Epoch 149/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 77.2753 - val_loss: 25.4101\n",
      "Epoch 150/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 77.1943 - val_loss: 25.3866\n",
      "Epoch 151/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 77.0998 - val_loss: 25.3633\n",
      "Epoch 152/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 76.9954 - val_loss: 25.3402\n",
      "Epoch 153/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 76.9871 - val_loss: 25.3173\n",
      "Epoch 154/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 104us/step - loss: 77.0050 - val_loss: 25.2944\n",
      "Epoch 155/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 76.8453 - val_loss: 25.2721\n",
      "Epoch 156/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 76.7075 - val_loss: 25.2498\n",
      "Epoch 157/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 76.6292 - val_loss: 25.2276\n",
      "Epoch 158/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 76.5603 - val_loss: 25.2058\n",
      "Epoch 159/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 76.5219 - val_loss: 25.1840\n",
      "Epoch 160/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 76.4771 - val_loss: 25.1623\n",
      "Epoch 161/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 76.4687 - val_loss: 25.1406\n",
      "Epoch 162/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 76.3579 - val_loss: 25.1192\n",
      "Epoch 163/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 76.2951 - val_loss: 25.0980\n",
      "Epoch 164/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 76.3021 - val_loss: 25.0768\n",
      "Epoch 165/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 76.1664 - val_loss: 25.0560\n",
      "Epoch 166/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 76.1453 - val_loss: 25.0351\n",
      "Epoch 167/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 76.0379 - val_loss: 25.0145\n",
      "Epoch 168/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 75.9829 - val_loss: 24.9940\n",
      "Epoch 169/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 75.9937 - val_loss: 24.9735\n",
      "Epoch 170/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 75.8770 - val_loss: 24.9534\n",
      "Epoch 171/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 75.7330 - val_loss: 24.9334\n",
      "Epoch 172/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 75.7852 - val_loss: 24.9133\n",
      "Epoch 173/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 75.7361 - val_loss: 24.8933\n",
      "Epoch 174/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 75.6831 - val_loss: 24.8734\n",
      "Epoch 175/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 75.6936 - val_loss: 24.8537\n",
      "Epoch 176/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 75.6161 - val_loss: 24.8340\n",
      "Epoch 177/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 75.4612 - val_loss: 24.8145\n",
      "Epoch 178/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 75.3210 - val_loss: 24.7954\n",
      "Epoch 179/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 75.2791 - val_loss: 24.7762\n",
      "Epoch 180/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 75.2931 - val_loss: 24.7570\n",
      "Epoch 181/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 75.1662 - val_loss: 24.7380\n",
      "Epoch 182/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 75.1896 - val_loss: 24.7189\n",
      "Epoch 183/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 75.0282 - val_loss: 24.7001\n",
      "Epoch 184/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 75.1629 - val_loss: 24.6810\n",
      "Epoch 185/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 75.0279 - val_loss: 24.6623\n",
      "Epoch 186/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 74.9035 - val_loss: 24.6437\n",
      "Epoch 187/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 74.7933 - val_loss: 24.6253\n",
      "Epoch 188/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 74.9126 - val_loss: 24.6067\n",
      "Epoch 189/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 74.7137 - val_loss: 24.5883\n",
      "Epoch 190/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 74.7413 - val_loss: 24.5699\n",
      "Epoch 191/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 74.5928 - val_loss: 24.5517\n",
      "Epoch 192/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 74.6282 - val_loss: 24.5336\n",
      "Epoch 193/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 74.5798 - val_loss: 24.5155\n",
      "Epoch 194/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 74.4339 - val_loss: 24.4974\n",
      "Epoch 195/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 74.5254 - val_loss: 24.4794\n",
      "Epoch 196/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 74.4539 - val_loss: 24.4613\n",
      "Epoch 197/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 74.2902 - val_loss: 24.4434\n",
      "Epoch 198/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 74.3638 - val_loss: 24.4255\n",
      "Epoch 199/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 74.2125 - val_loss: 24.4078\n",
      "Epoch 200/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 74.0649 - val_loss: 24.3903\n",
      "Epoch 201/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 74.1701 - val_loss: 24.3726\n",
      "Epoch 202/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 74.0596 - val_loss: 24.3549\n",
      "Epoch 203/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 73.9196 - val_loss: 24.3375\n",
      "Epoch 204/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 73.9624 - val_loss: 24.3199\n",
      "Epoch 205/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 73.9253 - val_loss: 24.3023\n",
      "Epoch 206/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 73.8422 - val_loss: 24.2850\n",
      "Epoch 207/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 73.7581 - val_loss: 24.2677\n",
      "Epoch 208/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 73.7656 - val_loss: 24.2503\n",
      "Epoch 209/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 73.6530 - val_loss: 24.2330\n",
      "Epoch 210/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 73.6580 - val_loss: 24.2157\n",
      "Epoch 211/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 73.6483 - val_loss: 24.1985\n",
      "Epoch 212/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 73.6366 - val_loss: 24.1812\n",
      "Epoch 213/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 73.4734 - val_loss: 24.1640\n",
      "Epoch 214/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 73.4477 - val_loss: 24.1469\n",
      "Epoch 215/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 73.4042 - val_loss: 24.1298\n",
      "Epoch 216/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 73.3230 - val_loss: 24.1127\n",
      "Epoch 217/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 73.1807 - val_loss: 24.0958\n",
      "Epoch 218/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 73.1523 - val_loss: 24.0788\n",
      "Epoch 219/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 73.1619 - val_loss: 24.0619\n",
      "Epoch 220/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 73.0987 - val_loss: 24.0449\n",
      "Epoch 221/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 73.0445 - val_loss: 24.0280\n",
      "Epoch 222/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 72.9163 - val_loss: 24.0111\n",
      "Epoch 223/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 73.0262 - val_loss: 23.9940\n",
      "Epoch 224/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 73.0020 - val_loss: 23.9770\n",
      "Epoch 225/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 72.8840 - val_loss: 23.9602\n",
      "Epoch 226/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 72.9448 - val_loss: 23.9432\n",
      "Epoch 227/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 72.6605 - val_loss: 23.9264\n",
      "Epoch 228/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 72.6392 - val_loss: 23.9096\n",
      "Epoch 229/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 72.6215 - val_loss: 23.8928\n",
      "Epoch 230/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 72.6507 - val_loss: 23.8759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 72.4444 - val_loss: 23.8591\n",
      "Epoch 232/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 72.5482 - val_loss: 23.8422\n",
      "Epoch 233/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 72.3359 - val_loss: 23.8256\n",
      "Epoch 234/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 72.4842 - val_loss: 23.8086\n",
      "Epoch 235/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 72.2587 - val_loss: 23.7920\n",
      "Epoch 236/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 72.3870 - val_loss: 23.7751\n",
      "Epoch 237/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 72.3369 - val_loss: 23.7582\n",
      "Epoch 238/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 72.1874 - val_loss: 23.7415\n",
      "Epoch 239/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 72.1459 - val_loss: 23.7247\n",
      "Epoch 240/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 72.0617 - val_loss: 23.7079\n",
      "Epoch 241/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 72.0232 - val_loss: 23.6911\n",
      "Epoch 242/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 72.0058 - val_loss: 23.6743\n",
      "Epoch 243/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 71.9211 - val_loss: 23.6574\n",
      "Epoch 244/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 71.8791 - val_loss: 23.6406\n",
      "Epoch 245/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 71.8162 - val_loss: 23.6238\n",
      "Epoch 246/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 71.8160 - val_loss: 23.6069\n",
      "Epoch 247/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 71.7025 - val_loss: 23.5901\n",
      "Epoch 248/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 71.7390 - val_loss: 23.5732\n",
      "Epoch 249/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 71.5698 - val_loss: 23.5563\n",
      "Epoch 250/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 71.6175 - val_loss: 23.5393\n",
      "Epoch 251/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 71.4881 - val_loss: 23.5224\n",
      "Epoch 252/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 71.5328 - val_loss: 23.5055\n",
      "Epoch 253/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 71.4229 - val_loss: 23.4885\n",
      "Epoch 254/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 71.4134 - val_loss: 23.4715\n",
      "Epoch 255/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 71.3168 - val_loss: 23.4545\n",
      "Epoch 256/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 71.1563 - val_loss: 23.4376\n",
      "Epoch 257/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 71.2584 - val_loss: 23.4205\n",
      "Epoch 258/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 71.1441 - val_loss: 23.4035\n",
      "Epoch 259/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 71.1302 - val_loss: 23.3864\n",
      "Epoch 260/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 71.0893 - val_loss: 23.3692\n",
      "Epoch 261/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 71.0309 - val_loss: 23.3521\n",
      "Epoch 262/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 70.9221 - val_loss: 23.3349\n",
      "Epoch 263/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 70.9059 - val_loss: 23.3178\n",
      "Epoch 264/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 70.7990 - val_loss: 23.3006\n",
      "Epoch 265/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 70.7817 - val_loss: 23.2834\n",
      "Epoch 266/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 70.7670 - val_loss: 23.2660\n",
      "Epoch 267/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 70.8327 - val_loss: 23.2486\n",
      "Epoch 268/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 70.6793 - val_loss: 23.2312\n",
      "Epoch 269/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 70.6390 - val_loss: 23.2138\n",
      "Epoch 270/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 70.4886 - val_loss: 23.1964\n",
      "Epoch 271/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 70.4936 - val_loss: 23.1789\n",
      "Epoch 272/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 70.5533 - val_loss: 23.1613\n",
      "Epoch 273/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 70.4214 - val_loss: 23.1439\n",
      "Epoch 274/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 70.3132 - val_loss: 23.1264\n",
      "Epoch 275/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 70.2238 - val_loss: 23.1087\n",
      "Epoch 276/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 70.2903 - val_loss: 23.0911\n",
      "Epoch 277/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 70.1758 - val_loss: 23.0733\n",
      "Epoch 278/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 70.1514 - val_loss: 23.0556\n",
      "Epoch 279/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 70.0862 - val_loss: 23.0379\n",
      "Epoch 280/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 70.0570 - val_loss: 23.0199\n",
      "Epoch 281/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 70.0080 - val_loss: 23.0018\n",
      "Epoch 282/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 69.9505 - val_loss: 22.9839\n",
      "Epoch 283/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 69.8345 - val_loss: 22.9659\n",
      "Epoch 284/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 69.8419 - val_loss: 22.9479\n",
      "Epoch 285/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 69.8033 - val_loss: 22.9298\n",
      "Epoch 286/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 69.7677 - val_loss: 22.9116\n",
      "Epoch 287/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 69.6406 - val_loss: 22.8935\n",
      "Epoch 288/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 69.5209 - val_loss: 22.8752\n",
      "Epoch 289/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 69.5009 - val_loss: 22.8568\n",
      "Epoch 290/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 69.4878 - val_loss: 22.8384\n",
      "Epoch 291/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 69.3821 - val_loss: 22.8199\n",
      "Epoch 292/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 69.3746 - val_loss: 22.8013\n",
      "Epoch 293/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 69.3387 - val_loss: 22.7827\n",
      "Epoch 294/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 69.1762 - val_loss: 22.7641\n",
      "Epoch 295/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 69.1699 - val_loss: 22.7453\n",
      "Epoch 296/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 69.1600 - val_loss: 22.7266\n",
      "Epoch 297/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 69.0208 - val_loss: 22.7078\n",
      "Epoch 298/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 69.0882 - val_loss: 22.6888\n",
      "Epoch 299/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 68.9924 - val_loss: 22.6698\n",
      "Epoch 300/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 68.9476 - val_loss: 22.6507\n",
      "Epoch 301/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 68.9350 - val_loss: 22.6315\n",
      "Epoch 302/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 68.8258 - val_loss: 22.6123\n",
      "Epoch 303/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 68.7603 - val_loss: 22.5929\n",
      "Epoch 304/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 68.6416 - val_loss: 22.5736\n",
      "Epoch 305/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 68.6998 - val_loss: 22.5541\n",
      "Epoch 306/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 68.5327 - val_loss: 22.5346\n",
      "Epoch 307/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 68.5099 - val_loss: 22.5150\n",
      "Epoch 308/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 68.3765 - val_loss: 22.4954\n",
      "Epoch 309/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 68.3751 - val_loss: 22.4755\n",
      "Epoch 310/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 68.3103 - val_loss: 22.4556\n",
      "Epoch 311/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 68.2869 - val_loss: 22.4356\n",
      "Epoch 312/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 68.1930 - val_loss: 22.4156\n",
      "Epoch 313/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 68.1522 - val_loss: 22.3955\n",
      "Epoch 314/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 68.1263 - val_loss: 22.3752\n",
      "Epoch 315/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 68.0953 - val_loss: 22.3548\n",
      "Epoch 316/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 68.0046 - val_loss: 22.3343\n",
      "Epoch 317/10000\n",
      "90/90 [==============================] - 0s 179us/step - loss: 67.9667 - val_loss: 22.3138\n",
      "Epoch 318/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 67.9055 - val_loss: 22.2932\n",
      "Epoch 319/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 67.7343 - val_loss: 22.2725\n",
      "Epoch 320/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 67.7552 - val_loss: 22.2516\n",
      "Epoch 321/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 67.6582 - val_loss: 22.2308\n",
      "Epoch 322/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 67.5926 - val_loss: 22.2096\n",
      "Epoch 323/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 67.4958 - val_loss: 22.1885\n",
      "Epoch 324/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 67.5745 - val_loss: 22.1671\n",
      "Epoch 325/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 67.4967 - val_loss: 22.1458\n",
      "Epoch 326/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 67.3319 - val_loss: 22.1243\n",
      "Epoch 327/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 67.2433 - val_loss: 22.1027\n",
      "Epoch 328/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 67.2078 - val_loss: 22.0809\n",
      "Epoch 329/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 67.1217 - val_loss: 22.0590\n",
      "Epoch 330/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 67.1315 - val_loss: 22.0371\n",
      "Epoch 331/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 67.1021 - val_loss: 22.0150\n",
      "Epoch 332/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 66.9364 - val_loss: 21.9928\n",
      "Epoch 333/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 66.8695 - val_loss: 21.9704\n",
      "Epoch 334/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 66.7798 - val_loss: 21.9479\n",
      "Epoch 335/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 66.7775 - val_loss: 21.9253\n",
      "Epoch 336/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 66.6967 - val_loss: 21.9025\n",
      "Epoch 337/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 69.73 - 0s 125us/step - loss: 66.6227 - val_loss: 21.8796\n",
      "Epoch 338/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 66.6732 - val_loss: 21.8565\n",
      "Epoch 339/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 66.5328 - val_loss: 21.8333\n",
      "Epoch 340/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 66.4123 - val_loss: 21.8100\n",
      "Epoch 341/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 66.4134 - val_loss: 21.7866\n",
      "Epoch 342/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 66.3544 - val_loss: 21.7629\n",
      "Epoch 343/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 66.1958 - val_loss: 21.7391\n",
      "Epoch 344/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 66.1001 - val_loss: 21.7152\n",
      "Epoch 345/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 66.0517 - val_loss: 21.6910\n",
      "Epoch 346/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 66.0757 - val_loss: 21.6668\n",
      "Epoch 347/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 65.8834 - val_loss: 21.6424\n",
      "Epoch 348/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 65.8994 - val_loss: 21.6178\n",
      "Epoch 349/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 65.8026 - val_loss: 21.5931\n",
      "Epoch 350/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 65.7253 - val_loss: 21.5681\n",
      "Epoch 351/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 65.5649 - val_loss: 21.5429\n",
      "Epoch 352/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 65.6199 - val_loss: 21.5176\n",
      "Epoch 353/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 65.4765 - val_loss: 21.4921\n",
      "Epoch 354/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 65.4720 - val_loss: 21.4665\n",
      "Epoch 355/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 65.3531 - val_loss: 21.4407\n",
      "Epoch 356/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 65.2689 - val_loss: 21.4146\n",
      "Epoch 357/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 65.1694 - val_loss: 21.3885\n",
      "Epoch 358/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 65.1604 - val_loss: 21.3620\n",
      "Epoch 359/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 65.1032 - val_loss: 21.3354\n",
      "Epoch 360/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 65.0147 - val_loss: 21.3086\n",
      "Epoch 361/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 64.9217 - val_loss: 21.2817\n",
      "Epoch 362/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 64.8279 - val_loss: 21.2545\n",
      "Epoch 363/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 64.7589 - val_loss: 21.2270\n",
      "Epoch 364/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 64.6494 - val_loss: 21.1994\n",
      "Epoch 365/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 64.5605 - val_loss: 21.1714\n",
      "Epoch 366/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 64.5014 - val_loss: 21.1434\n",
      "Epoch 367/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 64.4345 - val_loss: 21.1150\n",
      "Epoch 368/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 64.3562 - val_loss: 21.0864\n",
      "Epoch 369/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 64.3149 - val_loss: 21.0577\n",
      "Epoch 370/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 64.2744 - val_loss: 21.0286\n",
      "Epoch 371/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 64.0802 - val_loss: 20.9993\n",
      "Epoch 372/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 64.0472 - val_loss: 20.9699\n",
      "Epoch 373/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 63.9767 - val_loss: 20.9403\n",
      "Epoch 374/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 63.9707 - val_loss: 20.9104\n",
      "Epoch 375/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 63.6659 - val_loss: 20.8801\n",
      "Epoch 376/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 63.6751 - val_loss: 20.8496\n",
      "Epoch 377/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 63.6312 - val_loss: 20.8188\n",
      "Epoch 378/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 63.3590 - val_loss: 20.7877\n",
      "Epoch 379/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 63.3717 - val_loss: 20.7564\n",
      "Epoch 380/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 63.3057 - val_loss: 20.7248\n",
      "Epoch 381/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 63.2135 - val_loss: 20.6930\n",
      "Epoch 382/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 63.1032 - val_loss: 20.6610\n",
      "Epoch 383/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 103us/step - loss: 63.0481 - val_loss: 20.6285\n",
      "Epoch 384/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 63.0416 - val_loss: 20.5960\n",
      "Epoch 385/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 62.9038 - val_loss: 20.5631\n",
      "Epoch 386/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 62.7464 - val_loss: 20.5298\n",
      "Epoch 387/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 62.5777 - val_loss: 20.4962\n",
      "Epoch 388/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 62.5734 - val_loss: 20.4624\n",
      "Epoch 389/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 62.4887 - val_loss: 20.4284\n",
      "Epoch 390/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 62.3078 - val_loss: 20.3938\n",
      "Epoch 391/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 62.2963 - val_loss: 20.3591\n",
      "Epoch 392/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 62.2156 - val_loss: 20.3240\n",
      "Epoch 393/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 62.0848 - val_loss: 20.2886\n",
      "Epoch 394/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 61.9805 - val_loss: 20.2527\n",
      "Epoch 395/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 61.8267 - val_loss: 20.2166\n",
      "Epoch 396/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 61.7110 - val_loss: 20.1802\n",
      "Epoch 397/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 61.6340 - val_loss: 20.1435\n",
      "Epoch 398/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 61.6287 - val_loss: 20.1065\n",
      "Epoch 399/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 61.4782 - val_loss: 20.0691\n",
      "Epoch 400/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 61.3211 - val_loss: 20.0313\n",
      "Epoch 401/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 61.2478 - val_loss: 19.9932\n",
      "Epoch 402/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 61.1125 - val_loss: 19.9546\n",
      "Epoch 403/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 60.9483 - val_loss: 19.9155\n",
      "Epoch 404/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 60.8478 - val_loss: 19.8761\n",
      "Epoch 405/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 60.8084 - val_loss: 19.8363\n",
      "Epoch 406/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 60.6655 - val_loss: 19.7963\n",
      "Epoch 407/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 60.5650 - val_loss: 19.7558\n",
      "Epoch 408/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 60.4586 - val_loss: 19.7148\n",
      "Epoch 409/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 60.3332 - val_loss: 19.6734\n",
      "Epoch 410/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 60.1658 - val_loss: 19.6316\n",
      "Epoch 411/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 60.1552 - val_loss: 19.5895\n",
      "Epoch 412/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 59.9926 - val_loss: 19.5468\n",
      "Epoch 413/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 59.8346 - val_loss: 19.5035\n",
      "Epoch 414/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 59.8046 - val_loss: 19.4598\n",
      "Epoch 415/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 59.5870 - val_loss: 19.4155\n",
      "Epoch 416/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 59.5430 - val_loss: 19.3708\n",
      "Epoch 417/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 59.2092 - val_loss: 19.3253\n",
      "Epoch 418/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 59.2657 - val_loss: 19.2797\n",
      "Epoch 419/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 59.0941 - val_loss: 19.2332\n",
      "Epoch 420/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 58.9685 - val_loss: 19.1862\n",
      "Epoch 421/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 58.8795 - val_loss: 19.1386\n",
      "Epoch 422/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 58.6653 - val_loss: 19.0899\n",
      "Epoch 423/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 58.5688 - val_loss: 19.0410\n",
      "Epoch 424/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 58.3649 - val_loss: 18.9911\n",
      "Epoch 425/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 58.2490 - val_loss: 18.9405\n",
      "Epoch 426/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 58.2195 - val_loss: 18.8895\n",
      "Epoch 427/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 57.9585 - val_loss: 18.8372\n",
      "Epoch 428/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 57.7719 - val_loss: 18.7840\n",
      "Epoch 429/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 57.7183 - val_loss: 18.7302\n",
      "Epoch 430/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 57.5288 - val_loss: 18.6752\n",
      "Epoch 431/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 57.3554 - val_loss: 18.6194\n",
      "Epoch 432/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 57.1171 - val_loss: 18.5623\n",
      "Epoch 433/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 56.9693 - val_loss: 18.5043\n",
      "Epoch 434/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 56.8616 - val_loss: 18.4454\n",
      "Epoch 435/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 56.6974 - val_loss: 18.3857\n",
      "Epoch 436/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 56.4813 - val_loss: 18.3251\n",
      "Epoch 437/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 56.3929 - val_loss: 18.2643\n",
      "Epoch 438/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 56.1601 - val_loss: 18.2027\n",
      "Epoch 439/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 55.9751 - val_loss: 18.1408\n",
      "Epoch 440/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 55.7607 - val_loss: 18.0790\n",
      "Epoch 441/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 55.7135 - val_loss: 18.0171\n",
      "Epoch 442/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 55.3023 - val_loss: 17.9552\n",
      "Epoch 443/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 55.2079 - val_loss: 17.8932\n",
      "Epoch 444/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 55.0277 - val_loss: 17.8314\n",
      "Epoch 445/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 54.7854 - val_loss: 17.7690\n",
      "Epoch 446/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 54.6679 - val_loss: 17.7066\n",
      "Epoch 447/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 54.5437 - val_loss: 17.6443\n",
      "Epoch 448/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 54.2395 - val_loss: 17.5812\n",
      "Epoch 449/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 54.0701 - val_loss: 17.5178\n",
      "Epoch 450/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 53.9084 - val_loss: 17.4542\n",
      "Epoch 451/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 53.6851 - val_loss: 17.3900\n",
      "Epoch 452/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 53.3777 - val_loss: 17.3252\n",
      "Epoch 453/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 53.4678 - val_loss: 17.2602\n",
      "Epoch 454/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 53.2166 - val_loss: 17.1945\n",
      "Epoch 455/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 52.9511 - val_loss: 17.1283\n",
      "Epoch 456/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 52.7724 - val_loss: 17.0611\n",
      "Epoch 457/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 52.6590 - val_loss: 16.9941\n",
      "Epoch 458/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 52.2879 - val_loss: 16.9263\n",
      "Epoch 459/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 52.1155 - val_loss: 16.8574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 51.9208 - val_loss: 16.7883\n",
      "Epoch 461/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 51.8089 - val_loss: 16.7184\n",
      "Epoch 462/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 51.5840 - val_loss: 16.6480\n",
      "Epoch 463/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 51.3220 - val_loss: 16.5769\n",
      "Epoch 464/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 51.2194 - val_loss: 16.5052\n",
      "Epoch 465/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 50.8453 - val_loss: 16.4326\n",
      "Epoch 466/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 50.6125 - val_loss: 16.3589\n",
      "Epoch 467/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 50.4188 - val_loss: 16.2846\n",
      "Epoch 468/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 50.0986 - val_loss: 16.2097\n",
      "Epoch 469/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 50.1386 - val_loss: 16.1332\n",
      "Epoch 470/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 49.8196 - val_loss: 16.0561\n",
      "Epoch 471/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 49.5367 - val_loss: 15.9786\n",
      "Epoch 472/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 49.1946 - val_loss: 15.8999\n",
      "Epoch 473/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 49.1406 - val_loss: 15.8199\n",
      "Epoch 474/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 48.9393 - val_loss: 15.7391\n",
      "Epoch 475/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 48.6114 - val_loss: 15.6573\n",
      "Epoch 476/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 48.3685 - val_loss: 15.5745\n",
      "Epoch 477/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 48.1523 - val_loss: 15.4905\n",
      "Epoch 478/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 47.7490 - val_loss: 15.4054\n",
      "Epoch 479/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 47.6392 - val_loss: 15.3197\n",
      "Epoch 480/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 47.3320 - val_loss: 15.2330\n",
      "Epoch 481/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 47.1145 - val_loss: 15.1451\n",
      "Epoch 482/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 46.9213 - val_loss: 15.0556\n",
      "Epoch 483/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 46.5300 - val_loss: 14.9652\n",
      "Epoch 484/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 46.3749 - val_loss: 14.8736\n",
      "Epoch 485/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 46.0944 - val_loss: 14.7805\n",
      "Epoch 486/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 45.7170 - val_loss: 14.6865\n",
      "Epoch 487/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 45.4951 - val_loss: 14.5915\n",
      "Epoch 488/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 45.3399 - val_loss: 14.4947\n",
      "Epoch 489/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 44.9569 - val_loss: 14.3952\n",
      "Epoch 490/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 44.6564 - val_loss: 14.2959\n",
      "Epoch 491/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 44.3506 - val_loss: 14.1940\n",
      "Epoch 492/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 44.0822 - val_loss: 14.0907\n",
      "Epoch 493/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 43.6243 - val_loss: 13.9864\n",
      "Epoch 494/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 43.3693 - val_loss: 13.8793\n",
      "Epoch 495/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 43.0835 - val_loss: 13.7721\n",
      "Epoch 496/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 42.5706 - val_loss: 13.6613\n",
      "Epoch 497/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 42.5337 - val_loss: 13.5495\n",
      "Epoch 498/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 42.0749 - val_loss: 13.4351\n",
      "Epoch 499/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 41.7720 - val_loss: 13.3186\n",
      "Epoch 500/10000\n",
      "90/90 [==============================] - 0s 223us/step - loss: 41.3889 - val_loss: 13.2010\n",
      "Epoch 501/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 41.0311 - val_loss: 13.0820\n",
      "Epoch 502/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 40.7053 - val_loss: 12.9597\n",
      "Epoch 503/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 40.4707 - val_loss: 12.8350\n",
      "Epoch 504/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 40.0734 - val_loss: 12.7086\n",
      "Epoch 505/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 39.4003 - val_loss: 12.5785\n",
      "Epoch 506/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 39.1714 - val_loss: 12.4459\n",
      "Epoch 507/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 38.7291 - val_loss: 12.3121\n",
      "Epoch 508/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 38.3184 - val_loss: 12.1741\n",
      "Epoch 509/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 38.0494 - val_loss: 12.0330\n",
      "Epoch 510/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 37.6793 - val_loss: 11.8903\n",
      "Epoch 511/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 37.0420 - val_loss: 11.7432\n",
      "Epoch 512/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 36.6519 - val_loss: 11.5934\n",
      "Epoch 513/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 36.1350 - val_loss: 11.4414\n",
      "Epoch 514/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 35.8688 - val_loss: 11.2860\n",
      "Epoch 515/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 35.4479 - val_loss: 11.1261\n",
      "Epoch 516/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 35.0022 - val_loss: 10.9658\n",
      "Epoch 517/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 34.4651 - val_loss: 10.8037\n",
      "Epoch 518/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 34.0380 - val_loss: 10.6360\n",
      "Epoch 519/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 33.4010 - val_loss: 10.4668\n",
      "Epoch 520/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 32.9025 - val_loss: 10.2915\n",
      "Epoch 521/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 32.4330 - val_loss: 10.1144\n",
      "Epoch 522/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 31.9469 - val_loss: 9.9353\n",
      "Epoch 523/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 31.4239 - val_loss: 9.7508\n",
      "Epoch 524/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 30.8478 - val_loss: 9.5606\n",
      "Epoch 525/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 30.4211 - val_loss: 9.3735\n",
      "Epoch 526/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 29.8694 - val_loss: 9.1825\n",
      "Epoch 527/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 29.3253 - val_loss: 8.9894\n",
      "Epoch 528/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 28.7106 - val_loss: 8.7898\n",
      "Epoch 529/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 28.1290 - val_loss: 8.5849\n",
      "Epoch 530/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 27.2664 - val_loss: 8.3900\n",
      "Epoch 531/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 26.7488 - val_loss: 8.1763\n",
      "Epoch 532/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 25.8089 - val_loss: 7.9781\n",
      "Epoch 533/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 25.8755 - val_loss: 7.7731\n",
      "Epoch 534/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 25.0060 - val_loss: 7.5587\n",
      "Epoch 535/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 24.3527 - val_loss: 7.3453\n",
      "Epoch 536/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 23.4694 - val_loss: 7.1459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 537/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 23.2578 - val_loss: 6.9449\n",
      "Epoch 538/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 22.4744 - val_loss: 6.7494\n",
      "Epoch 539/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 21.8085 - val_loss: 6.5664\n",
      "Epoch 540/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 21.1563 - val_loss: 6.3900\n",
      "Epoch 541/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 20.5654 - val_loss: 6.2169\n",
      "Epoch 542/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 20.6270 - val_loss: 6.0478\n",
      "Epoch 543/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 19.9950 - val_loss: 5.8924\n",
      "Epoch 544/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 19.1861 - val_loss: 5.7496\n",
      "Epoch 545/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 18.8667 - val_loss: 5.6330\n",
      "Epoch 546/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 18.6328 - val_loss: 5.4994\n",
      "Epoch 547/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 18.3831 - val_loss: 5.4187\n",
      "Epoch 548/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 18.2253 - val_loss: 5.3390\n",
      "Epoch 549/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 17.5895 - val_loss: 5.2716\n",
      "Epoch 550/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 17.3817 - val_loss: 5.2225\n",
      "Epoch 551/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 17.2997 - val_loss: 5.1812\n",
      "Epoch 552/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 17.5575 - val_loss: 5.1650\n",
      "Epoch 553/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.6782 - val_loss: 5.1629\n",
      "Epoch 554/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.6269 - val_loss: 5.1696\n",
      "Epoch 555/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.5730 - val_loss: 5.1417\n",
      "Epoch 556/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.1883 - val_loss: 5.1723\n",
      "Epoch 557/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.9032 - val_loss: 5.1499\n",
      "Epoch 558/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 16.7505 - val_loss: 5.1486\n",
      "Epoch 559/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.2164 - val_loss: 5.1620\n",
      "Epoch 560/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.6360 - val_loss: 5.1736\n",
      "Epoch 561/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 16.4535 - val_loss: 5.1831\n",
      "Epoch 562/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 16.3224 - val_loss: 5.2044\n",
      "Epoch 563/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.8183 - val_loss: 5.2201\n",
      "Epoch 564/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.5106 - val_loss: 5.2373\n",
      "Epoch 565/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.7414 - val_loss: 5.2423\n",
      "Epoch 566/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.1854 - val_loss: 5.2534\n",
      "Epoch 567/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.7009 - val_loss: 5.2565\n",
      "Epoch 568/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.6299 - val_loss: 5.2616\n",
      "Epoch 569/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.5122 - val_loss: 5.2556\n",
      "Epoch 570/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.8132 - val_loss: 5.2536\n",
      "Epoch 571/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 16.8130 - val_loss: 5.2410\n",
      "Epoch 572/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 17.0660 - val_loss: 5.2309\n",
      "Epoch 573/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 16.0962 - val_loss: 5.2465\n",
      "Epoch 574/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.3339 - val_loss: 5.2416\n",
      "Epoch 575/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 16.9578 - val_loss: 5.2434\n",
      "Epoch 576/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.7425 - val_loss: 5.2516\n",
      "Epoch 577/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 16.6760 - val_loss: 5.2579\n",
      "Epoch 578/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.3796 - val_loss: 5.2409\n",
      "Epoch 579/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.8878 - val_loss: 5.2537\n",
      "Epoch 580/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.3209 - val_loss: 5.2631\n",
      "Epoch 581/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.8568 - val_loss: 5.2620\n",
      "Epoch 582/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 16.6836 - val_loss: 5.2432\n",
      "Epoch 583/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.6079 - val_loss: 5.2440\n",
      "Epoch 584/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.9168 - val_loss: 5.2617\n",
      "Epoch 585/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.2809 - val_loss: 5.2722\n",
      "Epoch 586/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.5678 - val_loss: 5.2479\n",
      "Epoch 587/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.0968 - val_loss: 5.2548\n",
      "Epoch 588/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 16.6650 - val_loss: 5.2434\n",
      "Epoch 589/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.9583 - val_loss: 5.2297\n",
      "Epoch 590/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.2753 - val_loss: 5.2410\n",
      "Epoch 591/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 16.0386 - val_loss: 5.2530\n",
      "Epoch 592/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 17.0304 - val_loss: 5.2450\n",
      "Epoch 593/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 16.4220 - val_loss: 5.2477\n",
      "Epoch 594/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.7938 - val_loss: 5.2377\n",
      "Epoch 595/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.5949 - val_loss: 5.2328\n",
      "Epoch 596/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.8552 - val_loss: 5.2281\n",
      "Epoch 597/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.6153 - val_loss: 5.2316\n",
      "Epoch 598/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.4562 - val_loss: 5.2370\n",
      "Epoch 599/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.3212 - val_loss: 5.2473\n",
      "Epoch 600/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.6421 - val_loss: 5.2296\n",
      "Epoch 601/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 16.5002 - val_loss: 5.2331\n",
      "Epoch 602/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.3380 - val_loss: 5.2535\n",
      "Epoch 603/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.6122 - val_loss: 5.2432\n",
      "Epoch 604/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.7433 - val_loss: 5.2287\n",
      "Epoch 605/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.0850 - val_loss: 5.2304\n",
      "Epoch 606/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 16.0650 - val_loss: 5.2440\n",
      "Epoch 607/10000\n",
      "90/90 [==============================] - 0s 77us/step - loss: 16.6077 - val_loss: 5.2364\n",
      "Epoch 608/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 16.2811 - val_loss: 5.2365\n",
      "Epoch 609/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.4855 - val_loss: 5.2318\n",
      "Epoch 610/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.4070 - val_loss: 5.2465\n",
      "Epoch 611/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 16.5672 - val_loss: 5.2569\n",
      "Epoch 612/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 16.8444 - val_loss: 5.2499\n",
      "Epoch 613/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.0041 - val_loss: 5.2455\n",
      "Epoch 614/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.3396 - val_loss: 5.2345\n",
      "Epoch 615/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 16.9435 - val_loss: 5.2262\n",
      "Epoch 616/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 16.5764 - val_loss: 5.2349\n",
      "Epoch 617/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.6128 - val_loss: 5.2262\n",
      "Epoch 618/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.6473 - val_loss: 5.2232\n",
      "Epoch 619/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.7143 - val_loss: 5.2183\n",
      "Epoch 620/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 16.5672 - val_loss: 5.2121\n",
      "Epoch 621/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 16.0902 - val_loss: 5.2265\n",
      "Epoch 622/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 16.5032 - val_loss: 5.2255\n",
      "Epoch 623/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 16.6084 - val_loss: 5.2257\n",
      "Epoch 624/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 16.6592 - val_loss: 5.2254\n",
      "Epoch 625/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 16.4311 - val_loss: 5.2144\n",
      "Epoch 626/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 16.3318 - val_loss: 5.2284\n",
      "Epoch 627/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 16.5173 - val_loss: 5.2291\n",
      "Epoch 628/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 16.2984 - val_loss: 5.2453\n",
      "Epoch 629/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 16.8559 - val_loss: 5.2365\n",
      "Epoch 630/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 16.2582 - val_loss: 5.2163\n",
      "Epoch 631/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.0301 - val_loss: 5.2228\n",
      "Epoch 632/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 16.2947 - val_loss: 5.2255\n",
      "Epoch 633/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.2187 - val_loss: 5.2320\n",
      "Epoch 634/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 16.2865 - val_loss: 5.2432\n",
      "Epoch 635/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 16.6956 - val_loss: 5.2276\n",
      "Epoch 636/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 16.5343 - val_loss: 5.2116\n",
      "Epoch 637/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 16.5022 - val_loss: 5.2153\n",
      "Epoch 638/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 16.2444 - val_loss: 5.2308\n",
      "Epoch 639/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.6400 - val_loss: 5.2213\n",
      "Epoch 640/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.9856 - val_loss: 5.2074\n",
      "Epoch 641/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.4781 - val_loss: 5.2046\n",
      "Epoch 642/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.4585 - val_loss: 5.1974\n",
      "Epoch 643/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 16.4059 - val_loss: 5.2128\n",
      "Epoch 644/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.2060 - val_loss: 5.2254\n",
      "Epoch 645/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.1449 - val_loss: 5.2351\n",
      "Epoch 646/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 15.5201 - val_loss: 5.2506\n",
      "Epoch 647/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.0787 - val_loss: 5.2246\n",
      "Epoch 648/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 16.0712 - val_loss: 5.2064\n",
      "Epoch 649/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.2222 - val_loss: 5.1894\n",
      "Epoch 650/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.8276 - val_loss: 5.1871\n",
      "Epoch 651/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.5782 - val_loss: 5.1872\n",
      "Epoch 652/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.9768 - val_loss: 5.1962\n",
      "Epoch 653/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.4583 - val_loss: 5.1844\n",
      "Epoch 654/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 16.7902 - val_loss: 5.1915\n",
      "Epoch 655/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.5792 - val_loss: 5.1988\n",
      "Epoch 656/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 16.2847 - val_loss: 5.2152\n",
      "Epoch 657/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.5469 - val_loss: 5.2182\n",
      "Epoch 658/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.5128 - val_loss: 5.2033\n",
      "Epoch 659/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 16.6614 - val_loss: 5.1935\n",
      "Epoch 660/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.5660 - val_loss: 5.2001\n",
      "Epoch 661/10000\n",
      "90/90 [==============================] - 0s 197us/step - loss: 16.2732 - val_loss: 5.1982\n",
      "Epoch 662/10000\n",
      "90/90 [==============================] - 0s 198us/step - loss: 16.3196 - val_loss: 5.1912\n",
      "Epoch 663/10000\n",
      "90/90 [==============================] - 0s 186us/step - loss: 16.5223 - val_loss: 5.2056\n",
      "Epoch 664/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 16.1270 - val_loss: 5.2248\n",
      "Epoch 665/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 15.6682 - val_loss: 5.2380\n",
      "Epoch 666/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.8303 - val_loss: 5.2097\n",
      "Epoch 667/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.5168 - val_loss: 5.2015\n",
      "Epoch 668/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.7849 - val_loss: 5.1942\n",
      "Epoch 669/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 16.2387 - val_loss: 5.1844\n",
      "Epoch 670/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 16.1720 - val_loss: 5.1768\n",
      "Epoch 671/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.4624 - val_loss: 5.1645\n",
      "Epoch 672/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.0775 - val_loss: 5.1755\n",
      "Epoch 673/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.1834 - val_loss: 5.1735\n",
      "Epoch 674/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.2128 - val_loss: 5.1699\n",
      "Epoch 675/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.7625 - val_loss: 5.1678\n",
      "Epoch 676/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.4480 - val_loss: 5.1672\n",
      "Epoch 677/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.5503 - val_loss: 5.1726\n",
      "Epoch 678/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 16.7380 - val_loss: 5.1661\n",
      "Epoch 679/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.1573 - val_loss: 5.1733\n",
      "Epoch 680/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.4794 - val_loss: 5.1735\n",
      "Epoch 681/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.4800 - val_loss: 5.1574\n",
      "Epoch 682/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.6706 - val_loss: 5.1621\n",
      "Epoch 683/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.5209 - val_loss: 5.1570\n",
      "Epoch 684/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.2737 - val_loss: 5.1720\n",
      "Epoch 685/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.5274 - val_loss: 5.1575\n",
      "Epoch 686/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 16.0507 - val_loss: 5.1853\n",
      "Epoch 687/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.4683 - val_loss: 5.1828\n",
      "Epoch 688/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.6328 - val_loss: 5.1676\n",
      "Epoch 689/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.4542 - val_loss: 5.1563\n",
      "Epoch 690/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.4165 - val_loss: 5.1578\n",
      "Epoch 691/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 95us/step - loss: 16.2594 - val_loss: 5.1651\n",
      "Epoch 692/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.5617 - val_loss: 5.1628\n",
      "Epoch 693/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.4448 - val_loss: 5.1399\n",
      "Epoch 694/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.8738 - val_loss: 5.1253\n",
      "Epoch 695/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 16.5373 - val_loss: 5.1364\n",
      "Epoch 696/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.2602 - val_loss: 5.1527\n",
      "Epoch 697/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 16.0117 - val_loss: 5.1721\n",
      "Epoch 698/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 16.7599 - val_loss: 5.1608\n",
      "Epoch 699/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 16.3220 - val_loss: 5.1467\n",
      "Epoch 700/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 15.9926 - val_loss: 5.1638\n",
      "Epoch 701/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.7232 - val_loss: 5.1565\n",
      "Epoch 702/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.7361 - val_loss: 5.1347\n",
      "Epoch 703/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.4784 - val_loss: 5.1294\n",
      "Epoch 704/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 16.7095 - val_loss: 5.1294\n",
      "Epoch 705/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.3899 - val_loss: 5.1337\n",
      "Epoch 706/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.0688 - val_loss: 5.1513\n",
      "Epoch 707/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.4940 - val_loss: 5.1633\n",
      "Epoch 708/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.8580 - val_loss: 5.1819\n",
      "Epoch 709/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 15.8598 - val_loss: 5.1664\n",
      "Epoch 710/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.6495 - val_loss: 5.1209\n",
      "Epoch 711/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.3472 - val_loss: 5.1170\n",
      "Epoch 712/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 16.1580 - val_loss: 5.1187\n",
      "Epoch 713/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.3929 - val_loss: 5.1161\n",
      "Epoch 714/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.3651 - val_loss: 5.1132\n",
      "Epoch 715/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.2109 - val_loss: 5.1178\n",
      "Epoch 716/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.2697 - val_loss: 5.1299\n",
      "Epoch 717/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 16.0924 - val_loss: 5.1315\n",
      "Epoch 718/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.1386 - val_loss: 5.1409\n",
      "Epoch 719/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 16.5672 - val_loss: 5.1266\n",
      "Epoch 720/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.8557 - val_loss: 5.1137\n",
      "Epoch 721/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.6364 - val_loss: 5.0991\n",
      "Epoch 722/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 16.5767 - val_loss: 5.1065\n",
      "Epoch 723/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.2343 - val_loss: 5.1038\n",
      "Epoch 724/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.3704 - val_loss: 5.0898\n",
      "Epoch 725/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 16.3053 - val_loss: 5.0929\n",
      "Epoch 726/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 16.0560 - val_loss: 5.0825\n",
      "Epoch 727/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 16.4423 - val_loss: 5.0858\n",
      "Epoch 728/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.3578 - val_loss: 5.0912\n",
      "Epoch 729/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.1389 - val_loss: 5.1060\n",
      "Epoch 730/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.8108 - val_loss: 5.1303\n",
      "Epoch 731/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.4972 - val_loss: 5.1128\n",
      "Epoch 732/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.4717 - val_loss: 5.1019\n",
      "Epoch 733/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.2129 - val_loss: 5.1187\n",
      "Epoch 734/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.8886 - val_loss: 5.1199\n",
      "Epoch 735/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.9416 - val_loss: 5.1143\n",
      "Epoch 736/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.0103 - val_loss: 5.0924\n",
      "Epoch 737/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.8867 - val_loss: 5.0931\n",
      "Epoch 738/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.6761 - val_loss: 5.1019\n",
      "Epoch 739/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.2177 - val_loss: 5.0970\n",
      "Epoch 740/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.9288 - val_loss: 5.0960\n",
      "Epoch 741/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.2600 - val_loss: 5.0898\n",
      "Epoch 742/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.0851 - val_loss: 5.0870\n",
      "Epoch 743/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 16.3519 - val_loss: 5.0651\n",
      "Epoch 744/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 16.4910 - val_loss: 5.0543\n",
      "Epoch 745/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.1525 - val_loss: 5.0554\n",
      "Epoch 746/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.3742 - val_loss: 5.0681\n",
      "Epoch 747/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.3212 - val_loss: 5.0657\n",
      "Epoch 748/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 16.3732 - val_loss: 5.0634\n",
      "Epoch 749/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.0631 - val_loss: 5.0835\n",
      "Epoch 750/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.3344 - val_loss: 5.0939\n",
      "Epoch 751/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 16.3823 - val_loss: 5.0674\n",
      "Epoch 752/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.4465 - val_loss: 5.0650\n",
      "Epoch 753/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.8822 - val_loss: 5.0892\n",
      "Epoch 754/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.6462 - val_loss: 5.0711\n",
      "Epoch 755/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 16.2883 - val_loss: 5.0644\n",
      "Epoch 756/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 15.8738 - val_loss: 5.0575\n",
      "Epoch 757/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.2559 - val_loss: 5.0905\n",
      "Epoch 758/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.7466 - val_loss: 5.0892\n",
      "Epoch 759/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.3139 - val_loss: 5.0506\n",
      "Epoch 760/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 15.5729 - val_loss: 5.0498\n",
      "Epoch 761/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.0960 - val_loss: 5.0283\n",
      "Epoch 762/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 15.7695 - val_loss: 5.0241\n",
      "Epoch 763/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.9602 - val_loss: 5.0433\n",
      "Epoch 764/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.2590 - val_loss: 5.0296\n",
      "Epoch 765/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.9332 - val_loss: 5.0169\n",
      "Epoch 766/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.7622 - val_loss: 5.0484\n",
      "Epoch 767/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.7485 - val_loss: 5.0337\n",
      "Epoch 768/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.3457 - val_loss: 5.0380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 769/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 16.0194 - val_loss: 5.0655\n",
      "Epoch 770/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.8947 - val_loss: 5.0672\n",
      "Epoch 771/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.2423 - val_loss: 5.0764\n",
      "Epoch 772/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.0674 - val_loss: 5.0636\n",
      "Epoch 773/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 16.3543 - val_loss: 5.0310\n",
      "Epoch 774/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.6436 - val_loss: 5.0168\n",
      "Epoch 775/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 16.5526 - val_loss: 5.0151\n",
      "Epoch 776/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 15.9021 - val_loss: 5.0261\n",
      "Epoch 777/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 16.4718 - val_loss: 5.0420\n",
      "Epoch 778/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 16.2394 - val_loss: 5.0424\n",
      "Epoch 779/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 16.0806 - val_loss: 5.0581\n",
      "Epoch 780/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.0061 - val_loss: 5.0875\n",
      "Epoch 781/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.8844 - val_loss: 5.0964\n",
      "Epoch 782/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.0535 - val_loss: 5.0746\n",
      "Epoch 783/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 16.2292 - val_loss: 5.0300\n",
      "Epoch 784/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.1974 - val_loss: 4.9982\n",
      "Epoch 785/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.0139 - val_loss: 5.0069\n",
      "Epoch 786/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.4708 - val_loss: 4.9890\n",
      "Epoch 787/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 16.3468 - val_loss: 5.0118\n",
      "Epoch 788/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.0321 - val_loss: 5.0340\n",
      "Epoch 789/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.0930 - val_loss: 5.0467\n",
      "Epoch 790/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.8177 - val_loss: 5.0782\n",
      "Epoch 791/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.0029 - val_loss: 5.0291\n",
      "Epoch 792/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.1085 - val_loss: 5.0032\n",
      "Epoch 793/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.9917 - val_loss: 4.9883\n",
      "Epoch 794/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.8101 - val_loss: 4.9677\n",
      "Epoch 795/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 16.0902 - val_loss: 4.9712\n",
      "Epoch 796/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.3095 - val_loss: 4.9647\n",
      "Epoch 797/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 15.5707 - val_loss: 5.0027\n",
      "Epoch 798/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.1094 - val_loss: 5.0227\n",
      "Epoch 799/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.9924 - val_loss: 5.0255\n",
      "Epoch 800/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.0546 - val_loss: 5.0177\n",
      "Epoch 801/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.9300 - val_loss: 4.9951\n",
      "Epoch 802/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.8978 - val_loss: 5.0149\n",
      "Epoch 803/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 15.8997 - val_loss: 5.0249\n",
      "Epoch 804/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 16.0047 - val_loss: 5.0189\n",
      "Epoch 805/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 16.0472 - val_loss: 5.0181\n",
      "Epoch 806/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 16.1500 - val_loss: 4.9941\n",
      "Epoch 807/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 16.4952 - val_loss: 4.9723\n",
      "Epoch 808/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.1687 - val_loss: 4.9806\n",
      "Epoch 809/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 16.0655 - val_loss: 4.9930\n",
      "Epoch 810/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.6887 - val_loss: 5.0154\n",
      "Epoch 811/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 15.3624 - val_loss: 5.0281\n",
      "Epoch 812/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.0879 - val_loss: 5.0170\n",
      "Epoch 813/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.1269 - val_loss: 5.0012\n",
      "Epoch 814/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.7726 - val_loss: 4.9933\n",
      "Epoch 815/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.5828 - val_loss: 4.9859\n",
      "Epoch 816/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.1176 - val_loss: 4.9738\n",
      "Epoch 817/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.2206 - val_loss: 4.9707\n",
      "Epoch 818/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 16.3243 - val_loss: 4.9748\n",
      "Epoch 819/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 15.9639 - val_loss: 4.9925\n",
      "Epoch 820/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.7464 - val_loss: 5.0115\n",
      "Epoch 821/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.7517 - val_loss: 5.0215\n",
      "Epoch 822/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 15.7351 - val_loss: 5.0183\n",
      "Epoch 823/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.0673 - val_loss: 5.0161\n",
      "Epoch 824/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.1606 - val_loss: 4.9851\n",
      "Epoch 825/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.7273 - val_loss: 4.9738\n",
      "Epoch 826/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.9001 - val_loss: 4.9725\n",
      "Epoch 827/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 16.0341 - val_loss: 4.9518\n",
      "Epoch 828/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.4748 - val_loss: 4.9703\n",
      "Epoch 829/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.2971 - val_loss: 4.9830\n",
      "Epoch 830/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 16.0678 - val_loss: 4.9630\n",
      "Epoch 831/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.0307 - val_loss: 4.9720\n",
      "Epoch 832/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.0547 - val_loss: 4.9864\n",
      "Epoch 833/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.9343 - val_loss: 4.9865\n",
      "Epoch 834/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 16.2543 - val_loss: 5.0111\n",
      "Epoch 835/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 16.4093 - val_loss: 4.9840\n",
      "Epoch 836/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.0487 - val_loss: 4.9990\n",
      "Epoch 837/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.9144 - val_loss: 4.9877\n",
      "Epoch 838/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.9529 - val_loss: 4.9788\n",
      "Epoch 839/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.7376 - val_loss: 4.9954\n",
      "Epoch 840/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.8937 - val_loss: 4.9857\n",
      "Epoch 841/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.1105 - val_loss: 4.9708\n",
      "Epoch 842/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.0316 - val_loss: 4.9609\n",
      "Epoch 843/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.9322 - val_loss: 4.9588\n",
      "Epoch 844/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.9909 - val_loss: 4.9654\n",
      "Epoch 845/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.1055 - val_loss: 4.9772\n",
      "Epoch 846/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.7301 - val_loss: 4.9894\n",
      "Epoch 847/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.3410 - val_loss: 5.0140\n",
      "Epoch 848/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.8742 - val_loss: 5.0109\n",
      "Epoch 849/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.7635 - val_loss: 4.9775\n",
      "Epoch 850/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.3083 - val_loss: 4.9702\n",
      "Epoch 851/10000\n",
      "90/90 [==============================] - 0s 82us/step - loss: 15.6405 - val_loss: 4.9533\n",
      "Epoch 852/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.4353 - val_loss: 4.9462\n",
      "Epoch 853/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.4423 - val_loss: 4.9422\n",
      "Epoch 854/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 16.2736 - val_loss: 4.9309\n",
      "Epoch 855/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.1756 - val_loss: 4.9208\n",
      "Epoch 856/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.8537 - val_loss: 4.9180\n",
      "Epoch 857/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.9614 - val_loss: 4.9195\n",
      "Epoch 858/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.8702 - val_loss: 4.9321\n",
      "Epoch 859/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 16.2691 - val_loss: 4.9635\n",
      "Epoch 860/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 15.7959 - val_loss: 4.9760\n",
      "Epoch 861/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.7551 - val_loss: 4.9986\n",
      "Epoch 862/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.0101 - val_loss: 4.9798\n",
      "Epoch 863/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 16.1084 - val_loss: 4.9775\n",
      "Epoch 864/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.1898 - val_loss: 4.9583\n",
      "Epoch 865/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 15.4054 - val_loss: 4.9710\n",
      "Epoch 866/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.4941 - val_loss: 4.9773\n",
      "Epoch 867/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.2006 - val_loss: 4.9598\n",
      "Epoch 868/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.3298 - val_loss: 4.9536\n",
      "Epoch 869/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 15.5476 - val_loss: 4.9754\n",
      "Epoch 870/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.8488 - val_loss: 4.9724\n",
      "Epoch 871/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 16.2222 - val_loss: 4.9623\n",
      "Epoch 872/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.6295 - val_loss: 4.9537\n",
      "Epoch 873/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.5659 - val_loss: 4.9639\n",
      "Epoch 874/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.9136 - val_loss: 4.9481\n",
      "Epoch 875/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.7268 - val_loss: 4.9459\n",
      "Epoch 876/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.0565 - val_loss: 4.9195\n",
      "Epoch 877/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 15.8812 - val_loss: 4.9339\n",
      "Epoch 878/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.8600 - val_loss: 4.9310\n",
      "Epoch 879/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.4634 - val_loss: 4.9708\n",
      "Epoch 880/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.6448 - val_loss: 4.9705\n",
      "Epoch 881/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 16.0946 - val_loss: 4.9288\n",
      "Epoch 882/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.6583 - val_loss: 4.9347\n",
      "Epoch 883/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 15.8401 - val_loss: 4.9219\n",
      "Epoch 884/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.3518 - val_loss: 4.9212\n",
      "Epoch 885/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.8450 - val_loss: 4.9228\n",
      "Epoch 886/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.6553 - val_loss: 4.9423\n",
      "Epoch 887/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.0902 - val_loss: 4.9465\n",
      "Epoch 888/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.5492 - val_loss: 4.9744\n",
      "Epoch 889/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 15.5003 - val_loss: 4.9879\n",
      "Epoch 890/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 15.6131 - val_loss: 4.9582\n",
      "Epoch 891/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 16.0911 - val_loss: 4.9377\n",
      "Epoch 892/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.9870 - val_loss: 4.9299\n",
      "Epoch 893/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 15.5083 - val_loss: 4.9362\n",
      "Epoch 894/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 15.7407 - val_loss: 4.9363\n",
      "Epoch 895/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.2709 - val_loss: 4.9429\n",
      "Epoch 896/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 15.6556 - val_loss: 4.9362\n",
      "Epoch 897/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 15.7145 - val_loss: 4.9514\n",
      "Epoch 898/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.6554 - val_loss: 4.9370\n",
      "Epoch 899/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 15.5145 - val_loss: 4.9375\n",
      "Epoch 900/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.2111 - val_loss: 4.9451\n",
      "Epoch 901/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.7418 - val_loss: 4.9451\n",
      "Epoch 902/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.5107 - val_loss: 4.9427\n",
      "Epoch 903/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.0071 - val_loss: 4.9212\n",
      "Epoch 904/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.1466 - val_loss: 4.9239\n",
      "Epoch 905/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 15.9253 - val_loss: 4.9346\n",
      "Epoch 906/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 15.9158 - val_loss: 4.9538\n",
      "Epoch 907/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 16.0930 - val_loss: 4.9587\n",
      "Epoch 908/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.3723 - val_loss: 4.9873\n",
      "Epoch 909/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 16.2380 - val_loss: 4.9302\n",
      "Epoch 910/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.2138 - val_loss: 4.9153\n",
      "Epoch 911/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 16.0350 - val_loss: 4.9206\n",
      "Epoch 912/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 15.9087 - val_loss: 4.9352\n",
      "Epoch 913/10000\n",
      "90/90 [==============================] - 0s 80us/step - loss: 16.0348 - val_loss: 4.9320\n",
      "Epoch 914/10000\n",
      "90/90 [==============================] - 0s 79us/step - loss: 16.1674 - val_loss: 4.9296\n",
      "Epoch 915/10000\n",
      "90/90 [==============================] - 0s 78us/step - loss: 15.5835 - val_loss: 4.9581\n",
      "Epoch 916/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 16.0478 - val_loss: 4.9514\n",
      "Epoch 917/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.5302 - val_loss: 4.9532\n",
      "Epoch 918/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 15.2855 - val_loss: 4.9416\n",
      "Epoch 919/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 15.8310 - val_loss: 4.9253\n",
      "Epoch 920/10000\n",
      "90/90 [==============================] - 0s 79us/step - loss: 15.3707 - val_loss: 4.9370\n",
      "Epoch 921/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 16.2213 - val_loss: 4.9073\n",
      "Epoch 922/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 15.4827 - val_loss: 4.8993\n",
      "Epoch 923/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.4134 - val_loss: 4.9088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 924/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 15.9518 - val_loss: 4.9130\n",
      "Epoch 925/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 16.2382 - val_loss: 4.9107\n",
      "Epoch 926/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 15.5939 - val_loss: 4.9330\n",
      "Epoch 927/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 16.4242 - val_loss: 4.9466\n",
      "Epoch 928/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 15.6673 - val_loss: 4.9641\n",
      "Epoch 929/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 15.9472 - val_loss: 4.9392\n",
      "Epoch 930/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.6487 - val_loss: 4.9360\n",
      "Epoch 931/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.9372 - val_loss: 4.9190\n",
      "Epoch 932/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.7076 - val_loss: 4.9261\n",
      "Epoch 933/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.6021 - val_loss: 4.9494\n",
      "Epoch 934/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 15.7405 - val_loss: 4.9330\n",
      "Epoch 935/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 15.3677 - val_loss: 4.9193\n",
      "Epoch 936/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.4846 - val_loss: 4.9164\n",
      "Epoch 937/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 16.1441 - val_loss: 4.9031\n",
      "Epoch 938/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 15.8022 - val_loss: 4.9112\n",
      "Epoch 939/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 15.0742 - val_loss: 4.9340\n",
      "Epoch 940/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.5565 - val_loss: 4.9194\n",
      "Epoch 941/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 15.5660 - val_loss: 4.9260\n",
      "Epoch 942/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.5267 - val_loss: 4.9352\n",
      "Epoch 943/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 15.4926 - val_loss: 4.9231\n",
      "Epoch 944/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 16.4100 - val_loss: 4.9293\n",
      "Epoch 945/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.8468 - val_loss: 4.9446\n",
      "Epoch 946/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 15.8525 - val_loss: 4.9429\n",
      "Epoch 947/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 16.3696 - val_loss: 4.9244\n",
      "Epoch 948/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 15.5919 - val_loss: 4.9529\n",
      "Epoch 949/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 15.4757 - val_loss: 4.9548\n",
      "Epoch 950/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 15.6764 - val_loss: 4.9538\n",
      "Epoch 951/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.5562 - val_loss: 4.9389\n",
      "Epoch 952/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.3804 - val_loss: 4.9517\n",
      "Epoch 953/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.7749 - val_loss: 4.9381\n",
      "Epoch 954/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.6461 - val_loss: 4.9346\n",
      "Epoch 955/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 16.0426 - val_loss: 4.9166\n",
      "Epoch 956/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 15.3039 - val_loss: 4.9283\n",
      "Epoch 957/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.4308 - val_loss: 4.9221\n",
      "Epoch 958/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 16.0565 - val_loss: 4.9024\n",
      "Epoch 959/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 15.5060 - val_loss: 4.9107\n",
      "Epoch 960/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 15.6875 - val_loss: 4.9198\n",
      "Epoch 961/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.6721 - val_loss: 4.9131\n",
      "Epoch 962/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.6926 - val_loss: 4.9397\n",
      "Epoch 963/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 15.3441 - val_loss: 4.9420\n",
      "Epoch 964/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.2103 - val_loss: 4.9156\n",
      "Epoch 965/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.8196 - val_loss: 4.8891\n",
      "Epoch 966/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.8917 - val_loss: 4.8869\n",
      "Epoch 967/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.5861 - val_loss: 4.8909\n",
      "Epoch 968/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 16.0974 - val_loss: 4.8929\n",
      "Epoch 969/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 16.0299 - val_loss: 4.9017\n",
      "Epoch 970/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.4568 - val_loss: 4.9263\n",
      "Epoch 971/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.4858 - val_loss: 4.9685\n",
      "Epoch 972/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.1218 - val_loss: 4.9430\n",
      "Epoch 973/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.6039 - val_loss: 4.9324\n",
      "Epoch 974/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.9069 - val_loss: 4.9140\n",
      "Epoch 975/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.6206 - val_loss: 4.9343\n",
      "Epoch 976/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.7508 - val_loss: 4.9023\n",
      "Epoch 977/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 15.2216 - val_loss: 4.9113\n",
      "Epoch 978/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 15.9811 - val_loss: 4.8859\n",
      "Epoch 979/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.5769 - val_loss: 4.8887\n",
      "Epoch 980/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 16.0883 - val_loss: 4.8932\n",
      "Epoch 981/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 16.0916 - val_loss: 4.9007\n",
      "Epoch 982/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.6255 - val_loss: 4.9438\n",
      "Epoch 983/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 16.0647 - val_loss: 4.9493\n",
      "Epoch 984/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 15.5593 - val_loss: 4.9260\n",
      "Epoch 985/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 15.8112 - val_loss: 4.9164\n",
      "Epoch 986/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 15.5244 - val_loss: 4.9259\n",
      "Epoch 987/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.7301 - val_loss: 4.8979\n",
      "Epoch 988/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.5048 - val_loss: 4.9077\n",
      "Epoch 989/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 15.3863 - val_loss: 4.9272\n",
      "Epoch 990/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 15.6704 - val_loss: 4.9007\n",
      "Epoch 991/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 15.9151 - val_loss: 4.8858\n",
      "Epoch 992/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 15.6473 - val_loss: 4.9149\n",
      "Epoch 993/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 16.0756 - val_loss: 4.9159\n",
      "Epoch 994/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 15.5697 - val_loss: 4.9220\n",
      "Epoch 995/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.5398 - val_loss: 4.9686\n",
      "Epoch 996/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.4122 - val_loss: 4.9434\n",
      "Epoch 997/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 16.1473 - val_loss: 4.9234\n",
      "Epoch 998/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.3173 - val_loss: 4.9194\n",
      "Epoch 999/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 16.0756 - val_loss: 4.8903\n",
      "Epoch 1000/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.6159 - val_loss: 4.9167\n",
      "Epoch 1001/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 15.8181 - val_loss: 4.9007\n",
      "Epoch 1002/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 15.6359 - val_loss: 4.9165\n",
      "Epoch 1003/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 15.4753 - val_loss: 4.9211\n",
      "Epoch 1004/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.6650 - val_loss: 4.9019\n",
      "Epoch 1005/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 15.2741 - val_loss: 4.9164\n",
      "Epoch 1006/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 15.9530 - val_loss: 4.9155\n",
      "Epoch 1007/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.9654 - val_loss: 4.8790\n",
      "Epoch 1008/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.4459 - val_loss: 4.8958\n",
      "Epoch 1009/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.2288 - val_loss: 4.8958\n",
      "Epoch 1010/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 15.7975 - val_loss: 4.9051\n",
      "Epoch 1011/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.6015 - val_loss: 4.9347\n",
      "Epoch 1012/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.4457 - val_loss: 4.9375\n",
      "Epoch 1013/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 15.5353 - val_loss: 4.9325\n",
      "Epoch 1014/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 15.3844 - val_loss: 4.9452\n",
      "Epoch 1015/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.7938 - val_loss: 4.9178\n",
      "Epoch 1016/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 15.8318 - val_loss: 4.9198\n",
      "Epoch 1017/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 15.6292 - val_loss: 4.9200\n",
      "Epoch 1018/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 15.4972 - val_loss: 4.9236\n",
      "Epoch 1019/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.8864 - val_loss: 4.9193\n",
      "Epoch 1020/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.7284 - val_loss: 4.9399\n",
      "Epoch 1021/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.3912 - val_loss: 4.9411\n",
      "Epoch 1022/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.5656 - val_loss: 4.9001\n",
      "Epoch 1023/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.3873 - val_loss: 4.8994\n",
      "Epoch 1024/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.7453 - val_loss: 4.9034\n",
      "Epoch 1025/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 14.9873 - val_loss: 4.9046\n",
      "Epoch 1026/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 15.8704 - val_loss: 4.8789\n",
      "Epoch 1027/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 15.6510 - val_loss: 4.8667\n",
      "Epoch 1028/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 15.6137 - val_loss: 4.8681\n",
      "Epoch 1029/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.6714 - val_loss: 4.9006\n",
      "Epoch 1030/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 15.5036 - val_loss: 4.9381\n",
      "Epoch 1031/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.4107 - val_loss: 4.9733\n",
      "Epoch 1032/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 15.0221 - val_loss: 4.9946\n",
      "Epoch 1033/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.4670 - val_loss: 4.9580\n",
      "Epoch 1034/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.3573 - val_loss: 4.9068\n",
      "Epoch 1035/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 15.7288 - val_loss: 4.8984\n",
      "Epoch 1036/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 15.6127 - val_loss: 4.8907\n",
      "Epoch 1037/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.9256 - val_loss: 4.8848\n",
      "Epoch 1038/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.8783 - val_loss: 4.9024\n",
      "Epoch 1039/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.5165 - val_loss: 4.9154\n",
      "Epoch 1040/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 15.9144 - val_loss: 4.8985\n",
      "Epoch 1041/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.3720 - val_loss: 4.9381\n",
      "Epoch 1042/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.8978 - val_loss: 4.9242\n",
      "Epoch 1043/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.0089 - val_loss: 4.9619\n",
      "Epoch 1044/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.3239 - val_loss: 4.9286\n",
      "Epoch 1045/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 15.7280 - val_loss: 4.8884\n",
      "Epoch 1046/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.5763 - val_loss: 4.8950\n",
      "Epoch 1047/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.0827 - val_loss: 4.9126\n",
      "Epoch 1048/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.7739 - val_loss: 4.8916\n",
      "Epoch 1049/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 16.0574 - val_loss: 4.8850\n",
      "Epoch 1050/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.9756 - val_loss: 4.8689\n",
      "Epoch 1051/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 15.6636 - val_loss: 4.8737\n",
      "Epoch 1052/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 15.1991 - val_loss: 4.9083\n",
      "Epoch 1053/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.2179 - val_loss: 4.9261\n",
      "Epoch 1054/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.4170 - val_loss: 4.9436\n",
      "Epoch 1055/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.8163 - val_loss: 4.9175\n",
      "Epoch 1056/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 14.9666 - val_loss: 4.9451\n",
      "Epoch 1057/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 14.6434 - val_loss: 4.9623\n",
      "Epoch 1058/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.6083 - val_loss: 4.9049\n",
      "Epoch 1059/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 15.4253 - val_loss: 4.8873\n",
      "Epoch 1060/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 15.3712 - val_loss: 4.8767\n",
      "Epoch 1061/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 15.3773 - val_loss: 4.8761\n",
      "Epoch 1062/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.3272 - val_loss: 4.8911\n",
      "Epoch 1063/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 15.4406 - val_loss: 4.9425\n",
      "Epoch 1064/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 15.9489 - val_loss: 4.9269\n",
      "Epoch 1065/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.1942 - val_loss: 4.9728\n",
      "Epoch 1066/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 15.0413 - val_loss: 4.9788\n",
      "Epoch 1067/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.6339 - val_loss: 4.9427\n",
      "Epoch 1068/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.6003 - val_loss: 4.9241\n",
      "Epoch 1069/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 15.1763 - val_loss: 4.9139\n",
      "Epoch 1070/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 15.3908 - val_loss: 4.9122\n",
      "Epoch 1071/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 15.1451 - val_loss: 4.9117\n",
      "Epoch 1072/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.3437 - val_loss: 4.9128\n",
      "Epoch 1073/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 15.5449 - val_loss: 4.8879\n",
      "Epoch 1074/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.8648 - val_loss: 4.8765\n",
      "Epoch 1075/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 14.8827 - val_loss: 4.9357\n",
      "Epoch 1076/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 15.2306 - val_loss: 4.9529\n",
      "Epoch 1077/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.2607 - val_loss: 4.9414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1078/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.3454 - val_loss: 4.9427\n",
      "Epoch 1079/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 15.2683 - val_loss: 4.9006\n",
      "Epoch 1080/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 14.8016 - val_loss: 4.8848\n",
      "Epoch 1081/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.5413 - val_loss: 4.8857\n",
      "Epoch 1082/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.1695 - val_loss: 4.9014\n",
      "Epoch 1083/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 15.3739 - val_loss: 4.9068\n",
      "Epoch 1084/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 15.2286 - val_loss: 4.9110\n",
      "Epoch 1085/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 15.3790 - val_loss: 4.9320\n",
      "Epoch 1086/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 14.8573 - val_loss: 4.9641\n",
      "Epoch 1087/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.3039 - val_loss: 4.9178\n",
      "Epoch 1088/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 15.5299 - val_loss: 4.8881\n",
      "Epoch 1089/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 15.4769 - val_loss: 4.8758\n",
      "Epoch 1090/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.1015 - val_loss: 4.9000\n",
      "Epoch 1091/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 15.5070 - val_loss: 4.9031\n",
      "Epoch 1092/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 14.7638 - val_loss: 4.9330\n",
      "Epoch 1093/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.1473 - val_loss: 4.9681\n",
      "Epoch 1094/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 15.4229 - val_loss: 4.9674\n",
      "Epoch 1095/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 15.2778 - val_loss: 4.9437\n",
      "Epoch 1096/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 14.9658 - val_loss: 4.9587\n",
      "Epoch 1097/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.6410 - val_loss: 4.9063\n",
      "Epoch 1098/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 15.2654 - val_loss: 4.8980\n",
      "Epoch 1099/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 14.9184 - val_loss: 4.9185\n",
      "Epoch 1100/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 15.2888 - val_loss: 4.9572\n",
      "Epoch 1101/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.0398 - val_loss: 4.9903\n",
      "Epoch 1102/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 14.9488 - val_loss: 4.9627\n",
      "Epoch 1103/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 14.8841 - val_loss: 4.9324\n",
      "Epoch 1104/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 15.2560 - val_loss: 4.8969\n",
      "Epoch 1105/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 15.0557 - val_loss: 4.8613\n",
      "Epoch 1106/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.2100 - val_loss: 4.8451\n",
      "Epoch 1107/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 15.5015 - val_loss: 4.8757\n",
      "Epoch 1108/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 15.5262 - val_loss: 4.8886\n",
      "Epoch 1109/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.9899 - val_loss: 4.9910\n",
      "Epoch 1110/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 15.7477 - val_loss: 4.9523\n",
      "Epoch 1111/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 15.3678 - val_loss: 4.9167\n",
      "Epoch 1112/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 14.5842 - val_loss: 4.9177\n",
      "Epoch 1113/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 14.9439 - val_loss: 4.9461\n",
      "Epoch 1114/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 14.6788 - val_loss: 4.9251\n",
      "Epoch 1115/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 15.2072 - val_loss: 4.9057\n",
      "Epoch 1116/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.3393 - val_loss: 4.8745\n",
      "Epoch 1117/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 14.8811 - val_loss: 4.9002\n",
      "Epoch 1118/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 14.7668 - val_loss: 4.8827\n",
      "Epoch 1119/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 14.8698 - val_loss: 4.9401\n",
      "Epoch 1120/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 15.2582 - val_loss: 4.9032\n",
      "Epoch 1121/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.0393 - val_loss: 4.8598\n",
      "Epoch 1122/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 14.5718 - val_loss: 4.9014\n",
      "Epoch 1123/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 14.7118 - val_loss: 4.9035\n",
      "Epoch 1124/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 14.9749 - val_loss: 4.8778\n",
      "Epoch 1125/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 14.7617 - val_loss: 4.9157\n",
      "Epoch 1126/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 14.9458 - val_loss: 4.9360\n",
      "Epoch 1127/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 15.0577 - val_loss: 4.9339\n",
      "Epoch 1128/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 14.8903 - val_loss: 4.9032\n",
      "Epoch 1129/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 15.3921 - val_loss: 4.9016\n",
      "Epoch 1130/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 14.5501 - val_loss: 4.9205\n",
      "Epoch 1131/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 14.7524 - val_loss: 4.9161\n",
      "Epoch 1132/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 14.7404 - val_loss: 4.9159\n",
      "Epoch 1133/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 15.0363 - val_loss: 4.8770\n",
      "Epoch 1134/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 14.9407 - val_loss: 4.8933\n",
      "Epoch 1135/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.9856 - val_loss: 4.9255\n",
      "Epoch 1136/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 14.9660 - val_loss: 4.9642\n",
      "Epoch 1137/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 14.6987 - val_loss: 4.9860\n",
      "Epoch 1138/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.9458 - val_loss: 4.9037\n",
      "Epoch 1139/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 14.7503 - val_loss: 4.9300\n",
      "Epoch 1140/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 14.7562 - val_loss: 4.8783\n",
      "Epoch 1141/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 15.1104 - val_loss: 4.8760\n",
      "Epoch 1142/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.9173 - val_loss: 4.9124\n",
      "Epoch 1143/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 14.8107 - val_loss: 4.8776\n",
      "Epoch 1144/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.9847 - val_loss: 4.8975\n",
      "Epoch 1145/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 14.7139 - val_loss: 4.9480\n",
      "Epoch 1146/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 15.2934 - val_loss: 4.8811\n",
      "Epoch 1147/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 14.7739 - val_loss: 4.8635\n",
      "Epoch 1148/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 14.6329 - val_loss: 4.8854\n",
      "Epoch 1149/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 14.9838 - val_loss: 4.9101\n",
      "Epoch 1150/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 14.7902 - val_loss: 4.9549\n",
      "Epoch 1151/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 14.5145 - val_loss: 4.9183\n",
      "Epoch 1152/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 14.7210 - val_loss: 4.8796\n",
      "Epoch 1153/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 14.2229 - val_loss: 4.8505\n",
      "Epoch 1154/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 14.6331 - val_loss: 4.8651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1155/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 14.6798 - val_loss: 4.8885\n",
      "Epoch 1156/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 14.7160 - val_loss: 4.9067\n",
      "Epoch 1157/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 14.3617 - val_loss: 4.9541\n",
      "Epoch 1158/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 14.4051 - val_loss: 4.9755\n",
      "Epoch 1159/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 14.3239 - val_loss: 4.9178\n",
      "Epoch 1160/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.1000 - val_loss: 4.9710\n",
      "Epoch 1161/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 14.4416 - val_loss: 4.9100\n",
      "Epoch 1162/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 14.4368 - val_loss: 4.9041\n",
      "Epoch 1163/10000\n",
      "90/90 [==============================] - 0s 79us/step - loss: 14.7513 - val_loss: 4.9176\n",
      "Epoch 1164/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 14.4579 - val_loss: 4.9293\n",
      "Epoch 1165/10000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 14.6003 - val_loss: 4.9485\n",
      "Epoch 1166/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 14.3851 - val_loss: 4.9535\n",
      "Epoch 1167/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 14.1619 - val_loss: 4.9882\n",
      "Epoch 1168/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 14.5509 - val_loss: 4.8839\n",
      "Epoch 1169/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 14.3008 - val_loss: 4.8375\n",
      "Epoch 1170/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 14.3526 - val_loss: 4.8169\n",
      "Epoch 1171/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 14.5744 - val_loss: 4.7974\n",
      "Epoch 1172/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 14.0134 - val_loss: 4.8664\n",
      "Epoch 1173/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 14.8186 - val_loss: 4.8268\n",
      "Epoch 1174/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.0999 - val_loss: 4.9409\n",
      "Epoch 1175/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 14.7235 - val_loss: 4.9004\n",
      "Epoch 1176/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.1799 - val_loss: 4.9368\n",
      "Epoch 1177/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 14.3911 - val_loss: 4.9549\n",
      "Epoch 1178/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 14.7460 - val_loss: 4.8393\n",
      "Epoch 1179/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 14.3477 - val_loss: 4.8147\n",
      "Epoch 1180/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 14.5851 - val_loss: 4.8508\n",
      "Epoch 1181/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 14.4534 - val_loss: 4.9383\n",
      "Epoch 1182/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 13.9846 - val_loss: 4.9828\n",
      "Epoch 1183/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 14.5526 - val_loss: 4.9177\n",
      "Epoch 1184/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 14.0511 - val_loss: 4.8584\n",
      "Epoch 1185/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 13.6472 - val_loss: 4.9187\n",
      "Epoch 1186/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 13.5323 - val_loss: 4.9516\n",
      "Epoch 1187/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 14.8457 - val_loss: 4.7711\n",
      "Epoch 1188/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 13.8194 - val_loss: 4.8021\n",
      "Epoch 1189/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 14.3360 - val_loss: 4.7791\n",
      "Epoch 1190/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 14.6084 - val_loss: 4.8106\n",
      "Epoch 1191/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 14.0200 - val_loss: 4.9025\n",
      "Epoch 1192/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 14.3990 - val_loss: 4.9497\n",
      "Epoch 1193/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 14.6345 - val_loss: 4.8692\n",
      "Epoch 1194/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 13.9803 - val_loss: 4.8953\n",
      "Epoch 1195/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 14.1695 - val_loss: 4.8541\n",
      "Epoch 1196/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 13.8750 - val_loss: 4.8490\n",
      "Epoch 1197/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 13.9653 - val_loss: 4.8124\n",
      "Epoch 1198/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 14.4284 - val_loss: 4.7829\n",
      "Epoch 1199/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.6110 - val_loss: 4.8692\n",
      "Epoch 1200/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 14.3577 - val_loss: 4.8183\n",
      "Epoch 1201/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 13.9896 - val_loss: 4.8609\n",
      "Epoch 1202/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 13.9035 - val_loss: 4.7928\n",
      "Epoch 1203/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 13.4273 - val_loss: 4.8721\n",
      "Epoch 1204/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 14.0777 - val_loss: 4.8105\n",
      "Epoch 1205/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.8263 - val_loss: 4.7437\n",
      "Epoch 1206/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 13.9876 - val_loss: 4.7630\n",
      "Epoch 1207/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 14.1030 - val_loss: 4.8355\n",
      "Epoch 1208/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.7746 - val_loss: 4.9475\n",
      "Epoch 1209/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 13.6768 - val_loss: 4.9064\n",
      "Epoch 1210/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 13.9625 - val_loss: 4.8317\n",
      "Epoch 1211/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 13.6986 - val_loss: 4.7454\n",
      "Epoch 1212/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.7666 - val_loss: 4.7742\n",
      "Epoch 1213/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 13.6681 - val_loss: 4.9087\n",
      "Epoch 1214/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 13.5790 - val_loss: 4.9313\n",
      "Epoch 1215/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 13.5970 - val_loss: 4.8852\n",
      "Epoch 1216/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 13.6823 - val_loss: 4.8592\n",
      "Epoch 1217/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 14.0620 - val_loss: 4.7912\n",
      "Epoch 1218/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 13.9018 - val_loss: 4.7868\n",
      "Epoch 1219/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 14.0537 - val_loss: 4.7875\n",
      "Epoch 1220/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 13.9550 - val_loss: 4.7939\n",
      "Epoch 1221/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 13.8622 - val_loss: 4.8015\n",
      "Epoch 1222/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 13.6555 - val_loss: 4.8986\n",
      "Epoch 1223/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 13.3543 - val_loss: 4.8156\n",
      "Epoch 1224/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 13.5297 - val_loss: 4.7278\n",
      "Epoch 1225/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.4660 - val_loss: 4.7179\n",
      "Epoch 1226/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 13.7905 - val_loss: 4.7195\n",
      "Epoch 1227/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 13.5103 - val_loss: 4.7537\n",
      "Epoch 1228/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 14.2592 - val_loss: 4.7537\n",
      "Epoch 1229/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 13.6341 - val_loss: 4.8927\n",
      "Epoch 1230/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 13.4782 - val_loss: 4.9252\n",
      "Epoch 1231/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 13.3179 - val_loss: 4.9786\n",
      "Epoch 1232/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 109us/step - loss: 13.6070 - val_loss: 4.8020\n",
      "Epoch 1233/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 13.5762 - val_loss: 4.6876\n",
      "Epoch 1234/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 13.3213 - val_loss: 4.7141\n",
      "Epoch 1235/10000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 13.3659 - val_loss: 4.8045\n",
      "Epoch 1236/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 13.5206 - val_loss: 4.8638\n",
      "Epoch 1237/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 13.2707 - val_loss: 4.7751\n",
      "Epoch 1238/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 13.4072 - val_loss: 4.6888\n",
      "Epoch 1239/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 13.4408 - val_loss: 4.6875\n",
      "Epoch 1240/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 13.8859 - val_loss: 4.6953\n",
      "Epoch 1241/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 13.3995 - val_loss: 4.7502\n",
      "Epoch 1242/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 13.1905 - val_loss: 4.7771\n",
      "Epoch 1243/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 13.7406 - val_loss: 4.7606\n",
      "Epoch 1244/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 13.4298 - val_loss: 4.7402\n",
      "Epoch 1245/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 13.3388 - val_loss: 4.7121\n",
      "Epoch 1246/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 13.1784 - val_loss: 4.7171\n",
      "Epoch 1247/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 13.3335 - val_loss: 4.7043\n",
      "Epoch 1248/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 13.8883 - val_loss: 4.6341\n",
      "Epoch 1249/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 13.2925 - val_loss: 4.6312\n",
      "Epoch 1250/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.6458 - val_loss: 4.6853\n",
      "Epoch 1251/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 13.1476 - val_loss: 4.7801\n",
      "Epoch 1252/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 13.2486 - val_loss: 4.8726\n",
      "Epoch 1253/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 13.8152 - val_loss: 4.7140\n",
      "Epoch 1254/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 13.6883 - val_loss: 4.7016\n",
      "Epoch 1255/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 13.1793 - val_loss: 4.7614\n",
      "Epoch 1256/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 13.4259 - val_loss: 4.6905\n",
      "Epoch 1257/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.7370 - val_loss: 4.7318\n",
      "Epoch 1258/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 13.1307 - val_loss: 4.7008\n",
      "Epoch 1259/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 13.1937 - val_loss: 4.6209\n",
      "Epoch 1260/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 13.3988 - val_loss: 4.6713\n",
      "Epoch 1261/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.3314 - val_loss: 4.6849\n",
      "Epoch 1262/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 13.4094 - val_loss: 4.6640\n",
      "Epoch 1263/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 13.1808 - val_loss: 4.6882\n",
      "Epoch 1264/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 12.7726 - val_loss: 4.7261\n",
      "Epoch 1265/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 13.2295 - val_loss: 4.6389\n",
      "Epoch 1266/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 13.0480 - val_loss: 4.6290\n",
      "Epoch 1267/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 13.1275 - val_loss: 4.5626\n",
      "Epoch 1268/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 12.7908 - val_loss: 4.5935\n",
      "Epoch 1269/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.8684 - val_loss: 4.6186\n",
      "Epoch 1270/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 13.0836 - val_loss: 4.6873\n",
      "Epoch 1271/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 12.7472 - val_loss: 4.7578\n",
      "Epoch 1272/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 12.8745 - val_loss: 4.7023\n",
      "Epoch 1273/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 12.8672 - val_loss: 4.6984\n",
      "Epoch 1274/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 13.2479 - val_loss: 4.5933\n",
      "Epoch 1275/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 13.2174 - val_loss: 4.6031\n",
      "Epoch 1276/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 13.0395 - val_loss: 4.6022\n",
      "Epoch 1277/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 13.0310 - val_loss: 4.6512\n",
      "Epoch 1278/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 12.4992 - val_loss: 4.6192\n",
      "Epoch 1279/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 12.7780 - val_loss: 4.5884\n",
      "Epoch 1280/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.8765 - val_loss: 4.5679\n",
      "Epoch 1281/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.6052 - val_loss: 4.6194\n",
      "Epoch 1282/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 13.2861 - val_loss: 4.5593\n",
      "Epoch 1283/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 13.0566 - val_loss: 4.5659\n",
      "Epoch 1284/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 13.0702 - val_loss: 4.6132\n",
      "Epoch 1285/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 12.5768 - val_loss: 4.7762\n",
      "Epoch 1286/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 13.0337 - val_loss: 4.5370\n",
      "Epoch 1287/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 12.4159 - val_loss: 4.5314\n",
      "Epoch 1288/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 12.6247 - val_loss: 4.5566\n",
      "Epoch 1289/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 12.3661 - val_loss: 4.5575\n",
      "Epoch 1290/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 12.5564 - val_loss: 4.5710\n",
      "Epoch 1291/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 12.9536 - val_loss: 4.5615\n",
      "Epoch 1292/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 12.5591 - val_loss: 4.6175\n",
      "Epoch 1293/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 12.1146 - val_loss: 4.5597\n",
      "Epoch 1294/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.4596 - val_loss: 4.5352\n",
      "Epoch 1295/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 12.2360 - val_loss: 4.4666\n",
      "Epoch 1296/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 12.8896 - val_loss: 4.4719\n",
      "Epoch 1297/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.7285 - val_loss: 4.4828\n",
      "Epoch 1298/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 12.8236 - val_loss: 4.5338\n",
      "Epoch 1299/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 12.5999 - val_loss: 4.5647\n",
      "Epoch 1300/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 13.1850 - val_loss: 4.4598\n",
      "Epoch 1301/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 12.7339 - val_loss: 4.4966\n",
      "Epoch 1302/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 12.4861 - val_loss: 4.5273\n",
      "Epoch 1303/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.5222 - val_loss: 4.5694\n",
      "Epoch 1304/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 12.5730 - val_loss: 4.5255\n",
      "Epoch 1305/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 12.7705 - val_loss: 4.5288\n",
      "Epoch 1306/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 12.4037 - val_loss: 4.5195\n",
      "Epoch 1307/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 12.4522 - val_loss: 4.5415\n",
      "Epoch 1308/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 13.1200 - val_loss: 4.4646\n",
      "Epoch 1309/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 117us/step - loss: 12.4912 - val_loss: 4.4637\n",
      "Epoch 1310/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.5402 - val_loss: 4.5201\n",
      "Epoch 1311/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 13.3779 - val_loss: 4.4088\n",
      "Epoch 1312/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 12.4652 - val_loss: 4.4240\n",
      "Epoch 1313/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 12.8227 - val_loss: 4.3975\n",
      "Epoch 1314/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 12.7868 - val_loss: 4.4860\n",
      "Epoch 1315/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 12.4982 - val_loss: 4.5133\n",
      "Epoch 1316/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 12.5051 - val_loss: 4.5449\n",
      "Epoch 1317/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.8568 - val_loss: 4.4378\n",
      "Epoch 1318/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 13.3780 - val_loss: 4.4143\n",
      "Epoch 1319/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 12.1886 - val_loss: 4.4642\n",
      "Epoch 1320/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 12.0914 - val_loss: 4.6295\n",
      "Epoch 1321/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 12.3282 - val_loss: 4.4268\n",
      "Epoch 1322/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 12.3839 - val_loss: 4.3478\n",
      "Epoch 1323/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.3052 - val_loss: 4.3543\n",
      "Epoch 1324/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 12.0596 - val_loss: 4.3849\n",
      "Epoch 1325/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.8210 - val_loss: 4.4260\n",
      "Epoch 1326/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.4591 - val_loss: 4.3936\n",
      "Epoch 1327/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 12.3399 - val_loss: 4.4299\n",
      "Epoch 1328/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 12.4432 - val_loss: 4.4007\n",
      "Epoch 1329/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.7863 - val_loss: 4.3627\n",
      "Epoch 1330/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.5115 - val_loss: 4.3568\n",
      "Epoch 1331/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 12.6196 - val_loss: 4.3217\n",
      "Epoch 1332/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 12.8468 - val_loss: 4.3410\n",
      "Epoch 1333/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 12.4817 - val_loss: 4.4429\n",
      "Epoch 1334/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 12.3742 - val_loss: 4.4555\n",
      "Epoch 1335/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 11.7803 - val_loss: 4.4304\n",
      "Epoch 1336/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 12.7674 - val_loss: 4.2888\n",
      "Epoch 1337/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 12.5212 - val_loss: 4.2604\n",
      "Epoch 1338/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.3974 - val_loss: 4.2900\n",
      "Epoch 1339/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.9963 - val_loss: 4.4564\n",
      "Epoch 1340/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.4790 - val_loss: 4.3911\n",
      "Epoch 1341/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 12.0978 - val_loss: 4.3345\n",
      "Epoch 1342/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 12.6819 - val_loss: 4.3271\n",
      "Epoch 1343/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 12.5038 - val_loss: 4.3215\n",
      "Epoch 1344/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.0944 - val_loss: 4.3446\n",
      "Epoch 1345/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 12.2879 - val_loss: 4.3739\n",
      "Epoch 1346/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 12.4893 - val_loss: 4.3876\n",
      "Epoch 1347/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 12.2820 - val_loss: 4.3361\n",
      "Epoch 1348/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 12.8078 - val_loss: 4.3465\n",
      "Epoch 1349/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 12.5002 - val_loss: 4.3017\n",
      "Epoch 1350/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.2783 - val_loss: 4.3708\n",
      "Epoch 1351/10000\n",
      "90/90 [==============================] - 0s 82us/step - loss: 12.3643 - val_loss: 4.3024\n",
      "Epoch 1352/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 12.3976 - val_loss: 4.2651\n",
      "Epoch 1353/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 12.5719 - val_loss: 4.2583\n",
      "Epoch 1354/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 12.1633 - val_loss: 4.2513\n",
      "Epoch 1355/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 12.1083 - val_loss: 4.2981\n",
      "Epoch 1356/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.3468 - val_loss: 4.2291\n",
      "Epoch 1357/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 12.1706 - val_loss: 4.2598\n",
      "Epoch 1358/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.1228 - val_loss: 4.2762\n",
      "Epoch 1359/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 12.2184 - val_loss: 4.2987\n",
      "Epoch 1360/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.3704 - val_loss: 4.2703\n",
      "Epoch 1361/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.2005 - val_loss: 4.2218\n",
      "Epoch 1362/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.8667 - val_loss: 4.2063\n",
      "Epoch 1363/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 12.2323 - val_loss: 4.2595\n",
      "Epoch 1364/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 12.2015 - val_loss: 4.3013\n",
      "Epoch 1365/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.7108 - val_loss: 4.3104\n",
      "Epoch 1366/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 12.0341 - val_loss: 4.2284\n",
      "Epoch 1367/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 12.7224 - val_loss: 4.2297\n",
      "Epoch 1368/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 12.6650 - val_loss: 4.2378\n",
      "Epoch 1369/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 12.5488 - val_loss: 4.2862\n",
      "Epoch 1370/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 12.3917 - val_loss: 4.4158\n",
      "Epoch 1371/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 12.0872 - val_loss: 4.3105\n",
      "Epoch 1372/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 12.3257 - val_loss: 4.1777\n",
      "Epoch 1373/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 11.8742 - val_loss: 4.1525\n",
      "Epoch 1374/10000\n",
      "90/90 [==============================] - 0s 82us/step - loss: 12.1102 - val_loss: 4.1631\n",
      "Epoch 1375/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 11.5732 - val_loss: 4.2087\n",
      "Epoch 1376/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.2927 - val_loss: 4.2012\n",
      "Epoch 1377/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 12.2705 - val_loss: 4.1968\n",
      "Epoch 1378/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 11.8387 - val_loss: 4.2408\n",
      "Epoch 1379/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 12.5314 - val_loss: 4.2612\n",
      "Epoch 1380/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.0824 - val_loss: 4.2545\n",
      "Epoch 1381/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.8857 - val_loss: 4.2765\n",
      "Epoch 1382/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.2649 - val_loss: 4.2101\n",
      "Epoch 1383/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.8334 - val_loss: 4.1599\n",
      "Epoch 1384/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.7633 - val_loss: 4.1083\n",
      "Epoch 1385/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.8882 - val_loss: 4.1429\n",
      "Epoch 1386/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 12.2518 - val_loss: 4.2397\n",
      "Epoch 1387/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 12.3554 - val_loss: 4.2141\n",
      "Epoch 1388/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.8622 - val_loss: 4.1774\n",
      "Epoch 1389/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 11.9879 - val_loss: 4.1563\n",
      "Epoch 1390/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 12.1093 - val_loss: 4.1320\n",
      "Epoch 1391/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 12.2708 - val_loss: 4.1264\n",
      "Epoch 1392/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 12.2301 - val_loss: 4.1455\n",
      "Epoch 1393/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.9504 - val_loss: 4.1217\n",
      "Epoch 1394/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 11.8988 - val_loss: 4.1320\n",
      "Epoch 1395/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.9787 - val_loss: 4.1515\n",
      "Epoch 1396/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.2940 - val_loss: 4.1505\n",
      "Epoch 1397/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.2656 - val_loss: 4.1777\n",
      "Epoch 1398/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 12.0038 - val_loss: 4.1399\n",
      "Epoch 1399/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 11.9800 - val_loss: 4.1221\n",
      "Epoch 1400/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 11.5956 - val_loss: 4.1530\n",
      "Epoch 1401/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 11.7947 - val_loss: 4.1356\n",
      "Epoch 1402/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.9019 - val_loss: 4.1268\n",
      "Epoch 1403/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 11.9822 - val_loss: 4.1780\n",
      "Epoch 1404/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.9785 - val_loss: 4.1713\n",
      "Epoch 1405/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.5425 - val_loss: 4.1775\n",
      "Epoch 1406/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.9985 - val_loss: 4.1866\n",
      "Epoch 1407/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.8245 - val_loss: 4.1627\n",
      "Epoch 1408/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.7684 - val_loss: 4.1315\n",
      "Epoch 1409/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.7684 - val_loss: 4.1220\n",
      "Epoch 1410/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 12.1114 - val_loss: 4.1113\n",
      "Epoch 1411/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.8870 - val_loss: 4.0916\n",
      "Epoch 1412/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 12.3664 - val_loss: 4.0621\n",
      "Epoch 1413/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 12.0707 - val_loss: 4.0564\n",
      "Epoch 1414/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.7462 - val_loss: 4.1612\n",
      "Epoch 1415/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.2799 - val_loss: 4.0656\n",
      "Epoch 1416/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.7341 - val_loss: 4.0791\n",
      "Epoch 1417/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.8246 - val_loss: 4.1274\n",
      "Epoch 1418/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 12.0647 - val_loss: 4.0874\n",
      "Epoch 1419/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.9336 - val_loss: 4.1417\n",
      "Epoch 1420/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 11.5002 - val_loss: 4.2575\n",
      "Epoch 1421/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.6292 - val_loss: 4.0838\n",
      "Epoch 1422/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 12.4042 - val_loss: 4.0882\n",
      "Epoch 1423/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.8924 - val_loss: 4.0755\n",
      "Epoch 1424/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 12.3491 - val_loss: 4.0737\n",
      "Epoch 1425/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.0501 - val_loss: 4.0811\n",
      "Epoch 1426/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.2403 - val_loss: 4.1136\n",
      "Epoch 1427/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.3997 - val_loss: 4.0770\n",
      "Epoch 1428/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.7645 - val_loss: 4.0099\n",
      "Epoch 1429/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.6942 - val_loss: 4.0427\n",
      "Epoch 1430/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.5105 - val_loss: 4.0467\n",
      "Epoch 1431/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.7532 - val_loss: 4.0828\n",
      "Epoch 1432/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.5530 - val_loss: 4.1469\n",
      "Epoch 1433/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.3178 - val_loss: 4.1055\n",
      "Epoch 1434/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.6804 - val_loss: 4.0586\n",
      "Epoch 1435/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 12.2191 - val_loss: 4.0604\n",
      "Epoch 1436/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.3779 - val_loss: 3.9920\n",
      "Epoch 1437/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.0192 - val_loss: 4.0530\n",
      "Epoch 1438/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.2841 - val_loss: 4.0203\n",
      "Epoch 1439/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 12.0091 - val_loss: 4.0518\n",
      "Epoch 1440/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 12.3239 - val_loss: 4.0765\n",
      "Epoch 1441/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.7612 - val_loss: 4.0618\n",
      "Epoch 1442/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.8216 - val_loss: 4.1095\n",
      "Epoch 1443/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 11.8955 - val_loss: 4.0862\n",
      "Epoch 1444/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 12.0862 - val_loss: 4.0506\n",
      "Epoch 1445/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.2850 - val_loss: 4.0415\n",
      "Epoch 1446/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 12.1508 - val_loss: 4.0326\n",
      "Epoch 1447/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.9640 - val_loss: 4.0565\n",
      "Epoch 1448/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.9750 - val_loss: 4.0271\n",
      "Epoch 1449/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.7405 - val_loss: 4.0177\n",
      "Epoch 1450/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 12.0189 - val_loss: 3.9956\n",
      "Epoch 1451/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.2244 - val_loss: 3.9528\n",
      "Epoch 1452/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.8110 - val_loss: 3.9722\n",
      "Epoch 1453/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.7539 - val_loss: 3.9952\n",
      "Epoch 1454/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 12.0561 - val_loss: 4.0229\n",
      "Epoch 1455/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.8128 - val_loss: 4.0387\n",
      "Epoch 1456/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.9390 - val_loss: 4.1023\n",
      "Epoch 1457/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 12.1968 - val_loss: 4.0157\n",
      "Epoch 1458/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.4977 - val_loss: 3.9934\n",
      "Epoch 1459/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.7627 - val_loss: 4.0055\n",
      "Epoch 1460/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.8020 - val_loss: 4.0259\n",
      "Epoch 1461/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.3754 - val_loss: 4.0104\n",
      "Epoch 1462/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 12.3662 - val_loss: 3.9804\n",
      "Epoch 1463/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 101us/step - loss: 12.0777 - val_loss: 3.9737\n",
      "Epoch 1464/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 11.7328 - val_loss: 3.9431\n",
      "Epoch 1465/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.5556 - val_loss: 4.0141\n",
      "Epoch 1466/10000\n",
      "90/90 [==============================] - 0s 78us/step - loss: 11.5375 - val_loss: 4.0683\n",
      "Epoch 1467/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.2393 - val_loss: 4.0119\n",
      "Epoch 1468/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 12.1625 - val_loss: 4.0986\n",
      "Epoch 1469/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.7756 - val_loss: 4.0403\n",
      "Epoch 1470/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.1591 - val_loss: 4.0365\n",
      "Epoch 1471/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 12.1990 - val_loss: 4.0130\n",
      "Epoch 1472/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.6236 - val_loss: 3.9943\n",
      "Epoch 1473/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.4564 - val_loss: 4.0104\n",
      "Epoch 1474/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 12.0614 - val_loss: 3.9771\n",
      "Epoch 1475/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.4422 - val_loss: 3.9634\n",
      "Epoch 1476/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.5528 - val_loss: 3.9421\n",
      "Epoch 1477/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.1740 - val_loss: 3.9743\n",
      "Epoch 1478/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.6455 - val_loss: 3.9752\n",
      "Epoch 1479/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.9029 - val_loss: 4.0595\n",
      "Epoch 1480/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.6780 - val_loss: 3.9980\n",
      "Epoch 1481/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 12.1529 - val_loss: 3.9776\n",
      "Epoch 1482/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.7947 - val_loss: 3.9895\n",
      "Epoch 1483/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 11.6290 - val_loss: 3.9302\n",
      "Epoch 1484/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.2398 - val_loss: 3.9231\n",
      "Epoch 1485/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.9297 - val_loss: 3.9577\n",
      "Epoch 1486/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.9768 - val_loss: 3.9412\n",
      "Epoch 1487/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.3736 - val_loss: 3.9806\n",
      "Epoch 1488/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 12.3362 - val_loss: 3.9814\n",
      "Epoch 1489/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 12.3309 - val_loss: 3.9810\n",
      "Epoch 1490/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.6479 - val_loss: 3.9842\n",
      "Epoch 1491/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.1927 - val_loss: 3.9709\n",
      "Epoch 1492/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.1668 - val_loss: 3.9726\n",
      "Epoch 1493/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.5894 - val_loss: 3.9489\n",
      "Epoch 1494/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.3953 - val_loss: 3.9589\n",
      "Epoch 1495/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.5793 - val_loss: 3.9599\n",
      "Epoch 1496/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 11.1426 - val_loss: 3.9324\n",
      "Epoch 1497/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.4226 - val_loss: 3.9566\n",
      "Epoch 1498/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.7857 - val_loss: 3.9564\n",
      "Epoch 1499/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.7006 - val_loss: 3.9453\n",
      "Epoch 1500/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.5952 - val_loss: 3.9385\n",
      "Epoch 1501/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 11.5919 - val_loss: 3.9352\n",
      "Epoch 1502/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.9942 - val_loss: 3.9088\n",
      "Epoch 1503/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 11.5935 - val_loss: 3.9205\n",
      "Epoch 1504/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.7814 - val_loss: 3.9240\n",
      "Epoch 1505/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.4706 - val_loss: 3.9302\n",
      "Epoch 1506/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.7110 - val_loss: 3.9394\n",
      "Epoch 1507/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.5610 - val_loss: 3.9235\n",
      "Epoch 1508/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.8406 - val_loss: 3.9177\n",
      "Epoch 1509/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.7519 - val_loss: 3.9205\n",
      "Epoch 1510/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.6884 - val_loss: 3.9517\n",
      "Epoch 1511/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.0426 - val_loss: 3.9606\n",
      "Epoch 1512/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.8800 - val_loss: 3.9591\n",
      "Epoch 1513/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.2494 - val_loss: 3.9471\n",
      "Epoch 1514/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 12.1430 - val_loss: 3.9698\n",
      "Epoch 1515/10000\n",
      "90/90 [==============================] - 0s 283us/step - loss: 12.2103 - val_loss: 3.9496\n",
      "Epoch 1516/10000\n",
      "90/90 [==============================] - 0s 240us/step - loss: 11.6930 - val_loss: 3.9421\n",
      "Epoch 1517/10000\n",
      "90/90 [==============================] - 0s 248us/step - loss: 11.3851 - val_loss: 4.0199\n",
      "Epoch 1518/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 11.8821 - val_loss: 3.9198\n",
      "Epoch 1519/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 11.5636 - val_loss: 3.9436\n",
      "Epoch 1520/10000\n",
      "90/90 [==============================] - 0s 290us/step - loss: 11.8422 - val_loss: 3.9664\n",
      "Epoch 1521/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 11.7162 - val_loss: 3.9220\n",
      "Epoch 1522/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 11.5478 - val_loss: 3.9247\n",
      "Epoch 1523/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.3910 - val_loss: 3.9076\n",
      "Epoch 1524/10000\n",
      "90/90 [==============================] - 0s 201us/step - loss: 11.7600 - val_loss: 3.9078\n",
      "Epoch 1525/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.7149 - val_loss: 3.9341\n",
      "Epoch 1526/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.2274 - val_loss: 3.8792\n",
      "Epoch 1527/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.6798 - val_loss: 3.8884\n",
      "Epoch 1528/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 11.7530 - val_loss: 3.9016\n",
      "Epoch 1529/10000\n",
      "90/90 [==============================] - 0s 305us/step - loss: 11.3076 - val_loss: 3.9550\n",
      "Epoch 1530/10000\n",
      "90/90 [==============================] - 0s 168us/step - loss: 11.2736 - val_loss: 3.9294\n",
      "Epoch 1531/10000\n",
      "90/90 [==============================] - 0s 350us/step - loss: 11.7185 - val_loss: 3.9932\n",
      "Epoch 1532/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.4456 - val_loss: 3.9371\n",
      "Epoch 1533/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.6288 - val_loss: 3.9433\n",
      "Epoch 1534/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 11.5434 - val_loss: 3.9235\n",
      "Epoch 1535/10000\n",
      "90/90 [==============================] - 0s 172us/step - loss: 11.2425 - val_loss: 3.9036\n",
      "Epoch 1536/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 11.6156 - val_loss: 3.9180\n",
      "Epoch 1537/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 11.7776 - val_loss: 3.9045\n",
      "Epoch 1538/10000\n",
      "90/90 [==============================] - 0s 179us/step - loss: 11.9347 - val_loss: 3.8878\n",
      "Epoch 1539/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 11.4791 - val_loss: 3.8618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1540/10000\n",
      "90/90 [==============================] - 0s 190us/step - loss: 11.4359 - val_loss: 3.8601\n",
      "Epoch 1541/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 11.1770 - val_loss: 3.8643\n",
      "Epoch 1542/10000\n",
      "90/90 [==============================] - 0s 205us/step - loss: 11.6664 - val_loss: 3.8755\n",
      "Epoch 1543/10000\n",
      "90/90 [==============================] - 0s 170us/step - loss: 11.6025 - val_loss: 3.8970\n",
      "Epoch 1544/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 11.6987 - val_loss: 3.8940\n",
      "Epoch 1545/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 11.0330 - val_loss: 3.9206\n",
      "Epoch 1546/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 11.6016 - val_loss: 3.9224\n",
      "Epoch 1547/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 11.6783 - val_loss: 3.9418\n",
      "Epoch 1548/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 11.5890 - val_loss: 3.9262\n",
      "Epoch 1549/10000\n",
      "90/90 [==============================] - 0s 232us/step - loss: 11.6413 - val_loss: 3.9153\n",
      "Epoch 1550/10000\n",
      "90/90 [==============================] - 0s 191us/step - loss: 11.5901 - val_loss: 3.9172\n",
      "Epoch 1551/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 11.7307 - val_loss: 3.9081\n",
      "Epoch 1552/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 11.3362 - val_loss: 3.8969\n",
      "Epoch 1553/10000\n",
      "90/90 [==============================] - 0s 237us/step - loss: 11.9849 - val_loss: 3.9090\n",
      "Epoch 1554/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.0717 - val_loss: 3.8764\n",
      "Epoch 1555/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 10.8922 - val_loss: 3.8264\n",
      "Epoch 1556/10000\n",
      "90/90 [==============================] - 0s 367us/step - loss: 11.4736 - val_loss: 3.8042\n",
      "Epoch 1557/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.9287 - val_loss: 3.8509\n",
      "Epoch 1558/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.0658 - val_loss: 3.8544\n",
      "Epoch 1559/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.8468 - val_loss: 3.9064\n",
      "Epoch 1560/10000\n",
      "90/90 [==============================] - 0s 222us/step - loss: 11.6405 - val_loss: 3.8861\n",
      "Epoch 1561/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 11.5204 - val_loss: 3.8920\n",
      "Epoch 1562/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.3513 - val_loss: 3.9202\n",
      "Epoch 1563/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.2738 - val_loss: 3.8924\n",
      "Epoch 1564/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.5875 - val_loss: 3.9181\n",
      "Epoch 1565/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 12.0421 - val_loss: 3.9717\n",
      "Epoch 1566/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 11.6488 - val_loss: 3.8691\n",
      "Epoch 1567/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.5475 - val_loss: 3.8853\n",
      "Epoch 1568/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.4441 - val_loss: 3.9035\n",
      "Epoch 1569/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 12.0550 - val_loss: 3.9057\n",
      "Epoch 1570/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.6582 - val_loss: 3.8936\n",
      "Epoch 1571/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.6903 - val_loss: 3.8320\n",
      "Epoch 1572/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.4426 - val_loss: 3.8214\n",
      "Epoch 1573/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.4002 - val_loss: 3.8207\n",
      "Epoch 1574/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.5943 - val_loss: 3.8299\n",
      "Epoch 1575/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.2491 - val_loss: 3.8684\n",
      "Epoch 1576/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.7874 - val_loss: 3.9308\n",
      "Epoch 1577/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 11.2503 - val_loss: 3.9133\n",
      "Epoch 1578/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.2169 - val_loss: 3.8906\n",
      "Epoch 1579/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 11.8429 - val_loss: 3.8784\n",
      "Epoch 1580/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.3318 - val_loss: 3.8815\n",
      "Epoch 1581/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.3374 - val_loss: 3.8417\n",
      "Epoch 1582/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.5492 - val_loss: 3.8627\n",
      "Epoch 1583/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 11.1523 - val_loss: 3.8568\n",
      "Epoch 1584/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.9443 - val_loss: 3.8643\n",
      "Epoch 1585/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.5325 - val_loss: 3.8570\n",
      "Epoch 1586/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.5044 - val_loss: 3.8385\n",
      "Epoch 1587/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.5028 - val_loss: 3.8524\n",
      "Epoch 1588/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 11.3599 - val_loss: 3.8591\n",
      "Epoch 1589/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.4593 - val_loss: 3.8697\n",
      "Epoch 1590/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 11.8350 - val_loss: 3.9068\n",
      "Epoch 1591/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.3920 - val_loss: 3.8808\n",
      "Epoch 1592/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 11.2809 - val_loss: 3.8548\n",
      "Epoch 1593/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.5260 - val_loss: 3.8718\n",
      "Epoch 1594/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.2771 - val_loss: 3.8594\n",
      "Epoch 1595/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 11.6007 - val_loss: 3.8639\n",
      "Epoch 1596/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.4306 - val_loss: 3.8651\n",
      "Epoch 1597/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.4559 - val_loss: 3.8663\n",
      "Epoch 1598/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.7447 - val_loss: 3.8690\n",
      "Epoch 1599/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.6203 - val_loss: 3.8528\n",
      "Epoch 1600/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.9987 - val_loss: 3.8258\n",
      "Epoch 1601/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 11.3006 - val_loss: 3.8087\n",
      "Epoch 1602/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.4172 - val_loss: 3.8342\n",
      "Epoch 1603/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 11.3914 - val_loss: 3.8253\n",
      "Epoch 1604/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 11.3671 - val_loss: 3.8551\n",
      "Epoch 1605/10000\n",
      "90/90 [==============================] - 0s 185us/step - loss: 11.3491 - val_loss: 3.8705\n",
      "Epoch 1606/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 11.2874 - val_loss: 3.8761\n",
      "Epoch 1607/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 11.3719 - val_loss: 3.8503\n",
      "Epoch 1608/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.8249 - val_loss: 3.8547\n",
      "Epoch 1609/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 11.5864 - val_loss: 3.8497\n",
      "Epoch 1610/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 11.4664 - val_loss: 3.8419\n",
      "Epoch 1611/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.0370 - val_loss: 3.8167\n",
      "Epoch 1612/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 11.7695 - val_loss: 3.8231\n",
      "Epoch 1613/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.6051 - val_loss: 3.8368\n",
      "Epoch 1614/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.3929 - val_loss: 3.8297\n",
      "Epoch 1615/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 11.6406 - val_loss: 3.8409\n",
      "Epoch 1616/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 11.3584 - val_loss: 3.8504\n",
      "Epoch 1617/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.2240 - val_loss: 3.8264\n",
      "Epoch 1618/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.7873 - val_loss: 3.8450\n",
      "Epoch 1619/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.1959 - val_loss: 3.8313\n",
      "Epoch 1620/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 11.0828 - val_loss: 3.7981\n",
      "Epoch 1621/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.4992 - val_loss: 3.8158\n",
      "Epoch 1622/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.7026 - val_loss: 3.8301\n",
      "Epoch 1623/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 11.0773 - val_loss: 3.8238\n",
      "Epoch 1624/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.2463 - val_loss: 3.8461\n",
      "Epoch 1625/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.1860 - val_loss: 3.8522\n",
      "Epoch 1626/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.1129 - val_loss: 3.8389\n",
      "Epoch 1627/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.6193 - val_loss: 3.8561\n",
      "Epoch 1628/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.1534 - val_loss: 3.8382\n",
      "Epoch 1629/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.1975 - val_loss: 3.8296\n",
      "Epoch 1630/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.6583 - val_loss: 3.8120\n",
      "Epoch 1631/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 11.4949 - val_loss: 3.8740\n",
      "Epoch 1632/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.7341 - val_loss: 3.8926\n",
      "Epoch 1633/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.3860 - val_loss: 3.7999\n",
      "Epoch 1634/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.8602 - val_loss: 3.8294\n",
      "Epoch 1635/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.1276 - val_loss: 3.7974\n",
      "Epoch 1636/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 11.4646 - val_loss: 3.8130\n",
      "Epoch 1637/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.7447 - val_loss: 3.8856\n",
      "Epoch 1638/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.6070 - val_loss: 3.8990\n",
      "Epoch 1639/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.8087 - val_loss: 3.8692\n",
      "Epoch 1640/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.8100 - val_loss: 3.8558\n",
      "Epoch 1641/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.4070 - val_loss: 3.8688\n",
      "Epoch 1642/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 12.0217 - val_loss: 3.8409\n",
      "Epoch 1643/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 10.8003 - val_loss: 3.8053\n",
      "Epoch 1644/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 10.9176 - val_loss: 3.7929\n",
      "Epoch 1645/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.4494 - val_loss: 3.7997\n",
      "Epoch 1646/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.5853 - val_loss: 3.8155\n",
      "Epoch 1647/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.7400 - val_loss: 3.8137\n",
      "Epoch 1648/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.3356 - val_loss: 3.7981\n",
      "Epoch 1649/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.9806 - val_loss: 3.8283\n",
      "Epoch 1650/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.8705 - val_loss: 3.8265\n",
      "Epoch 1651/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.0476 - val_loss: 3.8019\n",
      "Epoch 1652/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 11.7988 - val_loss: 3.8531\n",
      "Epoch 1653/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.2587 - val_loss: 3.8150\n",
      "Epoch 1654/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.0770 - val_loss: 3.7966\n",
      "Epoch 1655/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.5674 - val_loss: 3.7767\n",
      "Epoch 1656/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.2731 - val_loss: 3.7878\n",
      "Epoch 1657/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.2942 - val_loss: 3.8379\n",
      "Epoch 1658/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.0733 - val_loss: 3.8185\n",
      "Epoch 1659/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.1020 - val_loss: 3.8216\n",
      "Epoch 1660/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.1673 - val_loss: 3.8406\n",
      "Epoch 1661/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 11.6825 - val_loss: 3.8414\n",
      "Epoch 1662/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.2303 - val_loss: 3.8396\n",
      "Epoch 1663/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.7114 - val_loss: 3.8197\n",
      "Epoch 1664/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.7175 - val_loss: 3.8283\n",
      "Epoch 1665/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.3452 - val_loss: 3.7956\n",
      "Epoch 1666/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.4749 - val_loss: 3.8140\n",
      "Epoch 1667/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.2721 - val_loss: 3.8070\n",
      "Epoch 1668/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.6285 - val_loss: 3.7677\n",
      "Epoch 1669/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.3681 - val_loss: 3.7590\n",
      "Epoch 1670/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.5350 - val_loss: 3.7806\n",
      "Epoch 1671/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.4317 - val_loss: 3.8306\n",
      "Epoch 1672/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.4752 - val_loss: 3.8349\n",
      "Epoch 1673/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.1071 - val_loss: 3.8410\n",
      "Epoch 1674/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.8185 - val_loss: 3.8377\n",
      "Epoch 1675/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.2767 - val_loss: 3.8185\n",
      "Epoch 1676/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.5436 - val_loss: 3.8307\n",
      "Epoch 1677/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.2752 - val_loss: 3.7918\n",
      "Epoch 1678/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.3036 - val_loss: 3.7815\n",
      "Epoch 1679/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.8486 - val_loss: 3.7908\n",
      "Epoch 1680/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.5792 - val_loss: 3.7913\n",
      "Epoch 1681/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.5029 - val_loss: 3.7963\n",
      "Epoch 1682/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.2429 - val_loss: 3.7633\n",
      "Epoch 1683/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.5817 - val_loss: 3.7615\n",
      "Epoch 1684/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 11.8007 - val_loss: 3.7831\n",
      "Epoch 1685/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.9531 - val_loss: 3.7525\n",
      "Epoch 1686/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.7238 - val_loss: 3.7865\n",
      "Epoch 1687/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.1525 - val_loss: 3.7962\n",
      "Epoch 1688/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.1426 - val_loss: 3.8235\n",
      "Epoch 1689/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 11.3084 - val_loss: 3.8298\n",
      "Epoch 1690/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.4558 - val_loss: 3.8232\n",
      "Epoch 1691/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 11.4795 - val_loss: 3.8194\n",
      "Epoch 1692/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.8493 - val_loss: 3.8099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1693/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 11.3513 - val_loss: 3.8438\n",
      "Epoch 1694/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.0500 - val_loss: 3.8032\n",
      "Epoch 1695/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.2388 - val_loss: 3.8359\n",
      "Epoch 1696/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.4829 - val_loss: 3.8173\n",
      "Epoch 1697/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.6446 - val_loss: 3.7574\n",
      "Epoch 1698/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.2403 - val_loss: 3.7514\n",
      "Epoch 1699/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.7617 - val_loss: 3.7621\n",
      "Epoch 1700/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.7875 - val_loss: 3.7859\n",
      "Epoch 1701/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.3974 - val_loss: 3.7639\n",
      "Epoch 1702/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 10.5228 - val_loss: 3.7583\n",
      "Epoch 1703/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.5264 - val_loss: 3.8130\n",
      "Epoch 1704/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.6816 - val_loss: 3.8365\n",
      "Epoch 1705/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.1172 - val_loss: 3.8036\n",
      "Epoch 1706/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.3070 - val_loss: 3.8310\n",
      "Epoch 1707/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 12.1384 - val_loss: 3.8668\n",
      "Epoch 1708/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.6810 - val_loss: 3.7933\n",
      "Epoch 1709/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.1649 - val_loss: 3.7802\n",
      "Epoch 1710/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.6467 - val_loss: 3.7692\n",
      "Epoch 1711/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.3579 - val_loss: 3.7757\n",
      "Epoch 1712/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 11.4830 - val_loss: 3.8173\n",
      "Epoch 1713/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.5017 - val_loss: 3.8118\n",
      "Epoch 1714/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.2086 - val_loss: 3.7905\n",
      "Epoch 1715/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.3361 - val_loss: 3.7833\n",
      "Epoch 1716/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.3926 - val_loss: 3.8043\n",
      "Epoch 1717/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.8579 - val_loss: 3.7700\n",
      "Epoch 1718/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.2369 - val_loss: 3.7701\n",
      "Epoch 1719/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.2717 - val_loss: 3.7983\n",
      "Epoch 1720/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.0300 - val_loss: 3.7633\n",
      "Epoch 1721/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.3357 - val_loss: 3.7652\n",
      "Epoch 1722/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.4419 - val_loss: 3.7717\n",
      "Epoch 1723/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 11.2742 - val_loss: 3.7926\n",
      "Epoch 1724/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.8823 - val_loss: 3.7963\n",
      "Epoch 1725/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.2381 - val_loss: 3.8014\n",
      "Epoch 1726/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.5813 - val_loss: 3.8193\n",
      "Epoch 1727/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.1294 - val_loss: 3.7922\n",
      "Epoch 1728/10000\n",
      "90/90 [==============================] - 0s 168us/step - loss: 11.2353 - val_loss: 3.7700\n",
      "Epoch 1729/10000\n",
      "90/90 [==============================] - 0s 237us/step - loss: 11.4745 - val_loss: 3.7859\n",
      "Epoch 1730/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 11.3646 - val_loss: 3.7908\n",
      "Epoch 1731/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 11.1351 - val_loss: 3.7650\n",
      "Epoch 1732/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 11.2039 - val_loss: 3.7838\n",
      "Epoch 1733/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.4866 - val_loss: 3.7499\n",
      "Epoch 1734/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.8247 - val_loss: 3.7464\n",
      "Epoch 1735/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.6752 - val_loss: 3.7309\n",
      "Epoch 1736/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.6089 - val_loss: 3.7441\n",
      "Epoch 1737/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 11.4919 - val_loss: 3.7596\n",
      "Epoch 1738/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.2884 - val_loss: 3.7458\n",
      "Epoch 1739/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.0658 - val_loss: 3.7611\n",
      "Epoch 1740/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.8862 - val_loss: 3.7640\n",
      "Epoch 1741/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.6592 - val_loss: 3.8093\n",
      "Epoch 1742/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.3393 - val_loss: 3.8008\n",
      "Epoch 1743/10000\n",
      "90/90 [==============================] - 0s 175us/step - loss: 11.1722 - val_loss: 3.7822\n",
      "Epoch 1744/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 11.3646 - val_loss: 3.7830\n",
      "Epoch 1745/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.5198 - val_loss: 3.7601\n",
      "Epoch 1746/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 11.2986 - val_loss: 3.8258\n",
      "Epoch 1747/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.3878 - val_loss: 3.7866\n",
      "Epoch 1748/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.2825 - val_loss: 3.7466\n",
      "Epoch 1749/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.2621 - val_loss: 3.7625\n",
      "Epoch 1750/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.1423 - val_loss: 3.7480\n",
      "Epoch 1751/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.9120 - val_loss: 3.7762\n",
      "Epoch 1752/10000\n",
      "90/90 [==============================] - 0s 456us/step - loss: 11.1240 - val_loss: 3.8313\n",
      "Epoch 1753/10000\n",
      "90/90 [==============================] - 0s 382us/step - loss: 11.5531 - val_loss: 3.8159\n",
      "Epoch 1754/10000\n",
      "90/90 [==============================] - 0s 180us/step - loss: 11.6484 - val_loss: 3.7971\n",
      "Epoch 1755/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 11.7955 - val_loss: 3.7867\n",
      "Epoch 1756/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 11.2780 - val_loss: 3.7848\n",
      "Epoch 1757/10000\n",
      "90/90 [==============================] - 0s 204us/step - loss: 11.0212 - val_loss: 3.7591\n",
      "Epoch 1758/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 11.0506 - val_loss: 3.7412\n",
      "Epoch 1759/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 11.3887 - val_loss: 3.7396\n",
      "Epoch 1760/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 11.5959 - val_loss: 3.7520\n",
      "Epoch 1761/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.3356 - val_loss: 3.7596\n",
      "Epoch 1762/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.9853 - val_loss: 3.7283\n",
      "Epoch 1763/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 11.4925 - val_loss: 3.7620\n",
      "Epoch 1764/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.0719 - val_loss: 3.7872\n",
      "Epoch 1765/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.1584 - val_loss: 3.7769\n",
      "Epoch 1766/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 11.1005 - val_loss: 3.7600\n",
      "Epoch 1767/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.4192 - val_loss: 3.8005\n",
      "Epoch 1768/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.7001 - val_loss: 3.7693\n",
      "Epoch 1769/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 11.3951 - val_loss: 3.7373\n",
      "Epoch 1770/10000\n",
      "90/90 [==============================] - 0s 184us/step - loss: 10.9159 - val_loss: 3.7224\n",
      "Epoch 1771/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.1226 - val_loss: 3.7342\n",
      "Epoch 1772/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.0821 - val_loss: 3.7734\n",
      "Epoch 1773/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.4986 - val_loss: 3.8161\n",
      "Epoch 1774/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.2969 - val_loss: 3.7768\n",
      "Epoch 1775/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.3862 - val_loss: 3.7568\n",
      "Epoch 1776/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.3728 - val_loss: 3.7917\n",
      "Epoch 1777/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.9827 - val_loss: 3.7935\n",
      "Epoch 1778/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.1684 - val_loss: 3.7735\n",
      "Epoch 1779/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.7020 - val_loss: 3.8006\n",
      "Epoch 1780/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.5052 - val_loss: 3.7802\n",
      "Epoch 1781/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.2759 - val_loss: 3.7775\n",
      "Epoch 1782/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.2036 - val_loss: 3.7188\n",
      "Epoch 1783/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.2860 - val_loss: 3.7694\n",
      "Epoch 1784/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.4173 - val_loss: 3.8064\n",
      "Epoch 1785/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.2890 - val_loss: 3.7145\n",
      "Epoch 1786/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.7006 - val_loss: 3.7278\n",
      "Epoch 1787/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.2571 - val_loss: 3.7110\n",
      "Epoch 1788/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 11.4106 - val_loss: 3.7105\n",
      "Epoch 1789/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 11.6046 - val_loss: 3.7637\n",
      "Epoch 1790/10000\n",
      "90/90 [==============================] - 0s 188us/step - loss: 11.1463 - val_loss: 3.7491\n",
      "Epoch 1791/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 3.231 - 0s 125us/step - loss: 11.3246 - val_loss: 3.7604\n",
      "Epoch 1792/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.5106 - val_loss: 3.7444\n",
      "Epoch 1793/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 11.2134 - val_loss: 3.7442\n",
      "Epoch 1794/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.7755 - val_loss: 3.7649\n",
      "Epoch 1795/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 10.9096 - val_loss: 3.7655\n",
      "Epoch 1796/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.2637 - val_loss: 3.7891\n",
      "Epoch 1797/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.8224 - val_loss: 3.7517\n",
      "Epoch 1798/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 11.2969 - val_loss: 3.7664\n",
      "Epoch 1799/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.1333 - val_loss: 3.7549\n",
      "Epoch 1800/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 11.1608 - val_loss: 3.7469\n",
      "Epoch 1801/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.7260 - val_loss: 3.7022\n",
      "Epoch 1802/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.5495 - val_loss: 3.7332\n",
      "Epoch 1803/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 10.7606 - val_loss: 3.6883\n",
      "Epoch 1804/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.1615 - val_loss: 3.6838\n",
      "Epoch 1805/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.3297 - val_loss: 3.7197\n",
      "Epoch 1806/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 11.2046 - val_loss: 3.7351\n",
      "Epoch 1807/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.1122 - val_loss: 3.7342\n",
      "Epoch 1808/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.0237 - val_loss: 3.7194\n",
      "Epoch 1809/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.9171 - val_loss: 3.7310\n",
      "Epoch 1810/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.0655 - val_loss: 3.7082\n",
      "Epoch 1811/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.4792 - val_loss: 3.7548\n",
      "Epoch 1812/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 11.4734 - val_loss: 3.7306\n",
      "Epoch 1813/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.2147 - val_loss: 3.7085\n",
      "Epoch 1814/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.3106 - val_loss: 3.7344\n",
      "Epoch 1815/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.1119 - val_loss: 3.7618\n",
      "Epoch 1816/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.4052 - val_loss: 3.7819\n",
      "Epoch 1817/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.9162 - val_loss: 3.8307\n",
      "Epoch 1818/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.8478 - val_loss: 3.7650\n",
      "Epoch 1819/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.1481 - val_loss: 3.7008\n",
      "Epoch 1820/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 12.0289 - val_loss: 3.7036\n",
      "Epoch 1821/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.5092 - val_loss: 3.7064\n",
      "Epoch 1822/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.4184 - val_loss: 3.7015\n",
      "Epoch 1823/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.4521 - val_loss: 3.7210\n",
      "Epoch 1824/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.1375 - val_loss: 3.7219\n",
      "Epoch 1825/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.4628 - val_loss: 3.7216\n",
      "Epoch 1826/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.0435 - val_loss: 3.7040\n",
      "Epoch 1827/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.2805 - val_loss: 3.7381\n",
      "Epoch 1828/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.4997 - val_loss: 3.7550\n",
      "Epoch 1829/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 11.0792 - val_loss: 3.7449\n",
      "Epoch 1830/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.2910 - val_loss: 3.7431\n",
      "Epoch 1831/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 11.2698 - val_loss: 3.7405\n",
      "Epoch 1832/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.5526 - val_loss: 3.7613\n",
      "Epoch 1833/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 11.1191 - val_loss: 3.6942\n",
      "Epoch 1834/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 11.3587 - val_loss: 3.6837\n",
      "Epoch 1835/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 11.4157 - val_loss: 3.7077\n",
      "Epoch 1836/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.7791 - val_loss: 3.7324\n",
      "Epoch 1837/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.2591 - val_loss: 3.7242\n",
      "Epoch 1838/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.2165 - val_loss: 3.7268\n",
      "Epoch 1839/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.8848 - val_loss: 3.7397\n",
      "Epoch 1840/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.2609 - val_loss: 3.7173\n",
      "Epoch 1841/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 11.0101 - val_loss: 3.6798\n",
      "Epoch 1842/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.2224 - val_loss: 3.7209\n",
      "Epoch 1843/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.1300 - val_loss: 3.7453\n",
      "Epoch 1844/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.4102 - val_loss: 3.7084\n",
      "Epoch 1845/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 112us/step - loss: 10.9589 - val_loss: 3.7358\n",
      "Epoch 1846/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.0986 - val_loss: 3.7149\n",
      "Epoch 1847/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.1169 - val_loss: 3.7120\n",
      "Epoch 1848/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.7282 - val_loss: 3.7524\n",
      "Epoch 1849/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.4026 - val_loss: 3.6949\n",
      "Epoch 1850/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.1722 - val_loss: 3.7034\n",
      "Epoch 1851/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.7372 - val_loss: 3.6875\n",
      "Epoch 1852/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.6763 - val_loss: 3.6899\n",
      "Epoch 1853/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 11.2204 - val_loss: 3.6933\n",
      "Epoch 1854/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.0108 - val_loss: 3.6874\n",
      "Epoch 1855/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.1640 - val_loss: 3.6968\n",
      "Epoch 1856/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.0551 - val_loss: 3.6999\n",
      "Epoch 1857/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.6701 - val_loss: 3.6959\n",
      "Epoch 1858/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.7400 - val_loss: 3.7417\n",
      "Epoch 1859/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.9009 - val_loss: 3.6804\n",
      "Epoch 1860/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.4381 - val_loss: 3.6858\n",
      "Epoch 1861/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.9847 - val_loss: 3.6676\n",
      "Epoch 1862/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 11.5894 - val_loss: 3.7128\n",
      "Epoch 1863/10000\n",
      "90/90 [==============================] - 0s 265us/step - loss: 10.8459 - val_loss: 3.6828\n",
      "Epoch 1864/10000\n",
      "90/90 [==============================] - 0s 227us/step - loss: 10.9337 - val_loss: 3.6703\n",
      "Epoch 1865/10000\n",
      "90/90 [==============================] - 0s 197us/step - loss: 11.0745 - val_loss: 3.6786\n",
      "Epoch 1866/10000\n",
      "90/90 [==============================] - 0s 173us/step - loss: 11.1975 - val_loss: 3.7238\n",
      "Epoch 1867/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 11.2374 - val_loss: 3.7324\n",
      "Epoch 1868/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 12.27 - 0s 111us/step - loss: 11.1190 - val_loss: 3.7194\n",
      "Epoch 1869/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.0074 - val_loss: 3.7195\n",
      "Epoch 1870/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.3780 - val_loss: 3.7247\n",
      "Epoch 1871/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.1914 - val_loss: 3.7373\n",
      "Epoch 1872/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.3123 - val_loss: 3.6849\n",
      "Epoch 1873/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 11.3124 - val_loss: 3.6695\n",
      "Epoch 1874/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.9815 - val_loss: 3.6649\n",
      "Epoch 1875/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.3132 - val_loss: 3.6783\n",
      "Epoch 1876/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.2647 - val_loss: 3.6586\n",
      "Epoch 1877/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.4871 - val_loss: 3.6254\n",
      "Epoch 1878/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.2496 - val_loss: 3.6559\n",
      "Epoch 1879/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.0873 - val_loss: 3.6915\n",
      "Epoch 1880/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.2214 - val_loss: 3.6742\n",
      "Epoch 1881/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 11.3925 - val_loss: 3.7039\n",
      "Epoch 1882/10000\n",
      "90/90 [==============================] - 0s 239us/step - loss: 11.5680 - val_loss: 3.6639\n",
      "Epoch 1883/10000\n",
      "90/90 [==============================] - 0s 199us/step - loss: 10.9666 - val_loss: 3.6768\n",
      "Epoch 1884/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 10.9705 - val_loss: 3.7297\n",
      "Epoch 1885/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 11.5147 - val_loss: 3.6854\n",
      "Epoch 1886/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.8689 - val_loss: 3.7780\n",
      "Epoch 1887/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 11.2680 - val_loss: 3.6939\n",
      "Epoch 1888/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.8554 - val_loss: 3.6740\n",
      "Epoch 1889/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.2051 - val_loss: 3.7440\n",
      "Epoch 1890/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 11.0158 - val_loss: 3.7372\n",
      "Epoch 1891/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 11.2726 - val_loss: 3.7728\n",
      "Epoch 1892/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.4176 - val_loss: 3.7623\n",
      "Epoch 1893/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 11.5203 - val_loss: 3.6806\n",
      "Epoch 1894/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 11.0656 - val_loss: 3.7173\n",
      "Epoch 1895/10000\n",
      "90/90 [==============================] - 0s 348us/step - loss: 11.5710 - val_loss: 3.6812\n",
      "Epoch 1896/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 10.9428 - val_loss: 3.6855\n",
      "Epoch 1897/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 10.7146 - val_loss: 3.6813\n",
      "Epoch 1898/10000\n",
      "90/90 [==============================] - 0s 235us/step - loss: 11.5007 - val_loss: 3.6934\n",
      "Epoch 1899/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 11.3473 - val_loss: 3.6085\n",
      "Epoch 1900/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.9087 - val_loss: 3.6016\n",
      "Epoch 1901/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.3470 - val_loss: 3.6026\n",
      "Epoch 1902/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.1459 - val_loss: 3.6242\n",
      "Epoch 1903/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.1136 - val_loss: 3.6408\n",
      "Epoch 1904/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.2404 - val_loss: 3.6600\n",
      "Epoch 1905/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.2094 - val_loss: 3.6793\n",
      "Epoch 1906/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.9896 - val_loss: 3.7076\n",
      "Epoch 1907/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.1641 - val_loss: 3.6997\n",
      "Epoch 1908/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.7943 - val_loss: 3.6832\n",
      "Epoch 1909/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.6494 - val_loss: 3.6591\n",
      "Epoch 1910/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.2172 - val_loss: 3.6601\n",
      "Epoch 1911/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.0109 - val_loss: 3.6199\n",
      "Epoch 1912/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.2859 - val_loss: 3.6000\n",
      "Epoch 1913/10000\n",
      "90/90 [==============================] - 0s 218us/step - loss: 10.9206 - val_loss: 3.5937\n",
      "Epoch 1914/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 11.1333 - val_loss: 3.6212\n",
      "Epoch 1915/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.4987 - val_loss: 3.6438\n",
      "Epoch 1916/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.0924 - val_loss: 3.6309\n",
      "Epoch 1917/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.8715 - val_loss: 3.6382\n",
      "Epoch 1918/10000\n",
      "90/90 [==============================] - 0s 223us/step - loss: 11.2798 - val_loss: 3.7229\n",
      "Epoch 1919/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 10.9961 - val_loss: 3.6796\n",
      "Epoch 1920/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 10.9847 - val_loss: 3.6626\n",
      "Epoch 1921/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.9476 - val_loss: 3.6765\n",
      "Epoch 1922/10000\n",
      "90/90 [==============================] - 0s 220us/step - loss: 11.3349 - val_loss: 3.6490\n",
      "Epoch 1923/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.4596 - val_loss: 3.6809\n",
      "Epoch 1924/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 11.0753 - val_loss: 3.6459\n",
      "Epoch 1925/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 11.4178 - val_loss: 3.5967\n",
      "Epoch 1926/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.7140 - val_loss: 3.6329\n",
      "Epoch 1927/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 10.8929 - val_loss: 3.6357\n",
      "Epoch 1928/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 11.3178 - val_loss: 3.6892\n",
      "Epoch 1929/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.6726 - val_loss: 3.6876\n",
      "Epoch 1930/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.9360 - val_loss: 3.5680\n",
      "Epoch 1931/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.4606 - val_loss: 3.5802\n",
      "Epoch 1932/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 11.1464 - val_loss: 3.5830\n",
      "Epoch 1933/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 11.3841 - val_loss: 3.6479\n",
      "Epoch 1934/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 11.0964 - val_loss: 3.6346\n",
      "Epoch 1935/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 10.8104 - val_loss: 3.6402\n",
      "Epoch 1936/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 10.8017 - val_loss: 3.6242\n",
      "Epoch 1937/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 10.6423 - val_loss: 3.6299\n",
      "Epoch 1938/10000\n",
      "90/90 [==============================] - 0s 176us/step - loss: 10.4115 - val_loss: 3.6207\n",
      "Epoch 1939/10000\n",
      "90/90 [==============================] - 0s 278us/step - loss: 11.1410 - val_loss: 3.6739\n",
      "Epoch 1940/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.6867 - val_loss: 3.6743\n",
      "Epoch 1941/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 10.8143 - val_loss: 3.6286\n",
      "Epoch 1942/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.3566 - val_loss: 3.6348\n",
      "Epoch 1943/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.1409 - val_loss: 3.5914\n",
      "Epoch 1944/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.1537 - val_loss: 3.5898\n",
      "Epoch 1945/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 11.3112 - val_loss: 3.5922\n",
      "Epoch 1946/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.8581 - val_loss: 3.5742\n",
      "Epoch 1947/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.1028 - val_loss: 3.6075\n",
      "Epoch 1948/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.3143 - val_loss: 3.6185\n",
      "Epoch 1949/10000\n",
      "90/90 [==============================] - 0s 173us/step - loss: 11.2650 - val_loss: 3.6376\n",
      "Epoch 1950/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.9621 - val_loss: 3.6285\n",
      "Epoch 1951/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.6586 - val_loss: 3.6106\n",
      "Epoch 1952/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 11.1869 - val_loss: 3.5964\n",
      "Epoch 1953/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.9503 - val_loss: 3.5852\n",
      "Epoch 1954/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.4861 - val_loss: 3.6277\n",
      "Epoch 1955/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.9430 - val_loss: 3.5866\n",
      "Epoch 1956/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.7055 - val_loss: 3.5777\n",
      "Epoch 1957/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.5850 - val_loss: 3.6160\n",
      "Epoch 1958/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.1241 - val_loss: 3.6021\n",
      "Epoch 1959/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.0474 - val_loss: 3.5921\n",
      "Epoch 1960/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.1829 - val_loss: 3.6292\n",
      "Epoch 1961/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.7874 - val_loss: 3.6103\n",
      "Epoch 1962/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 11.2200 - val_loss: 3.6210\n",
      "Epoch 1963/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.8629 - val_loss: 3.5952\n",
      "Epoch 1964/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.9129 - val_loss: 3.5842\n",
      "Epoch 1965/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.7991 - val_loss: 3.6073\n",
      "Epoch 1966/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.4758 - val_loss: 3.6607\n",
      "Epoch 1967/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.3996 - val_loss: 3.6478\n",
      "Epoch 1968/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.8614 - val_loss: 3.6003\n",
      "Epoch 1969/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.1005 - val_loss: 3.5929\n",
      "Epoch 1970/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.9250 - val_loss: 3.5366\n",
      "Epoch 1971/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.2772 - val_loss: 3.6050\n",
      "Epoch 1972/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 11.2346 - val_loss: 3.6058\n",
      "Epoch 1973/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.8226 - val_loss: 3.5868\n",
      "Epoch 1974/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.5874 - val_loss: 3.6023\n",
      "Epoch 1975/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.7161 - val_loss: 3.5615\n",
      "Epoch 1976/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.8486 - val_loss: 3.5725\n",
      "Epoch 1977/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.1975 - val_loss: 3.6474\n",
      "Epoch 1978/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.3310 - val_loss: 3.6224\n",
      "Epoch 1979/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.2694 - val_loss: 3.5572\n",
      "Epoch 1980/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.0167 - val_loss: 3.5396\n",
      "Epoch 1981/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.1061 - val_loss: 3.5357\n",
      "Epoch 1982/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.7019 - val_loss: 3.5671\n",
      "Epoch 1983/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.1458 - val_loss: 3.6098\n",
      "Epoch 1984/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.0442 - val_loss: 3.6013\n",
      "Epoch 1985/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.9618 - val_loss: 3.5451\n",
      "Epoch 1986/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.2407 - val_loss: 3.5487\n",
      "Epoch 1987/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.9121 - val_loss: 3.5475\n",
      "Epoch 1988/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.2439 - val_loss: 3.5783\n",
      "Epoch 1989/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.7522 - val_loss: 3.5990\n",
      "Epoch 1990/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.2155 - val_loss: 3.6009\n",
      "Epoch 1991/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 10.9230 - val_loss: 3.5751\n",
      "Epoch 1992/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.9695 - val_loss: 3.5479\n",
      "Epoch 1993/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 11.1901 - val_loss: 3.5363\n",
      "Epoch 1994/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 11.1132 - val_loss: 3.5017\n",
      "Epoch 1995/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.8216 - val_loss: 3.5251\n",
      "Epoch 1996/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.0845 - val_loss: 3.5474\n",
      "Epoch 1997/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.7842 - val_loss: 3.4916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1998/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.1796 - val_loss: 3.5261\n",
      "Epoch 1999/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.8698 - val_loss: 3.5579\n",
      "Epoch 2000/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.7180 - val_loss: 3.5535\n",
      "Epoch 2001/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.2695 - val_loss: 3.5173\n",
      "Epoch 2002/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 11.0748 - val_loss: 3.5458\n",
      "Epoch 2003/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.4251 - val_loss: 3.5582\n",
      "Epoch 2004/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.0315 - val_loss: 3.5188\n",
      "Epoch 2005/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 11.3809 - val_loss: 3.5090\n",
      "Epoch 2006/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.7264 - val_loss: 3.5204\n",
      "Epoch 2007/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.3289 - val_loss: 3.5466\n",
      "Epoch 2008/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.6443 - val_loss: 3.5036\n",
      "Epoch 2009/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.7632 - val_loss: 3.5257\n",
      "Epoch 2010/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 11.1174 - val_loss: 3.5644\n",
      "Epoch 2011/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.3975 - val_loss: 3.5539\n",
      "Epoch 2012/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 11.1388 - val_loss: 3.5606\n",
      "Epoch 2013/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 11.0295 - val_loss: 3.5285\n",
      "Epoch 2014/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.0776 - val_loss: 3.5493\n",
      "Epoch 2015/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.7839 - val_loss: 3.5289\n",
      "Epoch 2016/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.1604 - val_loss: 3.5412\n",
      "Epoch 2017/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.7641 - val_loss: 3.5518\n",
      "Epoch 2018/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 11.0639 - val_loss: 3.4984\n",
      "Epoch 2019/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.9524 - val_loss: 3.4769\n",
      "Epoch 2020/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.9780 - val_loss: 3.4750\n",
      "Epoch 2021/10000\n",
      "90/90 [==============================] - 0s 175us/step - loss: 10.3938 - val_loss: 3.4508\n",
      "Epoch 2022/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 11.0161 - val_loss: 3.4634\n",
      "Epoch 2023/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 11.0699 - val_loss: 3.4992\n",
      "Epoch 2024/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 10.8230 - val_loss: 3.5081\n",
      "Epoch 2025/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.1258 - val_loss: 3.5317\n",
      "Epoch 2026/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 11.1828 - val_loss: 3.5413\n",
      "Epoch 2027/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 10.9263 - val_loss: 3.5244\n",
      "Epoch 2028/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 11.2563 - val_loss: 3.5353\n",
      "Epoch 2029/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 11.0984 - val_loss: 3.4895\n",
      "Epoch 2030/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 11.2183 - val_loss: 3.4809\n",
      "Epoch 2031/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 11.1677 - val_loss: 3.4536\n",
      "Epoch 2032/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 11.2084 - val_loss: 3.5438\n",
      "Epoch 2033/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 11.1292 - val_loss: 3.5207\n",
      "Epoch 2034/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.8136 - val_loss: 3.4805\n",
      "Epoch 2035/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 10.9791 - val_loss: 3.4836\n",
      "Epoch 2036/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 10.9016 - val_loss: 3.4313\n",
      "Epoch 2037/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.7558 - val_loss: 3.5024\n",
      "Epoch 2038/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.8709 - val_loss: 3.4937\n",
      "Epoch 2039/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 10.8482 - val_loss: 3.4847\n",
      "Epoch 2040/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.7424 - val_loss: 3.5128\n",
      "Epoch 2041/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.7392 - val_loss: 3.5151\n",
      "Epoch 2042/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.9027 - val_loss: 3.5492\n",
      "Epoch 2043/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 11.6352 - val_loss: 3.5681\n",
      "Epoch 2044/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.4378 - val_loss: 3.4631\n",
      "Epoch 2045/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.5597 - val_loss: 3.4527\n",
      "Epoch 2046/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.7078 - val_loss: 3.4962\n",
      "Epoch 2047/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.4197 - val_loss: 3.4673\n",
      "Epoch 2048/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 11.2271 - val_loss: 3.4843\n",
      "Epoch 2049/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.9994 - val_loss: 3.4537\n",
      "Epoch 2050/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 11.1817 - val_loss: 3.4200\n",
      "Epoch 2051/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.4169 - val_loss: 3.4051\n",
      "Epoch 2052/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 11.1406 - val_loss: 3.4260\n",
      "Epoch 2053/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.2237 - val_loss: 3.4468\n",
      "Epoch 2054/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 11.0947 - val_loss: 3.4182\n",
      "Epoch 2055/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.8639 - val_loss: 3.3628\n",
      "Epoch 2056/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 11.2185 - val_loss: 3.4162\n",
      "Epoch 2057/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.0385 - val_loss: 3.4141\n",
      "Epoch 2058/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.9231 - val_loss: 3.4552\n",
      "Epoch 2059/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 11.2004 - val_loss: 3.4656\n",
      "Epoch 2060/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 11.1679 - val_loss: 3.4494\n",
      "Epoch 2061/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.6833 - val_loss: 3.4209\n",
      "Epoch 2062/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.0776 - val_loss: 3.4338\n",
      "Epoch 2063/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.3019 - val_loss: 3.4217\n",
      "Epoch 2064/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.4993 - val_loss: 3.4199\n",
      "Epoch 2065/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.7302 - val_loss: 3.4454\n",
      "Epoch 2066/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.8341 - val_loss: 3.4657\n",
      "Epoch 2067/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.1189 - val_loss: 3.4808\n",
      "Epoch 2068/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.7212 - val_loss: 3.5060\n",
      "Epoch 2069/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 11.3426 - val_loss: 3.5017\n",
      "Epoch 2070/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.1478 - val_loss: 3.4114\n",
      "Epoch 2071/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 11.2028 - val_loss: 3.4212\n",
      "Epoch 2072/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 11.0089 - val_loss: 3.4553\n",
      "Epoch 2073/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.8702 - val_loss: 3.3510\n",
      "Epoch 2074/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.0972 - val_loss: 3.4895\n",
      "Epoch 2075/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.9795 - val_loss: 3.4724\n",
      "Epoch 2076/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.8959 - val_loss: 3.4075\n",
      "Epoch 2077/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.0356 - val_loss: 3.4617\n",
      "Epoch 2078/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.6966 - val_loss: 3.4121\n",
      "Epoch 2079/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.7355 - val_loss: 3.3971\n",
      "Epoch 2080/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.8957 - val_loss: 3.3940\n",
      "Epoch 2081/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.6954 - val_loss: 3.3787\n",
      "Epoch 2082/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.5328 - val_loss: 3.3904\n",
      "Epoch 2083/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.8558 - val_loss: 3.3843\n",
      "Epoch 2084/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.6604 - val_loss: 3.3309\n",
      "Epoch 2085/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.2336 - val_loss: 3.3913\n",
      "Epoch 2086/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.9175 - val_loss: 3.4030\n",
      "Epoch 2087/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.9649 - val_loss: 3.4062\n",
      "Epoch 2088/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.9061 - val_loss: 3.3928\n",
      "Epoch 2089/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.9935 - val_loss: 3.3554\n",
      "Epoch 2090/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.5565 - val_loss: 3.2895\n",
      "Epoch 2091/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 10.7138 - val_loss: 3.3310\n",
      "Epoch 2092/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.7927 - val_loss: 3.3819\n",
      "Epoch 2093/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.8254 - val_loss: 3.3881\n",
      "Epoch 2094/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.7794 - val_loss: 3.3741\n",
      "Epoch 2095/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.6521 - val_loss: 3.3454\n",
      "Epoch 2096/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.7651 - val_loss: 3.3771\n",
      "Epoch 2097/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.6367 - val_loss: 3.3291\n",
      "Epoch 2098/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.8663 - val_loss: 3.3681\n",
      "Epoch 2099/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.9651 - val_loss: 3.4057\n",
      "Epoch 2100/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.9908 - val_loss: 3.3596\n",
      "Epoch 2101/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.5387 - val_loss: 3.3486\n",
      "Epoch 2102/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.6345 - val_loss: 3.3228\n",
      "Epoch 2103/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.0560 - val_loss: 3.3186\n",
      "Epoch 2104/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.6148 - val_loss: 3.3546\n",
      "Epoch 2105/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.9380 - val_loss: 3.4101\n",
      "Epoch 2106/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.8031 - val_loss: 3.3979\n",
      "Epoch 2107/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.6147 - val_loss: 3.3593\n",
      "Epoch 2108/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.9364 - val_loss: 3.2819\n",
      "Epoch 2109/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.8062 - val_loss: 3.2909\n",
      "Epoch 2110/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.7147 - val_loss: 3.2782\n",
      "Epoch 2111/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.7287 - val_loss: 3.3125\n",
      "Epoch 2112/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.0365 - val_loss: 3.3146\n",
      "Epoch 2113/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.8366 - val_loss: 3.2977\n",
      "Epoch 2114/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.6238 - val_loss: 3.3358\n",
      "Epoch 2115/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 11.2566 - val_loss: 3.3357\n",
      "Epoch 2116/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.1697 - val_loss: 3.3005\n",
      "Epoch 2117/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.8692 - val_loss: 3.2991\n",
      "Epoch 2118/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.5043 - val_loss: 3.2736\n",
      "Epoch 2119/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.3877 - val_loss: 3.2828\n",
      "Epoch 2120/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.6751 - val_loss: 3.3070\n",
      "Epoch 2121/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.4236 - val_loss: 3.3132\n",
      "Epoch 2122/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 11.0935 - val_loss: 3.3618\n",
      "Epoch 2123/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 11.0175 - val_loss: 3.3101\n",
      "Epoch 2124/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.4502 - val_loss: 3.3695\n",
      "Epoch 2125/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.8474 - val_loss: 3.3420\n",
      "Epoch 2126/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.8808 - val_loss: 3.3128\n",
      "Epoch 2127/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.4585 - val_loss: 3.2459\n",
      "Epoch 2128/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.8775 - val_loss: 3.2245\n",
      "Epoch 2129/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 11.0555 - val_loss: 3.2352\n",
      "Epoch 2130/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.7358 - val_loss: 3.2711\n",
      "Epoch 2131/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.4965 - val_loss: 3.2689\n",
      "Epoch 2132/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 11.0392 - val_loss: 3.3001\n",
      "Epoch 2133/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.5831 - val_loss: 3.3057\n",
      "Epoch 2134/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.8163 - val_loss: 3.3402\n",
      "Epoch 2135/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.7119 - val_loss: 3.2987\n",
      "Epoch 2136/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.0871 - val_loss: 3.2772\n",
      "Epoch 2137/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.7308 - val_loss: 3.2310\n",
      "Epoch 2138/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.2547 - val_loss: 3.2132\n",
      "Epoch 2139/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.5896 - val_loss: 3.1771\n",
      "Epoch 2140/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.8110 - val_loss: 3.1936\n",
      "Epoch 2141/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.6987 - val_loss: 3.2309\n",
      "Epoch 2142/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 11.0784 - val_loss: 3.3047\n",
      "Epoch 2143/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.9171 - val_loss: 3.3070\n",
      "Epoch 2144/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.8603 - val_loss: 3.3084\n",
      "Epoch 2145/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.0731 - val_loss: 3.2623\n",
      "Epoch 2146/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 11.0888 - val_loss: 3.2393\n",
      "Epoch 2147/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 10.4707 - val_loss: 3.1976\n",
      "Epoch 2148/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.5142 - val_loss: 3.1878\n",
      "Epoch 2149/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.9079 - val_loss: 3.2255\n",
      "Epoch 2150/10000\n",
      "90/90 [==============================] - 0s 224us/step - loss: 10.4531 - val_loss: 3.1999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2151/10000\n",
      "90/90 [==============================] - 0s 189us/step - loss: 10.7037 - val_loss: 3.2128\n",
      "Epoch 2152/10000\n",
      "90/90 [==============================] - 0s 225us/step - loss: 10.5101 - val_loss: 3.1804\n",
      "Epoch 2153/10000\n",
      "90/90 [==============================] - 0s 211us/step - loss: 10.3827 - val_loss: 3.2249\n",
      "Epoch 2154/10000\n",
      "90/90 [==============================] - 0s 237us/step - loss: 10.7969 - val_loss: 3.2018\n",
      "Epoch 2155/10000\n",
      "90/90 [==============================] - 0s 245us/step - loss: 10.2984 - val_loss: 3.2105\n",
      "Epoch 2156/10000\n",
      "90/90 [==============================] - 0s 228us/step - loss: 10.5865 - val_loss: 3.2046\n",
      "Epoch 2157/10000\n",
      "90/90 [==============================] - 0s 164us/step - loss: 10.6271 - val_loss: 3.2244\n",
      "Epoch 2158/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 10.1963 - val_loss: 3.2254\n",
      "Epoch 2159/10000\n",
      "90/90 [==============================] - 0s 233us/step - loss: 10.9577 - val_loss: 3.2443\n",
      "Epoch 2160/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 10.9777 - val_loss: 3.2655\n",
      "Epoch 2161/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 10.8987 - val_loss: 3.2700\n",
      "Epoch 2162/10000\n",
      "90/90 [==============================] - 0s 185us/step - loss: 11.0118 - val_loss: 3.2160\n",
      "Epoch 2163/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 10.7391 - val_loss: 3.1689\n",
      "Epoch 2164/10000\n",
      "90/90 [==============================] - 0s 232us/step - loss: 10.6333 - val_loss: 3.1672\n",
      "Epoch 2165/10000\n",
      "90/90 [==============================] - 0s 306us/step - loss: 10.4879 - val_loss: 3.1697\n",
      "Epoch 2166/10000\n",
      "90/90 [==============================] - 0s 185us/step - loss: 10.9979 - val_loss: 3.1852\n",
      "Epoch 2167/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 10.5489 - val_loss: 3.1242\n",
      "Epoch 2168/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.6779 - val_loss: 3.1502\n",
      "Epoch 2169/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 11.0677 - val_loss: 3.1504\n",
      "Epoch 2170/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.7042 - val_loss: 3.1097\n",
      "Epoch 2171/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.7424 - val_loss: 3.1309\n",
      "Epoch 2172/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.7638 - val_loss: 3.1282\n",
      "Epoch 2173/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.6210 - val_loss: 3.1839\n",
      "Epoch 2174/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.5162 - val_loss: 3.1985\n",
      "Epoch 2175/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.2403 - val_loss: 3.1626\n",
      "Epoch 2176/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.4736 - val_loss: 3.2112\n",
      "Epoch 2177/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 11.1228 - val_loss: 3.2088\n",
      "Epoch 2178/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.7893 - val_loss: 3.1852\n",
      "Epoch 2179/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 10.7047 - val_loss: 3.1499\n",
      "Epoch 2180/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 10.4985 - val_loss: 3.1306\n",
      "Epoch 2181/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 11.1074 - val_loss: 3.1400\n",
      "Epoch 2182/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 10.4173 - val_loss: 3.1211\n",
      "Epoch 2183/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.3543 - val_loss: 3.1168\n",
      "Epoch 2184/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.2650 - val_loss: 3.1069\n",
      "Epoch 2185/10000\n",
      "90/90 [==============================] - 0s 340us/step - loss: 10.6216 - val_loss: 3.1508\n",
      "Epoch 2186/10000\n",
      "90/90 [==============================] - 0s 224us/step - loss: 10.6962 - val_loss: 3.1511\n",
      "Epoch 2187/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 10.6461 - val_loss: 3.1888\n",
      "Epoch 2188/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 10.3023 - val_loss: 3.1498\n",
      "Epoch 2189/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 10.7883 - val_loss: 3.1406\n",
      "Epoch 2190/10000\n",
      "90/90 [==============================] - 0s 650us/step - loss: 10.6088 - val_loss: 3.1852\n",
      "Epoch 2191/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 10.3908 - val_loss: 3.1060\n",
      "Epoch 2192/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.7390 - val_loss: 3.0514\n",
      "Epoch 2193/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.5739 - val_loss: 3.0333\n",
      "Epoch 2194/10000\n",
      "90/90 [==============================] - 0s 257us/step - loss: 10.2600 - val_loss: 3.0061\n",
      "Epoch 2195/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.4461 - val_loss: 3.0909\n",
      "Epoch 2196/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.4268 - val_loss: 3.1026\n",
      "Epoch 2197/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.5595 - val_loss: 3.1202\n",
      "Epoch 2198/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.3357 - val_loss: 3.1325\n",
      "Epoch 2199/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.7274 - val_loss: 3.1231\n",
      "Epoch 2200/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 10.9703 - val_loss: 3.1340\n",
      "Epoch 2201/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.3194 - val_loss: 3.0783\n",
      "Epoch 2202/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.1850 - val_loss: 3.0352\n",
      "Epoch 2203/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.1700 - val_loss: 3.0341\n",
      "Epoch 2204/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 10.6579 - val_loss: 3.0475\n",
      "Epoch 2205/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.4360 - val_loss: 3.0576\n",
      "Epoch 2206/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.8178 - val_loss: 3.0682\n",
      "Epoch 2207/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.9308 - val_loss: 3.1300\n",
      "Epoch 2208/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 10.6544 - val_loss: 3.1642\n",
      "Epoch 2209/10000\n",
      "90/90 [==============================] - 0s 191us/step - loss: 10.7750 - val_loss: 3.0772\n",
      "Epoch 2210/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.7589 - val_loss: 3.0589\n",
      "Epoch 2211/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.7388 - val_loss: 3.0246\n",
      "Epoch 2212/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 10.4810 - val_loss: 2.9766\n",
      "Epoch 2213/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.8541 - val_loss: 3.0114\n",
      "Epoch 2214/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.6066 - val_loss: 3.0601\n",
      "Epoch 2215/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.3919 - val_loss: 3.0488\n",
      "Epoch 2216/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 10.5180 - val_loss: 3.0623\n",
      "Epoch 2217/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.2676 - val_loss: 3.0420\n",
      "Epoch 2218/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 11.0966 - val_loss: 3.0623\n",
      "Epoch 2219/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 10.9495 - val_loss: 3.0436\n",
      "Epoch 2220/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 10.7370 - val_loss: 3.0337\n",
      "Epoch 2221/10000\n",
      "90/90 [==============================] - 0s 206us/step - loss: 10.5889 - val_loss: 3.0800\n",
      "Epoch 2222/10000\n",
      "90/90 [==============================] - 0s 200us/step - loss: 10.2023 - val_loss: 3.0448\n",
      "Epoch 2223/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.9478 - val_loss: 3.0727\n",
      "Epoch 2224/10000\n",
      "90/90 [==============================] - 0s 273us/step - loss: 10.5060 - val_loss: 3.0170\n",
      "Epoch 2225/10000\n",
      "90/90 [==============================] - 0s 190us/step - loss: 10.2928 - val_loss: 2.9474\n",
      "Epoch 2226/10000\n",
      "90/90 [==============================] - 0s 209us/step - loss: 10.6733 - val_loss: 2.9707\n",
      "Epoch 2227/10000\n",
      "90/90 [==============================] - 0s 211us/step - loss: 10.7481 - val_loss: 2.9758\n",
      "Epoch 2228/10000\n",
      "90/90 [==============================] - 0s 280us/step - loss: 10.4238 - val_loss: 2.9812\n",
      "Epoch 2229/10000\n",
      "90/90 [==============================] - 0s 305us/step - loss: 10.4239 - val_loss: 2.9914\n",
      "Epoch 2230/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 10.4471 - val_loss: 2.9591\n",
      "Epoch 2231/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 10.6320 - val_loss: 2.9695\n",
      "Epoch 2232/10000\n",
      "90/90 [==============================] - 0s 198us/step - loss: 10.2105 - val_loss: 2.9918\n",
      "Epoch 2233/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 11.0831 - val_loss: 3.0340\n",
      "Epoch 2234/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 10.4816 - val_loss: 3.0328\n",
      "Epoch 2235/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 10.3011 - val_loss: 3.0338\n",
      "Epoch 2236/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.8817 - val_loss: 3.0154\n",
      "Epoch 2237/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.4904 - val_loss: 3.0099\n",
      "Epoch 2238/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.3350 - val_loss: 2.9709\n",
      "Epoch 2239/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.4129 - val_loss: 2.9497\n",
      "Epoch 2240/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.6045 - val_loss: 2.9285\n",
      "Epoch 2241/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.3783 - val_loss: 2.9573\n",
      "Epoch 2242/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.7124 - val_loss: 2.9541\n",
      "Epoch 2243/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.5052 - val_loss: 2.9735\n",
      "Epoch 2244/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 10.2914 - val_loss: 2.9864\n",
      "Epoch 2245/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.6025 - val_loss: 2.9790\n",
      "Epoch 2246/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.4183 - val_loss: 3.0054\n",
      "Epoch 2247/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.2581 - val_loss: 2.9769\n",
      "Epoch 2248/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.4865 - val_loss: 2.9632\n",
      "Epoch 2249/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.3774 - val_loss: 2.9312\n",
      "Epoch 2250/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.3614 - val_loss: 2.9218\n",
      "Epoch 2251/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.2497 - val_loss: 2.8840\n",
      "Epoch 2252/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.3949 - val_loss: 2.9104\n",
      "Epoch 2253/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.2153 - val_loss: 2.9099\n",
      "Epoch 2254/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.4751 - val_loss: 2.9158\n",
      "Epoch 2255/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.1604 - val_loss: 2.8927\n",
      "Epoch 2256/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.5483 - val_loss: 2.9118\n",
      "Epoch 2257/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9027 - val_loss: 2.8958\n",
      "Epoch 2258/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.9973 - val_loss: 2.9205\n",
      "Epoch 2259/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.9259 - val_loss: 2.9801\n",
      "Epoch 2260/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.2595 - val_loss: 2.9694\n",
      "Epoch 2261/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.0261 - val_loss: 2.9559\n",
      "Epoch 2262/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.3141 - val_loss: 2.9468\n",
      "Epoch 2263/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.0690 - val_loss: 2.9215\n",
      "Epoch 2264/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.5841 - val_loss: 2.9263\n",
      "Epoch 2265/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.7225 - val_loss: 2.9367\n",
      "Epoch 2266/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.3630 - val_loss: 2.8780\n",
      "Epoch 2267/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 10.3583 - val_loss: 2.8577\n",
      "Epoch 2268/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.3420 - val_loss: 2.8606\n",
      "Epoch 2269/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.8062 - val_loss: 2.8572\n",
      "Epoch 2270/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.2956 - val_loss: 2.8282\n",
      "Epoch 2271/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.3042 - val_loss: 2.8724\n",
      "Epoch 2272/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.5994 - val_loss: 2.8915\n",
      "Epoch 2273/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.0833 - val_loss: 2.8931\n",
      "Epoch 2274/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.9151 - val_loss: 2.9022\n",
      "Epoch 2275/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.6720 - val_loss: 2.9305\n",
      "Epoch 2276/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.5681 - val_loss: 2.9305\n",
      "Epoch 2277/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.5539 - val_loss: 2.9295\n",
      "Epoch 2278/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.4640 - val_loss: 2.9356\n",
      "Epoch 2279/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.9530 - val_loss: 2.8756\n",
      "Epoch 2280/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 10.2398 - val_loss: 2.8370\n",
      "Epoch 2281/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 9.9770 - val_loss: 2.7785\n",
      "Epoch 2282/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.1741 - val_loss: 2.7764\n",
      "Epoch 2283/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.7101 - val_loss: 2.7834\n",
      "Epoch 2284/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.3602 - val_loss: 2.7570\n",
      "Epoch 2285/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.5128 - val_loss: 2.7775\n",
      "Epoch 2286/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.3874 - val_loss: 2.8400\n",
      "Epoch 2287/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.5108 - val_loss: 2.8852\n",
      "Epoch 2288/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.0157 - val_loss: 2.8979\n",
      "Epoch 2289/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.3619 - val_loss: 2.9104\n",
      "Epoch 2290/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.3382 - val_loss: 2.8992\n",
      "Epoch 2291/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.0590 - val_loss: 2.8987\n",
      "Epoch 2292/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.4625 - val_loss: 2.8773\n",
      "Epoch 2293/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.2274 - val_loss: 2.8581\n",
      "Epoch 2294/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.4402 - val_loss: 2.8668\n",
      "Epoch 2295/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.4685 - val_loss: 2.8277\n",
      "Epoch 2296/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.3457 - val_loss: 2.7967\n",
      "Epoch 2297/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.4701 - val_loss: 2.7704\n",
      "Epoch 2298/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.0892 - val_loss: 2.7453\n",
      "Epoch 2299/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 4.338 - 0s 151us/step - loss: 9.7724 - val_loss: 2.7669\n",
      "Epoch 2300/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.5399 - val_loss: 2.8418\n",
      "Epoch 2301/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.2351 - val_loss: 2.8737\n",
      "Epoch 2302/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.2789 - val_loss: 2.8520\n",
      "Epoch 2303/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 122us/step - loss: 10.5242 - val_loss: 2.8447\n",
      "Epoch 2304/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.4761 - val_loss: 2.8583\n",
      "Epoch 2305/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 10.0604 - val_loss: 2.8103\n",
      "Epoch 2306/10000\n",
      "90/90 [==============================] - 0s 186us/step - loss: 10.2279 - val_loss: 2.7959\n",
      "Epoch 2307/10000\n",
      "90/90 [==============================] - 0s 187us/step - loss: 10.1011 - val_loss: 2.7484\n",
      "Epoch 2308/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 10.3355 - val_loss: 2.8312\n",
      "Epoch 2309/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.3217 - val_loss: 2.8131\n",
      "Epoch 2310/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 10.7979 - val_loss: 2.7968\n",
      "Epoch 2311/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.3797 - val_loss: 2.8123\n",
      "Epoch 2312/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.5134 - val_loss: 2.8097\n",
      "Epoch 2313/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 10.3379 - val_loss: 2.7452\n",
      "Epoch 2314/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 10.0974 - val_loss: 2.7454\n",
      "Epoch 2315/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 10.1837 - val_loss: 2.6900\n",
      "Epoch 2316/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.5367 - val_loss: 2.7311\n",
      "Epoch 2317/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.4511 - val_loss: 2.7867\n",
      "Epoch 2318/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.1522 - val_loss: 2.7516\n",
      "Epoch 2319/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 10.5423 - val_loss: 2.7730\n",
      "Epoch 2320/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 10.4282 - val_loss: 2.7747\n",
      "Epoch 2321/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.5339 - val_loss: 2.7939\n",
      "Epoch 2322/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 10.2080 - val_loss: 2.7838\n",
      "Epoch 2323/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.7257 - val_loss: 2.7815\n",
      "Epoch 2324/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.1103 - val_loss: 2.7301\n",
      "Epoch 2325/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 10.5781 - val_loss: 2.7252\n",
      "Epoch 2326/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 10.1569 - val_loss: 2.7174\n",
      "Epoch 2327/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.2845 - val_loss: 2.7587\n",
      "Epoch 2328/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.5253 - val_loss: 2.7471\n",
      "Epoch 2329/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.2969 - val_loss: 2.7369\n",
      "Epoch 2330/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.6131 - val_loss: 2.7634\n",
      "Epoch 2331/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 10.3361 - val_loss: 2.7655\n",
      "Epoch 2332/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.3761 - val_loss: 2.7843\n",
      "Epoch 2333/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.5588 - val_loss: 2.7850\n",
      "Epoch 2334/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.6668 - val_loss: 2.8048\n",
      "Epoch 2335/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.5232 - val_loss: 2.7598\n",
      "Epoch 2336/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.1995 - val_loss: 2.6710\n",
      "Epoch 2337/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.3425 - val_loss: 2.6222\n",
      "Epoch 2338/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.4118 - val_loss: 2.6497\n",
      "Epoch 2339/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 10.1015 - val_loss: 2.6318\n",
      "Epoch 2340/10000\n",
      "90/90 [==============================] - 0s 204us/step - loss: 10.6969 - val_loss: 2.7324\n",
      "Epoch 2341/10000\n",
      "90/90 [==============================] - 0s 199us/step - loss: 10.1181 - val_loss: 2.7082\n",
      "Epoch 2342/10000\n",
      "90/90 [==============================] - 0s 189us/step - loss: 10.1282 - val_loss: 2.6981\n",
      "Epoch 2343/10000\n",
      "90/90 [==============================] - 0s 218us/step - loss: 10.3047 - val_loss: 2.7178\n",
      "Epoch 2344/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 10.5975 - val_loss: 2.7358\n",
      "Epoch 2345/10000\n",
      "90/90 [==============================] - 0s 210us/step - loss: 10.4632 - val_loss: 2.7678\n",
      "Epoch 2346/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 10.6107 - val_loss: 2.7325\n",
      "Epoch 2347/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.3698 - val_loss: 2.7204\n",
      "Epoch 2348/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.2566 - val_loss: 2.7141\n",
      "Epoch 2349/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 10.4304 - val_loss: 2.6840\n",
      "Epoch 2350/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.2621 - val_loss: 2.6649\n",
      "Epoch 2351/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.2858 - val_loss: 2.6538\n",
      "Epoch 2352/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.0681 - val_loss: 2.6469\n",
      "Epoch 2353/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.5310 - val_loss: 2.6938\n",
      "Epoch 2354/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.1148 - val_loss: 2.6596\n",
      "Epoch 2355/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 10.6511 - val_loss: 2.6647\n",
      "Epoch 2356/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.6150 - val_loss: 2.6644\n",
      "Epoch 2357/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.3448 - val_loss: 2.6663\n",
      "Epoch 2358/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.7313 - val_loss: 2.6662\n",
      "Epoch 2359/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.2858 - val_loss: 2.6607\n",
      "Epoch 2360/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.1690 - val_loss: 2.6647\n",
      "Epoch 2361/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.5307 - val_loss: 2.6655\n",
      "Epoch 2362/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0354 - val_loss: 2.6342\n",
      "Epoch 2363/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.5202 - val_loss: 2.6765\n",
      "Epoch 2364/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.0337 - val_loss: 2.6639\n",
      "Epoch 2365/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.2238 - val_loss: 2.6350\n",
      "Epoch 2366/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.9700 - val_loss: 2.7451\n",
      "Epoch 2367/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.3850 - val_loss: 2.6499\n",
      "Epoch 2368/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.9228 - val_loss: 2.6408\n",
      "Epoch 2369/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9866 - val_loss: 2.6022\n",
      "Epoch 2370/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8843 - val_loss: 2.5992\n",
      "Epoch 2371/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 10.3816 - val_loss: 2.6952\n",
      "Epoch 2372/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 10.4645 - val_loss: 2.6889\n",
      "Epoch 2373/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.2241 - val_loss: 2.6484\n",
      "Epoch 2374/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 10.4201 - val_loss: 2.7040\n",
      "Epoch 2375/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.2229 - val_loss: 2.6662\n",
      "Epoch 2376/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.2085 - val_loss: 2.6318\n",
      "Epoch 2377/10000\n",
      "90/90 [==============================] - 0s 200us/step - loss: 10.2013 - val_loss: 2.6710\n",
      "Epoch 2378/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.7183 - val_loss: 2.6666\n",
      "Epoch 2379/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.2178 - val_loss: 2.5449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2380/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.0785 - val_loss: 2.5711\n",
      "Epoch 2381/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 10.3689 - val_loss: 2.5849\n",
      "Epoch 2382/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 10.9610 - val_loss: 2.6827\n",
      "Epoch 2383/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.0339 - val_loss: 2.6680\n",
      "Epoch 2384/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 10.3358 - val_loss: 2.6498\n",
      "Epoch 2385/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 9.9758 - val_loss: 2.6571\n",
      "Epoch 2386/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.6747 - val_loss: 2.6214\n",
      "Epoch 2387/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.1926 - val_loss: 2.5732\n",
      "Epoch 2388/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 10.5094 - val_loss: 2.6044\n",
      "Epoch 2389/10000\n",
      "90/90 [==============================] - 0s 278us/step - loss: 9.8662 - val_loss: 2.5703\n",
      "Epoch 2390/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 10.1951 - val_loss: 2.5379\n",
      "Epoch 2391/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 10.1809 - val_loss: 2.5723\n",
      "Epoch 2392/10000\n",
      "90/90 [==============================] - 0s 196us/step - loss: 10.2822 - val_loss: 2.6055\n",
      "Epoch 2393/10000\n",
      "90/90 [==============================] - 0s 298us/step - loss: 10.3377 - val_loss: 2.6238\n",
      "Epoch 2394/10000\n",
      "90/90 [==============================] - 0s 289us/step - loss: 10.4438 - val_loss: 2.6075\n",
      "Epoch 2395/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 10.5599 - val_loss: 2.5645\n",
      "Epoch 2396/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 10.1748 - val_loss: 2.6029\n",
      "Epoch 2397/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.7683 - val_loss: 2.5749\n",
      "Epoch 2398/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.4332 - val_loss: 2.6005\n",
      "Epoch 2399/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 10.1065 - val_loss: 2.5591\n",
      "Epoch 2400/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 10.0359 - val_loss: 2.5509\n",
      "Epoch 2401/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 9.8048 - val_loss: 2.5604\n",
      "Epoch 2402/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 10.1402 - val_loss: 2.5543\n",
      "Epoch 2403/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 9.8464 - val_loss: 2.5122\n",
      "Epoch 2404/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.0695 - val_loss: 2.6175\n",
      "Epoch 2405/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 10.2399 - val_loss: 2.6090\n",
      "Epoch 2406/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.0822 - val_loss: 2.5782\n",
      "Epoch 2407/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 9.9021 - val_loss: 2.6147\n",
      "Epoch 2408/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 10.1587 - val_loss: 2.5224\n",
      "Epoch 2409/10000\n",
      "90/90 [==============================] - 0s 245us/step - loss: 9.8622 - val_loss: 2.5332\n",
      "Epoch 2410/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 10.2690 - val_loss: 2.6356\n",
      "Epoch 2411/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 10.2998 - val_loss: 2.5611\n",
      "Epoch 2412/10000\n",
      "90/90 [==============================] - 0s 397us/step - loss: 10.0704 - val_loss: 2.5190\n",
      "Epoch 2413/10000\n",
      "90/90 [==============================] - 0s 341us/step - loss: 10.4452 - val_loss: 2.5401\n",
      "Epoch 2414/10000\n",
      "90/90 [==============================] - 0s 176us/step - loss: 9.9623 - val_loss: 2.5465\n",
      "Epoch 2415/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 10.1494 - val_loss: 2.5274\n",
      "Epoch 2416/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 10.0643 - val_loss: 2.5301\n",
      "Epoch 2417/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 10.1445 - val_loss: 2.5332\n",
      "Epoch 2418/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.4679 - val_loss: 2.5229\n",
      "Epoch 2419/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 10.1119 - val_loss: 2.5188\n",
      "Epoch 2420/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.0435 - val_loss: 2.5113\n",
      "Epoch 2421/10000\n",
      "90/90 [==============================] - 0s 214us/step - loss: 9.5438 - val_loss: 2.4556\n",
      "Epoch 2422/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 9.7372 - val_loss: 2.4655\n",
      "Epoch 2423/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 9.7775 - val_loss: 2.4766\n",
      "Epoch 2424/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.3781 - val_loss: 2.5561\n",
      "Epoch 2425/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.2220 - val_loss: 2.5187\n",
      "Epoch 2426/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.7520 - val_loss: 2.5404\n",
      "Epoch 2427/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.2776 - val_loss: 2.5357\n",
      "Epoch 2428/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 9.9963 - val_loss: 2.5105\n",
      "Epoch 2429/10000\n",
      "90/90 [==============================] - 0s 354us/step - loss: 10.0845 - val_loss: 2.5390\n",
      "Epoch 2430/10000\n",
      "90/90 [==============================] - 0s 215us/step - loss: 10.3648 - val_loss: 2.5129\n",
      "Epoch 2431/10000\n",
      "90/90 [==============================] - 0s 315us/step - loss: 10.4378 - val_loss: 2.5004\n",
      "Epoch 2432/10000\n",
      "90/90 [==============================] - 0s 176us/step - loss: 9.8255 - val_loss: 2.4839\n",
      "Epoch 2433/10000\n",
      "90/90 [==============================] - 0s 264us/step - loss: 10.3343 - val_loss: 2.5136\n",
      "Epoch 2434/10000\n",
      "90/90 [==============================] - 0s 249us/step - loss: 10.4160 - val_loss: 2.5235\n",
      "Epoch 2435/10000\n",
      "90/90 [==============================] - 0s 503us/step - loss: 10.6075 - val_loss: 2.5616\n",
      "Epoch 2436/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.3442 - val_loss: 2.5232\n",
      "Epoch 2437/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 10.2049 - val_loss: 2.4731\n",
      "Epoch 2438/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 9.9842 - val_loss: 2.4517\n",
      "Epoch 2439/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 10.1901 - val_loss: 2.4729\n",
      "Epoch 2440/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 10.1195 - val_loss: 2.4442\n",
      "Epoch 2441/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.3067 - val_loss: 2.4695\n",
      "Epoch 2442/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.2533 - val_loss: 2.4798\n",
      "Epoch 2443/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.1564 - val_loss: 2.4582\n",
      "Epoch 2444/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.9041 - val_loss: 2.4031\n",
      "Epoch 2445/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 10.5467 - val_loss: 2.4776\n",
      "Epoch 2446/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 9.9302 - val_loss: 2.5148\n",
      "Epoch 2447/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.2519 - val_loss: 2.5495\n",
      "Epoch 2448/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.2381 - val_loss: 2.5074\n",
      "Epoch 2449/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 10.2674 - val_loss: 2.5422\n",
      "Epoch 2450/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 10.2496 - val_loss: 2.5305\n",
      "Epoch 2451/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 10.4934 - val_loss: 2.5676\n",
      "Epoch 2452/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 10.0146 - val_loss: 2.5243\n",
      "Epoch 2453/10000\n",
      "90/90 [==============================] - 0s 176us/step - loss: 10.2546 - val_loss: 2.4862\n",
      "Epoch 2454/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 10.1545 - val_loss: 2.4639\n",
      "Epoch 2455/10000\n",
      "90/90 [==============================] - 0s 190us/step - loss: 10.0539 - val_loss: 2.4453\n",
      "Epoch 2456/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 9.9248 - val_loss: 2.4191\n",
      "Epoch 2457/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 10.1733 - val_loss: 2.3968\n",
      "Epoch 2458/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.7541 - val_loss: 2.5469\n",
      "Epoch 2459/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 10.0507 - val_loss: 2.3966\n",
      "Epoch 2460/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.3125 - val_loss: 2.3777\n",
      "Epoch 2461/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8268 - val_loss: 2.3848\n",
      "Epoch 2462/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8525 - val_loss: 2.4317\n",
      "Epoch 2463/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.3926 - val_loss: 2.4697\n",
      "Epoch 2464/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7297 - val_loss: 2.4808\n",
      "Epoch 2465/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.0243 - val_loss: 2.4883\n",
      "Epoch 2466/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 10.3383 - val_loss: 2.5105\n",
      "Epoch 2467/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 10.0523 - val_loss: 2.4288\n",
      "Epoch 2468/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 10.4452 - val_loss: 2.3870\n",
      "Epoch 2469/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 9.9020 - val_loss: 2.3976\n",
      "Epoch 2470/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.0429 - val_loss: 2.3600\n",
      "Epoch 2471/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.0240 - val_loss: 2.3782\n",
      "Epoch 2472/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.4227 - val_loss: 2.4039\n",
      "Epoch 2473/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.8719 - val_loss: 2.3715\n",
      "Epoch 2474/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.6093 - val_loss: 2.3989\n",
      "Epoch 2475/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.2572 - val_loss: 2.4303\n",
      "Epoch 2476/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.3857 - val_loss: 2.4459\n",
      "Epoch 2477/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.4078 - val_loss: 2.4707\n",
      "Epoch 2478/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8047 - val_loss: 2.4397\n",
      "Epoch 2479/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.5592 - val_loss: 2.3949\n",
      "Epoch 2480/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.7018 - val_loss: 2.3902\n",
      "Epoch 2481/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8719 - val_loss: 2.3541\n",
      "Epoch 2482/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.1110 - val_loss: 2.4016\n",
      "Epoch 2483/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9505 - val_loss: 2.3868\n",
      "Epoch 2484/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.3141 - val_loss: 2.3769\n",
      "Epoch 2485/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.2353 - val_loss: 2.3939\n",
      "Epoch 2486/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.6401 - val_loss: 2.4490\n",
      "Epoch 2487/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.3998 - val_loss: 2.4474\n",
      "Epoch 2488/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.2967 - val_loss: 2.4210\n",
      "Epoch 2489/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.4075 - val_loss: 2.3975\n",
      "Epoch 2490/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.3037 - val_loss: 2.4275\n",
      "Epoch 2491/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.1678 - val_loss: 2.3815\n",
      "Epoch 2492/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.1132 - val_loss: 2.3668\n",
      "Epoch 2493/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.1931 - val_loss: 2.3618\n",
      "Epoch 2494/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.3162 - val_loss: 2.3749\n",
      "Epoch 2495/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.9453 - val_loss: 2.3759\n",
      "Epoch 2496/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7984 - val_loss: 2.4029\n",
      "Epoch 2497/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.8529 - val_loss: 2.3396\n",
      "Epoch 2498/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.4158 - val_loss: 2.4447\n",
      "Epoch 2499/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.8581 - val_loss: 2.3880\n",
      "Epoch 2500/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 9.9763 - val_loss: 2.3634\n",
      "Epoch 2501/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.4315 - val_loss: 2.3718\n",
      "Epoch 2502/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.8560 - val_loss: 2.3332\n",
      "Epoch 2503/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 10.3757 - val_loss: 2.3488\n",
      "Epoch 2504/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 10.3813 - val_loss: 2.3744\n",
      "Epoch 2505/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.7756 - val_loss: 2.3356\n",
      "Epoch 2506/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 10.0990 - val_loss: 2.3534\n",
      "Epoch 2507/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 10.1915 - val_loss: 2.3828\n",
      "Epoch 2508/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 10.0348 - val_loss: 2.3573\n",
      "Epoch 2509/10000\n",
      "90/90 [==============================] - 0s 175us/step - loss: 9.8306 - val_loss: 2.3233\n",
      "Epoch 2510/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 10.0103 - val_loss: 2.3505\n",
      "Epoch 2511/10000\n",
      "90/90 [==============================] - 0s 262us/step - loss: 9.8393 - val_loss: 2.3070\n",
      "Epoch 2512/10000\n",
      "90/90 [==============================] - 0s 218us/step - loss: 9.9778 - val_loss: 2.3470\n",
      "Epoch 2513/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.9214 - val_loss: 2.3457\n",
      "Epoch 2514/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 10.1337 - val_loss: 2.3198\n",
      "Epoch 2515/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 10.3022 - val_loss: 2.2890\n",
      "Epoch 2516/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 10.5212 - val_loss: 2.3067\n",
      "Epoch 2517/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 9.9679 - val_loss: 2.3361\n",
      "Epoch 2518/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.3432 - val_loss: 2.3361\n",
      "Epoch 2519/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.9331 - val_loss: 2.3620\n",
      "Epoch 2520/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.0296 - val_loss: 2.4180\n",
      "Epoch 2521/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 10.6367 - val_loss: 2.4485\n",
      "Epoch 2522/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9263 - val_loss: 2.3176\n",
      "Epoch 2523/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.3751 - val_loss: 2.3206\n",
      "Epoch 2524/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 10.0794 - val_loss: 2.2808\n",
      "Epoch 2525/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 9.6437 - val_loss: 2.2494\n",
      "Epoch 2526/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.3038 - val_loss: 2.3040\n",
      "Epoch 2527/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.2246 - val_loss: 2.2926\n",
      "Epoch 2528/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.8051 - val_loss: 2.2752\n",
      "Epoch 2529/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 9.9805 - val_loss: 2.3054\n",
      "Epoch 2530/10000\n",
      "90/90 [==============================] - 0s 260us/step - loss: 10.1743 - val_loss: 2.3173\n",
      "Epoch 2531/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 10.2374 - val_loss: 2.3397\n",
      "Epoch 2532/10000\n",
      "90/90 [==============================] - 0s 179us/step - loss: 10.2926 - val_loss: 2.3438\n",
      "Epoch 2533/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 122us/step - loss: 9.6217 - val_loss: 2.2996\n",
      "Epoch 2534/10000\n",
      "90/90 [==============================] - 0s 189us/step - loss: 10.5464 - val_loss: 2.3357\n",
      "Epoch 2535/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.0269 - val_loss: 2.3181\n",
      "Epoch 2536/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.2258 - val_loss: 2.2611\n",
      "Epoch 2537/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8206 - val_loss: 2.2556\n",
      "Epoch 2538/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.1541 - val_loss: 2.2635\n",
      "Epoch 2539/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.3278 - val_loss: 2.2857\n",
      "Epoch 2540/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8250 - val_loss: 2.2865\n",
      "Epoch 2541/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8370 - val_loss: 2.2657\n",
      "Epoch 2542/10000\n",
      "90/90 [==============================] - 0s 185us/step - loss: 10.1685 - val_loss: 2.2858\n",
      "Epoch 2543/10000\n",
      "90/90 [==============================] - 0s 199us/step - loss: 10.2140 - val_loss: 2.2846\n",
      "Epoch 2544/10000\n",
      "90/90 [==============================] - 0s 221us/step - loss: 10.2274 - val_loss: 2.2751\n",
      "Epoch 2545/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.2851 - val_loss: 2.3159\n",
      "Epoch 2546/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.1435 - val_loss: 2.3324\n",
      "Epoch 2547/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.0205 - val_loss: 2.2986\n",
      "Epoch 2548/10000\n",
      "90/90 [==============================] - 0s 340us/step - loss: 9.8114 - val_loss: 2.2531\n",
      "Epoch 2549/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 9.7498 - val_loss: 2.2245\n",
      "Epoch 2550/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.0377 - val_loss: 2.2441\n",
      "Epoch 2551/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 10.2209 - val_loss: 2.2839\n",
      "Epoch 2552/10000\n",
      "90/90 [==============================] - 0s 230us/step - loss: 9.9500 - val_loss: 2.2575\n",
      "Epoch 2553/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.2749 - val_loss: 2.2377\n",
      "Epoch 2554/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.9241 - val_loss: 2.2245\n",
      "Epoch 2555/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.2240 - val_loss: 2.2246\n",
      "Epoch 2556/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8901 - val_loss: 2.2423\n",
      "Epoch 2557/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3987 - val_loss: 2.1665\n",
      "Epoch 2558/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.9063 - val_loss: 2.1929\n",
      "Epoch 2559/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8977 - val_loss: 2.2357\n",
      "Epoch 2560/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9876 - val_loss: 2.3029\n",
      "Epoch 2561/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.1862 - val_loss: 2.3000\n",
      "Epoch 2562/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9882 - val_loss: 2.2703\n",
      "Epoch 2563/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9509 - val_loss: 2.3308\n",
      "Epoch 2564/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.0531 - val_loss: 2.2898\n",
      "Epoch 2565/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9309 - val_loss: 2.3062\n",
      "Epoch 2566/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.2137 - val_loss: 2.2930\n",
      "Epoch 2567/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9123 - val_loss: 2.2197\n",
      "Epoch 2568/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.1143 - val_loss: 2.2054\n",
      "Epoch 2569/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8608 - val_loss: 2.1433\n",
      "Epoch 2570/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.0805 - val_loss: 2.1192\n",
      "Epoch 2571/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.2993 - val_loss: 2.2395\n",
      "Epoch 2572/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.0284 - val_loss: 2.2315\n",
      "Epoch 2573/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9899 - val_loss: 2.2308\n",
      "Epoch 2574/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.0091 - val_loss: 2.2586\n",
      "Epoch 2575/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.3010 - val_loss: 2.2776\n",
      "Epoch 2576/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.1433 - val_loss: 2.2690\n",
      "Epoch 2577/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.7471 - val_loss: 2.2276\n",
      "Epoch 2578/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0966 - val_loss: 2.2179\n",
      "Epoch 2579/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0577 - val_loss: 2.2125\n",
      "Epoch 2580/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.1159 - val_loss: 2.1908\n",
      "Epoch 2581/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.0769 - val_loss: 2.1572\n",
      "Epoch 2582/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.1563 - val_loss: 2.1827\n",
      "Epoch 2583/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.2390 - val_loss: 2.1997\n",
      "Epoch 2584/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7183 - val_loss: 2.1833\n",
      "Epoch 2585/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.2799 - val_loss: 2.1891\n",
      "Epoch 2586/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.1135 - val_loss: 2.2309\n",
      "Epoch 2587/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.7529 - val_loss: 2.2597\n",
      "Epoch 2588/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.2949 - val_loss: 2.2578\n",
      "Epoch 2589/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7868 - val_loss: 2.2409\n",
      "Epoch 2590/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.1332 - val_loss: 2.1942\n",
      "Epoch 2591/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3583 - val_loss: 2.1536\n",
      "Epoch 2592/10000\n",
      "90/90 [==============================] - 0s 323us/step - loss: 10.0527 - val_loss: 2.1713\n",
      "Epoch 2593/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.9202 - val_loss: 2.1190\n",
      "Epoch 2594/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 10.5392 - val_loss: 2.1746\n",
      "Epoch 2595/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.7217 - val_loss: 2.1631\n",
      "Epoch 2596/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.3094 - val_loss: 2.1913\n",
      "Epoch 2597/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.9036 - val_loss: 2.1503\n",
      "Epoch 2598/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8414 - val_loss: 2.1660\n",
      "Epoch 2599/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8273 - val_loss: 2.1569\n",
      "Epoch 2600/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.5390 - val_loss: 2.2070\n",
      "Epoch 2601/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.3242 - val_loss: 2.1941\n",
      "Epoch 2602/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.7445 - val_loss: 2.1634\n",
      "Epoch 2603/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 10.2750 - val_loss: 2.1624\n",
      "Epoch 2604/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 10.0262 - val_loss: 2.1703\n",
      "Epoch 2605/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.8879 - val_loss: 2.2296\n",
      "Epoch 2606/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 10.3259 - val_loss: 2.2230\n",
      "Epoch 2607/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.9617 - val_loss: 2.2067\n",
      "Epoch 2608/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.8370 - val_loss: 2.1661\n",
      "Epoch 2609/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.7536 - val_loss: 2.1602\n",
      "Epoch 2610/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 9.7311 - val_loss: 2.1676\n",
      "Epoch 2611/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.9087 - val_loss: 2.1556\n",
      "Epoch 2612/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.1425 - val_loss: 2.1638\n",
      "Epoch 2613/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 9.9668 - val_loss: 2.1493\n",
      "Epoch 2614/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.9901 - val_loss: 2.1412\n",
      "Epoch 2615/10000\n",
      "90/90 [==============================] - 0s 191us/step - loss: 10.4452 - val_loss: 2.1610\n",
      "Epoch 2616/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 9.9632 - val_loss: 2.1952\n",
      "Epoch 2617/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 10.0434 - val_loss: 2.1185\n",
      "Epoch 2618/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 9.9129 - val_loss: 2.1651\n",
      "Epoch 2619/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 10.1236 - val_loss: 2.2085\n",
      "Epoch 2620/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.9064 - val_loss: 2.2182\n",
      "Epoch 2621/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.8663 - val_loss: 2.2165\n",
      "Epoch 2622/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 10.1365 - val_loss: 2.1912\n",
      "Epoch 2623/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 10.1733 - val_loss: 2.1768\n",
      "Epoch 2624/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 10.0120 - val_loss: 2.1592\n",
      "Epoch 2625/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.6811 - val_loss: 2.1244\n",
      "Epoch 2626/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.9000 - val_loss: 2.0661\n",
      "Epoch 2627/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 9.7644 - val_loss: 2.0677\n",
      "Epoch 2628/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9536 - val_loss: 2.1144\n",
      "Epoch 2629/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6401 - val_loss: 2.0834\n",
      "Epoch 2630/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.9634 - val_loss: 2.1027\n",
      "Epoch 2631/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.0435 - val_loss: 2.1647\n",
      "Epoch 2632/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.1920 - val_loss: 2.1742\n",
      "Epoch 2633/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.2305 - val_loss: 2.2004\n",
      "Epoch 2634/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6733 - val_loss: 2.1394\n",
      "Epoch 2635/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.0538 - val_loss: 2.1042\n",
      "Epoch 2636/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9928 - val_loss: 2.1038\n",
      "Epoch 2637/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3574 - val_loss: 2.0504\n",
      "Epoch 2638/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.0783 - val_loss: 2.1452\n",
      "Epoch 2639/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5635 - val_loss: 2.1075\n",
      "Epoch 2640/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.2462 - val_loss: 2.1518\n",
      "Epoch 2641/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4664 - val_loss: 2.0828\n",
      "Epoch 2642/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8382 - val_loss: 2.1002\n",
      "Epoch 2643/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.2706 - val_loss: 2.1522\n",
      "Epoch 2644/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0262 - val_loss: 2.1314\n",
      "Epoch 2645/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7907 - val_loss: 2.1114\n",
      "Epoch 2646/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.1916 - val_loss: 2.1327\n",
      "Epoch 2647/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.9821 - val_loss: 2.1056\n",
      "Epoch 2648/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.8666 - val_loss: 2.0905\n",
      "Epoch 2649/10000\n",
      "90/90 [==============================] - 0s 170us/step - loss: 9.7314 - val_loss: 2.0737\n",
      "Epoch 2650/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.8298 - val_loss: 2.0825\n",
      "Epoch 2651/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.1336 - val_loss: 2.0866\n",
      "Epoch 2652/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.4869 - val_loss: 2.0696\n",
      "Epoch 2653/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.0145 - val_loss: 2.0720\n",
      "Epoch 2654/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 10.2339 - val_loss: 2.1247\n",
      "Epoch 2655/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 9.8599 - val_loss: 2.0770\n",
      "Epoch 2656/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 10.5066 - val_loss: 2.0711\n",
      "Epoch 2657/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.9949 - val_loss: 2.0493\n",
      "Epoch 2658/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7204 - val_loss: 2.0272\n",
      "Epoch 2659/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6005 - val_loss: 2.0280\n",
      "Epoch 2660/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9090 - val_loss: 2.0529\n",
      "Epoch 2661/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8742 - val_loss: 2.0761\n",
      "Epoch 2662/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.9726 - val_loss: 2.0797\n",
      "Epoch 2663/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8696 - val_loss: 2.0672\n",
      "Epoch 2664/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9541 - val_loss: 2.0853\n",
      "Epoch 2665/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.1510 - val_loss: 2.0836\n",
      "Epoch 2666/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5529 - val_loss: 2.0113\n",
      "Epoch 2667/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7397 - val_loss: 2.0225\n",
      "Epoch 2668/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7717 - val_loss: 2.0301\n",
      "Epoch 2669/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.1671 - val_loss: 2.1341\n",
      "Epoch 2670/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7115 - val_loss: 2.0585\n",
      "Epoch 2671/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9567 - val_loss: 2.0813\n",
      "Epoch 2672/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3236 - val_loss: 2.0606\n",
      "Epoch 2673/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.1244 - val_loss: 2.2123\n",
      "Epoch 2674/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.4304 - val_loss: 2.2504\n",
      "Epoch 2675/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.9847 - val_loss: 2.1050\n",
      "Epoch 2676/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8519 - val_loss: 2.0276\n",
      "Epoch 2677/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.1227 - val_loss: 2.0173\n",
      "Epoch 2678/10000\n",
      "90/90 [==============================] - 0s 245us/step - loss: 9.8622 - val_loss: 2.0667\n",
      "Epoch 2679/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 9.9406 - val_loss: 2.1020\n",
      "Epoch 2680/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 10.1433 - val_loss: 2.1364\n",
      "Epoch 2681/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.9475 - val_loss: 2.0662\n",
      "Epoch 2682/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.9346 - val_loss: 2.0260\n",
      "Epoch 2683/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.0334 - val_loss: 2.0459\n",
      "Epoch 2684/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.2476 - val_loss: 2.0406\n",
      "Epoch 2685/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7806 - val_loss: 1.9979\n",
      "Epoch 2686/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.5132 - val_loss: 1.9949\n",
      "Epoch 2687/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 104us/step - loss: 10.1842 - val_loss: 1.9556\n",
      "Epoch 2688/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6495 - val_loss: 1.9598\n",
      "Epoch 2689/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.0413 - val_loss: 2.0147\n",
      "Epoch 2690/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6550 - val_loss: 2.0622\n",
      "Epoch 2691/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.9015 - val_loss: 2.0510\n",
      "Epoch 2692/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.8681 - val_loss: 2.0247\n",
      "Epoch 2693/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.7561 - val_loss: 1.9694\n",
      "Epoch 2694/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.9454 - val_loss: 1.9815\n",
      "Epoch 2695/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.0624 - val_loss: 1.9967\n",
      "Epoch 2696/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.0156 - val_loss: 2.0852\n",
      "Epoch 2697/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.9646 - val_loss: 2.0709\n",
      "Epoch 2698/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6828 - val_loss: 2.0675\n",
      "Epoch 2699/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.9922 - val_loss: 2.0635\n",
      "Epoch 2700/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.9605 - val_loss: 2.0433\n",
      "Epoch 2701/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.5988 - val_loss: 2.0122\n",
      "Epoch 2702/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8573 - val_loss: 1.9732\n",
      "Epoch 2703/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.3149 - val_loss: 2.0280\n",
      "Epoch 2704/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.7115 - val_loss: 1.9968\n",
      "Epoch 2705/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.1509 - val_loss: 2.0343\n",
      "Epoch 2706/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.0700 - val_loss: 1.9581\n",
      "Epoch 2707/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8585 - val_loss: 1.9840\n",
      "Epoch 2708/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.8865 - val_loss: 2.0490\n",
      "Epoch 2709/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 9.8494 - val_loss: 2.0192\n",
      "Epoch 2710/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.8593 - val_loss: 2.0164\n",
      "Epoch 2711/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 10.0636 - val_loss: 1.9878\n",
      "Epoch 2712/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.9098 - val_loss: 1.9772\n",
      "Epoch 2713/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.7993 - val_loss: 1.9881\n",
      "Epoch 2714/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.1162 - val_loss: 2.0082\n",
      "Epoch 2715/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7642 - val_loss: 2.0417\n",
      "Epoch 2716/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6623 - val_loss: 2.0331\n",
      "Epoch 2717/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8264 - val_loss: 1.9612\n",
      "Epoch 2718/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0483 - val_loss: 1.9457\n",
      "Epoch 2719/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9029 - val_loss: 1.9936\n",
      "Epoch 2720/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8746 - val_loss: 2.0017\n",
      "Epoch 2721/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7075 - val_loss: 1.9554\n",
      "Epoch 2722/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.1602 - val_loss: 1.9363\n",
      "Epoch 2723/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7185 - val_loss: 1.9233\n",
      "Epoch 2724/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.9302 - val_loss: 1.9461\n",
      "Epoch 2725/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5605 - val_loss: 1.9471\n",
      "Epoch 2726/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8628 - val_loss: 1.9951\n",
      "Epoch 2727/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.8946 - val_loss: 1.9679\n",
      "Epoch 2728/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.0651 - val_loss: 1.9924\n",
      "Epoch 2729/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8420 - val_loss: 2.0298\n",
      "Epoch 2730/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7978 - val_loss: 2.0476\n",
      "Epoch 2731/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.9624 - val_loss: 2.0327\n",
      "Epoch 2732/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.1768 - val_loss: 2.0533\n",
      "Epoch 2733/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8183 - val_loss: 1.9978\n",
      "Epoch 2734/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7745 - val_loss: 1.9513\n",
      "Epoch 2735/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.9298 - val_loss: 1.9010\n",
      "Epoch 2736/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.3326 - val_loss: 2.0574\n",
      "Epoch 2737/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.6099 - val_loss: 1.9417\n",
      "Epoch 2738/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9328 - val_loss: 1.8995\n",
      "Epoch 2739/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9961 - val_loss: 1.9166\n",
      "Epoch 2740/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8974 - val_loss: 1.9368\n",
      "Epoch 2741/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8749 - val_loss: 2.0155\n",
      "Epoch 2742/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8755 - val_loss: 2.0370\n",
      "Epoch 2743/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7697 - val_loss: 1.9644\n",
      "Epoch 2744/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.9066 - val_loss: 1.9510\n",
      "Epoch 2745/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5265 - val_loss: 1.9112\n",
      "Epoch 2746/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9949 - val_loss: 1.9532\n",
      "Epoch 2747/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6669 - val_loss: 1.9385\n",
      "Epoch 2748/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9446 - val_loss: 1.9371\n",
      "Epoch 2749/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.3800 - val_loss: 2.0141\n",
      "Epoch 2750/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8565 - val_loss: 1.9394\n",
      "Epoch 2751/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7039 - val_loss: 1.9119\n",
      "Epoch 2752/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8231 - val_loss: 1.9433\n",
      "Epoch 2753/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 9.9095 - val_loss: 2.0362\n",
      "Epoch 2754/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 10.1023 - val_loss: 2.0748\n",
      "Epoch 2755/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.8542 - val_loss: 1.9557\n",
      "Epoch 2756/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.5252 - val_loss: 1.9995\n",
      "Epoch 2757/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9800 - val_loss: 1.9492\n",
      "Epoch 2758/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 9.6513 - val_loss: 1.8653\n",
      "Epoch 2759/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 9.9090 - val_loss: 1.8761\n",
      "Epoch 2760/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.5172 - val_loss: 1.8643\n",
      "Epoch 2761/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0669 - val_loss: 1.9466\n",
      "Epoch 2762/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.1690 - val_loss: 1.9740\n",
      "Epoch 2763/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9161 - val_loss: 1.9311\n",
      "Epoch 2764/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.0599 - val_loss: 1.8717\n",
      "Epoch 2765/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 9.5967 - val_loss: 1.8468\n",
      "Epoch 2766/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7382 - val_loss: 1.8879\n",
      "Epoch 2767/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8505 - val_loss: 1.8988\n",
      "Epoch 2768/10000\n",
      "90/90 [==============================] - 0s 231us/step - loss: 10.2611 - val_loss: 1.9256\n",
      "Epoch 2769/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.9760 - val_loss: 1.9378\n",
      "Epoch 2770/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.0541 - val_loss: 1.9625\n",
      "Epoch 2771/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7477 - val_loss: 1.8821\n",
      "Epoch 2772/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 10.0819 - val_loss: 1.9032\n",
      "Epoch 2773/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8569 - val_loss: 2.0161\n",
      "Epoch 2774/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.1605 - val_loss: 1.9753\n",
      "Epoch 2775/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8667 - val_loss: 1.9070\n",
      "Epoch 2776/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7056 - val_loss: 1.8613\n",
      "Epoch 2777/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8632 - val_loss: 1.8509\n",
      "Epoch 2778/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.3159 - val_loss: 1.8986\n",
      "Epoch 2779/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7205 - val_loss: 1.8812\n",
      "Epoch 2780/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.9357 - val_loss: 1.9179\n",
      "Epoch 2781/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7039 - val_loss: 1.8541\n",
      "Epoch 2782/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.3509 - val_loss: 1.9283\n",
      "Epoch 2783/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4843 - val_loss: 1.8233\n",
      "Epoch 2784/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.8309 - val_loss: 1.8296\n",
      "Epoch 2785/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8932 - val_loss: 1.8829\n",
      "Epoch 2786/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8792 - val_loss: 1.9256\n",
      "Epoch 2787/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.2374 - val_loss: 1.9559\n",
      "Epoch 2788/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8165 - val_loss: 1.9190\n",
      "Epoch 2789/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5996 - val_loss: 1.7894\n",
      "Epoch 2790/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6610 - val_loss: 1.8597\n",
      "Epoch 2791/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6120 - val_loss: 1.8837\n",
      "Epoch 2792/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.0174 - val_loss: 1.9304\n",
      "Epoch 2793/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.3752 - val_loss: 1.9137\n",
      "Epoch 2794/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.0326 - val_loss: 2.0233\n",
      "Epoch 2795/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9119 - val_loss: 1.9676\n",
      "Epoch 2796/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7751 - val_loss: 1.8892\n",
      "Epoch 2797/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 10.2995 - val_loss: 1.9186\n",
      "Epoch 2798/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.9577 - val_loss: 1.8733\n",
      "Epoch 2799/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8016 - val_loss: 1.8418\n",
      "Epoch 2800/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.0420 - val_loss: 1.8032\n",
      "Epoch 2801/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9216 - val_loss: 1.7605\n",
      "Epoch 2802/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.9607 - val_loss: 1.7915\n",
      "Epoch 2803/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 10.2040 - val_loss: 1.8972\n",
      "Epoch 2804/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5808 - val_loss: 1.8796\n",
      "Epoch 2805/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6347 - val_loss: 1.9100\n",
      "Epoch 2806/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9743 - val_loss: 1.9770\n",
      "Epoch 2807/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6943 - val_loss: 1.8499\n",
      "Epoch 2808/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.8877 - val_loss: 1.8209\n",
      "Epoch 2809/10000\n",
      "90/90 [==============================] - 0s 207us/step - loss: 9.7980 - val_loss: 1.8498\n",
      "Epoch 2810/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 10.0865 - val_loss: 1.8835\n",
      "Epoch 2811/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 9.7782 - val_loss: 1.7792\n",
      "Epoch 2812/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 10.1360 - val_loss: 1.8107\n",
      "Epoch 2813/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.8168 - val_loss: 1.8250\n",
      "Epoch 2814/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.8286 - val_loss: 1.8651\n",
      "Epoch 2815/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.0112 - val_loss: 1.8779\n",
      "Epoch 2816/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 10.0394 - val_loss: 1.8994\n",
      "Epoch 2817/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.0591 - val_loss: 1.8355\n",
      "Epoch 2818/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9183 - val_loss: 1.8286\n",
      "Epoch 2819/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.0209 - val_loss: 1.8011\n",
      "Epoch 2820/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8820 - val_loss: 1.7793\n",
      "Epoch 2821/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 9.9559 - val_loss: 1.8511\n",
      "Epoch 2822/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9765 - val_loss: 1.9126\n",
      "Epoch 2823/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.8388 - val_loss: 1.8925\n",
      "Epoch 2824/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.1078 - val_loss: 1.8787\n",
      "Epoch 2825/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.9314 - val_loss: 1.8463\n",
      "Epoch 2826/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.8925 - val_loss: 1.8572\n",
      "Epoch 2827/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9314 - val_loss: 1.8529\n",
      "Epoch 2828/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6750 - val_loss: 1.8565\n",
      "Epoch 2829/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.6706 - val_loss: 1.8474\n",
      "Epoch 2830/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 10.0482 - val_loss: 1.8016\n",
      "Epoch 2831/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.5825 - val_loss: 1.7269\n",
      "Epoch 2832/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8246 - val_loss: 1.8056\n",
      "Epoch 2833/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8040 - val_loss: 1.8211\n",
      "Epoch 2834/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.1591 - val_loss: 1.9496\n",
      "Epoch 2835/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.4808 - val_loss: 2.0106\n",
      "Epoch 2836/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.3050 - val_loss: 1.9362\n",
      "Epoch 2837/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.9420 - val_loss: 1.8166\n",
      "Epoch 2838/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9856 - val_loss: 1.7990\n",
      "Epoch 2839/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.1661 - val_loss: 1.8602\n",
      "Epoch 2840/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6261 - val_loss: 1.8338\n",
      "Epoch 2841/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 104us/step - loss: 9.8844 - val_loss: 1.8199\n",
      "Epoch 2842/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8880 - val_loss: 1.7678\n",
      "Epoch 2843/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8322 - val_loss: 1.7619\n",
      "Epoch 2844/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.9858 - val_loss: 1.7276\n",
      "Epoch 2845/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0890 - val_loss: 1.7449\n",
      "Epoch 2846/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5172 - val_loss: 1.7853\n",
      "Epoch 2847/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.2918 - val_loss: 1.9058\n",
      "Epoch 2848/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7524 - val_loss: 1.8473\n",
      "Epoch 2849/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.7114 - val_loss: 1.8075\n",
      "Epoch 2850/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8829 - val_loss: 1.8384\n",
      "Epoch 2851/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8287 - val_loss: 1.8600\n",
      "Epoch 2852/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8296 - val_loss: 1.8998\n",
      "Epoch 2853/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.0265 - val_loss: 1.8222\n",
      "Epoch 2854/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6354 - val_loss: 1.7044\n",
      "Epoch 2855/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0543 - val_loss: 1.7386\n",
      "Epoch 2856/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.8719 - val_loss: 1.8076\n",
      "Epoch 2857/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8722 - val_loss: 1.8744\n",
      "Epoch 2858/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 10.0238 - val_loss: 1.9122\n",
      "Epoch 2859/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.0846 - val_loss: 1.8488\n",
      "Epoch 2860/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8961 - val_loss: 1.7530\n",
      "Epoch 2861/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.2075 - val_loss: 1.7163\n",
      "Epoch 2862/10000\n",
      "90/90 [==============================] - 0s 208us/step - loss: 9.8217 - val_loss: 1.7539\n",
      "Epoch 2863/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8657 - val_loss: 1.7782\n",
      "Epoch 2864/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7817 - val_loss: 1.8319\n",
      "Epoch 2865/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7588 - val_loss: 1.7718\n",
      "Epoch 2866/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.0772 - val_loss: 1.7698\n",
      "Epoch 2867/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.3069 - val_loss: 1.8251\n",
      "Epoch 2868/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8414 - val_loss: 1.8662\n",
      "Epoch 2869/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.5997 - val_loss: 1.8374\n",
      "Epoch 2870/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9386 - val_loss: 1.7470\n",
      "Epoch 2871/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7429 - val_loss: 1.7142\n",
      "Epoch 2872/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6373 - val_loss: 1.7042\n",
      "Epoch 2873/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.2930 - val_loss: 1.8874\n",
      "Epoch 2874/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5979 - val_loss: 1.8309\n",
      "Epoch 2875/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.8252 - val_loss: 1.8118\n",
      "Epoch 2876/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.8429 - val_loss: 1.7639\n",
      "Epoch 2877/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8166 - val_loss: 1.6934\n",
      "Epoch 2878/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.7169 - val_loss: 1.8365\n",
      "Epoch 2879/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6343 - val_loss: 1.8063\n",
      "Epoch 2880/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.0622 - val_loss: 1.8248\n",
      "Epoch 2881/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5384 - val_loss: 1.8326\n",
      "Epoch 2882/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8062 - val_loss: 1.7781\n",
      "Epoch 2883/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.6208 - val_loss: 1.8380\n",
      "Epoch 2884/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7703 - val_loss: 1.8231\n",
      "Epoch 2885/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5462 - val_loss: 1.8645\n",
      "Epoch 2886/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6573 - val_loss: 1.7775\n",
      "Epoch 2887/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7406 - val_loss: 1.7769\n",
      "Epoch 2888/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8853 - val_loss: 1.7558\n",
      "Epoch 2889/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9517 - val_loss: 1.7498\n",
      "Epoch 2890/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9464 - val_loss: 1.8048\n",
      "Epoch 2891/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.6783 - val_loss: 1.7237\n",
      "Epoch 2892/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8301 - val_loss: 1.7659\n",
      "Epoch 2893/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7973 - val_loss: 1.7697\n",
      "Epoch 2894/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.4926 - val_loss: 1.6931\n",
      "Epoch 2895/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.0153 - val_loss: 1.7511\n",
      "Epoch 2896/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4449 - val_loss: 1.7011\n",
      "Epoch 2897/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4192 - val_loss: 1.7966\n",
      "Epoch 2898/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.9775 - val_loss: 1.8954\n",
      "Epoch 2899/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.0932 - val_loss: 1.8421\n",
      "Epoch 2900/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7635 - val_loss: 1.8005\n",
      "Epoch 2901/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0091 - val_loss: 1.7493\n",
      "Epoch 2902/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7975 - val_loss: 1.7203\n",
      "Epoch 2903/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 10.2756 - val_loss: 1.8277\n",
      "Epoch 2904/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7713 - val_loss: 1.7166\n",
      "Epoch 2905/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8877 - val_loss: 1.7513\n",
      "Epoch 2906/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4483 - val_loss: 1.7287\n",
      "Epoch 2907/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7082 - val_loss: 1.7579\n",
      "Epoch 2908/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.9194 - val_loss: 1.7879\n",
      "Epoch 2909/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7522 - val_loss: 1.7592\n",
      "Epoch 2910/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8671 - val_loss: 1.7027\n",
      "Epoch 2911/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6009 - val_loss: 1.6596\n",
      "Epoch 2912/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.4184 - val_loss: 1.6788\n",
      "Epoch 2913/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.2494 - val_loss: 1.8067\n",
      "Epoch 2914/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5166 - val_loss: 1.7335\n",
      "Epoch 2915/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8911 - val_loss: 1.7600\n",
      "Epoch 2916/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.9651 - val_loss: 1.7861\n",
      "Epoch 2917/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5165 - val_loss: 1.6900\n",
      "Epoch 2918/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7165 - val_loss: 1.6799\n",
      "Epoch 2919/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7741 - val_loss: 1.6806\n",
      "Epoch 2920/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.6824 - val_loss: 1.7490\n",
      "Epoch 2921/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9793 - val_loss: 1.7823\n",
      "Epoch 2922/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4446 - val_loss: 1.6945\n",
      "Epoch 2923/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.0221 - val_loss: 1.7896\n",
      "Epoch 2924/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7194 - val_loss: 1.7695\n",
      "Epoch 2925/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.8763 - val_loss: 1.7393\n",
      "Epoch 2926/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4201 - val_loss: 1.7361\n",
      "Epoch 2927/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.2826 - val_loss: 1.7488\n",
      "Epoch 2928/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7754 - val_loss: 1.7310\n",
      "Epoch 2929/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8638 - val_loss: 1.6800\n",
      "Epoch 2930/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7736 - val_loss: 1.6641\n",
      "Epoch 2931/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.8144 - val_loss: 1.6884\n",
      "Epoch 2932/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.3174 - val_loss: 1.8133\n",
      "Epoch 2933/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5686 - val_loss: 1.7020\n",
      "Epoch 2934/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6488 - val_loss: 1.7263\n",
      "Epoch 2935/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.5645 - val_loss: 1.7169\n",
      "Epoch 2936/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.1998 - val_loss: 1.8066\n",
      "Epoch 2937/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.8924 - val_loss: 1.7509\n",
      "Epoch 2938/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4615 - val_loss: 1.7417\n",
      "Epoch 2939/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7752 - val_loss: 1.6773\n",
      "Epoch 2940/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2517 - val_loss: 1.6095\n",
      "Epoch 2941/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8218 - val_loss: 1.6752\n",
      "Epoch 2942/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7429 - val_loss: 1.7505\n",
      "Epoch 2943/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0032 - val_loss: 1.7880\n",
      "Epoch 2944/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6973 - val_loss: 1.7248\n",
      "Epoch 2945/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5505 - val_loss: 1.7343\n",
      "Epoch 2946/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5076 - val_loss: 1.7013\n",
      "Epoch 2947/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5606 - val_loss: 1.7053\n",
      "Epoch 2948/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.1648 - val_loss: 1.8205\n",
      "Epoch 2949/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.8873 - val_loss: 1.7779\n",
      "Epoch 2950/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6865 - val_loss: 1.6652\n",
      "Epoch 2951/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3757 - val_loss: 1.5205\n",
      "Epoch 2952/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.5917 - val_loss: 1.5782\n",
      "Epoch 2953/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.1234 - val_loss: 1.8439\n",
      "Epoch 2954/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8829 - val_loss: 1.8923\n",
      "Epoch 2955/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.0618 - val_loss: 1.7009\n",
      "Epoch 2956/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 10.0455 - val_loss: 1.6999\n",
      "Epoch 2957/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9738 - val_loss: 1.6675\n",
      "Epoch 2958/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7871 - val_loss: 1.6709\n",
      "Epoch 2959/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6897 - val_loss: 1.7180\n",
      "Epoch 2960/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0978 - val_loss: 1.7266\n",
      "Epoch 2961/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.9868 - val_loss: 1.7304\n",
      "Epoch 2962/10000\n",
      "90/90 [==============================] - 0s 226us/step - loss: 9.3145 - val_loss: 1.6519\n",
      "Epoch 2963/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8946 - val_loss: 1.6778\n",
      "Epoch 2964/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9495 - val_loss: 1.7178\n",
      "Epoch 2965/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8972 - val_loss: 1.6709\n",
      "Epoch 2966/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5764 - val_loss: 1.6563\n",
      "Epoch 2967/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.1298 - val_loss: 1.6353\n",
      "Epoch 2968/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.0129 - val_loss: 1.6236\n",
      "Epoch 2969/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4434 - val_loss: 1.6231\n",
      "Epoch 2970/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.6681 - val_loss: 1.6673\n",
      "Epoch 2971/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4761 - val_loss: 1.6998\n",
      "Epoch 2972/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8692 - val_loss: 1.6928\n",
      "Epoch 2973/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.0490 - val_loss: 1.7728\n",
      "Epoch 2974/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8166 - val_loss: 1.6958\n",
      "Epoch 2975/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.2868 - val_loss: 1.6363\n",
      "Epoch 2976/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.9685 - val_loss: 1.5912\n",
      "Epoch 2977/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5609 - val_loss: 1.5699\n",
      "Epoch 2978/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6478 - val_loss: 1.6832\n",
      "Epoch 2979/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0587 - val_loss: 1.7442\n",
      "Epoch 2980/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.1058 - val_loss: 1.7620\n",
      "Epoch 2981/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.1467 - val_loss: 1.7280\n",
      "Epoch 2982/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7031 - val_loss: 1.5913\n",
      "Epoch 2983/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6793 - val_loss: 1.6082\n",
      "Epoch 2984/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9341 - val_loss: 1.7718\n",
      "Epoch 2985/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7360 - val_loss: 1.7007\n",
      "Epoch 2986/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8128 - val_loss: 1.6534\n",
      "Epoch 2987/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.1755 - val_loss: 1.6492\n",
      "Epoch 2988/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8059 - val_loss: 1.6414\n",
      "Epoch 2989/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4045 - val_loss: 1.5658\n",
      "Epoch 2990/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.1276 - val_loss: 1.6216\n",
      "Epoch 2991/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5966 - val_loss: 1.6468\n",
      "Epoch 2992/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3177 - val_loss: 1.6810\n",
      "Epoch 2993/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6241 - val_loss: 1.6872\n",
      "Epoch 2994/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7678 - val_loss: 1.7051\n",
      "Epoch 2995/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 89us/step - loss: 9.7692 - val_loss: 1.6415\n",
      "Epoch 2996/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7573 - val_loss: 1.6580\n",
      "Epoch 2997/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7675 - val_loss: 1.7141\n",
      "Epoch 2998/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.5295 - val_loss: 1.6778\n",
      "Epoch 2999/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.1188 - val_loss: 1.6250\n",
      "Epoch 3000/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9840 - val_loss: 1.5954\n",
      "Epoch 3001/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6899 - val_loss: 1.4691\n",
      "Epoch 3002/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8236 - val_loss: 1.5056\n",
      "Epoch 3003/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8651 - val_loss: 1.7470\n",
      "Epoch 3004/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.8602 - val_loss: 1.7793\n",
      "Epoch 3005/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8467 - val_loss: 1.7102\n",
      "Epoch 3006/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5363 - val_loss: 1.6907\n",
      "Epoch 3007/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8585 - val_loss: 1.6333\n",
      "Epoch 3008/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8494 - val_loss: 1.7741\n",
      "Epoch 3009/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8311 - val_loss: 1.7956\n",
      "Epoch 3010/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7600 - val_loss: 1.5767\n",
      "Epoch 3011/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7420 - val_loss: 1.5583\n",
      "Epoch 3012/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.3695 - val_loss: 1.6926\n",
      "Epoch 3013/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.7412 - val_loss: 1.6543\n",
      "Epoch 3014/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.0116 - val_loss: 1.6854\n",
      "Epoch 3015/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.1207 - val_loss: 1.6924\n",
      "Epoch 3016/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.5496 - val_loss: 1.6258\n",
      "Epoch 3017/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5230 - val_loss: 1.5476\n",
      "Epoch 3018/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7079 - val_loss: 1.6077\n",
      "Epoch 3019/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8623 - val_loss: 1.7085\n",
      "Epoch 3020/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7009 - val_loss: 1.7300\n",
      "Epoch 3021/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8448 - val_loss: 1.6528\n",
      "Epoch 3022/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.3561 - val_loss: 1.6195\n",
      "Epoch 3023/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.2592 - val_loss: 1.6122\n",
      "Epoch 3024/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6672 - val_loss: 1.5644\n",
      "Epoch 3025/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5198 - val_loss: 1.5469\n",
      "Epoch 3026/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.7329 - val_loss: 1.5451\n",
      "Epoch 3027/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.9462 - val_loss: 1.6708\n",
      "Epoch 3028/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.9232 - val_loss: 1.6432\n",
      "Epoch 3029/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6230 - val_loss: 1.5180\n",
      "Epoch 3030/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7307 - val_loss: 1.5598\n",
      "Epoch 3031/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8632 - val_loss: 1.6522\n",
      "Epoch 3032/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.1976 - val_loss: 1.7883\n",
      "Epoch 3033/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9314 - val_loss: 1.7390\n",
      "Epoch 3034/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8168 - val_loss: 1.7024\n",
      "Epoch 3035/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.0817 - val_loss: 1.6878\n",
      "Epoch 3036/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4105 - val_loss: 1.6519\n",
      "Epoch 3037/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 10.0687 - val_loss: 1.6604\n",
      "Epoch 3038/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5838 - val_loss: 1.6141\n",
      "Epoch 3039/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7321 - val_loss: 1.5438\n",
      "Epoch 3040/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8435 - val_loss: 1.5340\n",
      "Epoch 3041/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8550 - val_loss: 1.5734\n",
      "Epoch 3042/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4989 - val_loss: 1.5945\n",
      "Epoch 3043/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7791 - val_loss: 1.5659\n",
      "Epoch 3044/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6491 - val_loss: 1.5686\n",
      "Epoch 3045/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8666 - val_loss: 1.6798\n",
      "Epoch 3046/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8222 - val_loss: 1.6473\n",
      "Epoch 3047/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8896 - val_loss: 1.6233\n",
      "Epoch 3048/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7181 - val_loss: 1.6486\n",
      "Epoch 3049/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.9700 - val_loss: 1.7269\n",
      "Epoch 3050/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6219 - val_loss: 1.6168\n",
      "Epoch 3051/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5085 - val_loss: 1.5295\n",
      "Epoch 3052/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.1776 - val_loss: 1.6145\n",
      "Epoch 3053/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9751 - val_loss: 1.7060\n",
      "Epoch 3054/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6088 - val_loss: 1.6357\n",
      "Epoch 3055/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0062 - val_loss: 1.5515\n",
      "Epoch 3056/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8593 - val_loss: 1.6199\n",
      "Epoch 3057/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.0172 - val_loss: 1.6182\n",
      "Epoch 3058/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4110 - val_loss: 1.5735\n",
      "Epoch 3059/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8712 - val_loss: 1.5984\n",
      "Epoch 3060/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.8444 - val_loss: 1.6296\n",
      "Epoch 3061/10000\n",
      "90/90 [==============================] - 0s 202us/step - loss: 9.3764 - val_loss: 1.5986\n",
      "Epoch 3062/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.1356 - val_loss: 1.5567\n",
      "Epoch 3063/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.9791 - val_loss: 1.6207\n",
      "Epoch 3064/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8224 - val_loss: 1.6353\n",
      "Epoch 3065/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7407 - val_loss: 1.6341\n",
      "Epoch 3066/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7015 - val_loss: 1.6302\n",
      "Epoch 3067/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6766 - val_loss: 1.5601\n",
      "Epoch 3068/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.1806 - val_loss: 1.5860\n",
      "Epoch 3069/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7611 - val_loss: 1.5408\n",
      "Epoch 3070/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7613 - val_loss: 1.5243\n",
      "Epoch 3071/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.6812 - val_loss: 1.5477\n",
      "Epoch 3072/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7859 - val_loss: 1.5713\n",
      "Epoch 3073/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.9076 - val_loss: 1.6360\n",
      "Epoch 3074/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0391 - val_loss: 1.6763\n",
      "Epoch 3075/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7016 - val_loss: 1.5482\n",
      "Epoch 3076/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4781 - val_loss: 1.5361\n",
      "Epoch 3077/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.4863 - val_loss: 1.7403\n",
      "Epoch 3078/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7016 - val_loss: 1.5980\n",
      "Epoch 3079/10000\n",
      "90/90 [==============================] - 0s 82us/step - loss: 9.6904 - val_loss: 1.5626\n",
      "Epoch 3080/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7237 - val_loss: 1.5771\n",
      "Epoch 3081/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6905 - val_loss: 1.5669\n",
      "Epoch 3082/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.1070 - val_loss: 1.6679\n",
      "Epoch 3083/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4821 - val_loss: 1.5827\n",
      "Epoch 3084/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8375 - val_loss: 1.5580\n",
      "Epoch 3085/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.0406 - val_loss: 1.6168\n",
      "Epoch 3086/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.5972 - val_loss: 1.5974\n",
      "Epoch 3087/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.1140 - val_loss: 1.5626\n",
      "Epoch 3088/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.3661 - val_loss: 1.4513\n",
      "Epoch 3089/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.5794 - val_loss: 1.5369\n",
      "Epoch 3090/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 9.7369 - val_loss: 1.6667\n",
      "Epoch 3091/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7685 - val_loss: 1.6092\n",
      "Epoch 3092/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.0353 - val_loss: 1.6061\n",
      "Epoch 3093/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7250 - val_loss: 1.5724\n",
      "Epoch 3094/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5875 - val_loss: 1.6177\n",
      "Epoch 3095/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 9.7765 - val_loss: 1.5797\n",
      "Epoch 3096/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5359 - val_loss: 1.5913\n",
      "Epoch 3097/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6820 - val_loss: 1.6580\n",
      "Epoch 3098/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7887 - val_loss: 1.6228\n",
      "Epoch 3099/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6116 - val_loss: 1.5524\n",
      "Epoch 3100/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7797 - val_loss: 1.4216\n",
      "Epoch 3101/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.0739 - val_loss: 1.5725\n",
      "Epoch 3102/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.0938 - val_loss: 1.7511\n",
      "Epoch 3103/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8001 - val_loss: 1.6716\n",
      "Epoch 3104/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8761 - val_loss: 1.5807\n",
      "Epoch 3105/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.8996 - val_loss: 1.5906\n",
      "Epoch 3106/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7428 - val_loss: 1.6409\n",
      "Epoch 3107/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9316 - val_loss: 1.5946\n",
      "Epoch 3108/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9276 - val_loss: 1.5438\n",
      "Epoch 3109/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.0601 - val_loss: 1.5180\n",
      "Epoch 3110/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5813 - val_loss: 1.3946\n",
      "Epoch 3111/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6539 - val_loss: 1.4934\n",
      "Epoch 3112/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.2086 - val_loss: 1.7286\n",
      "Epoch 3113/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.0320 - val_loss: 1.7089\n",
      "Epoch 3114/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8484 - val_loss: 1.5810\n",
      "Epoch 3115/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8861 - val_loss: 1.4996\n",
      "Epoch 3116/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.1924 - val_loss: 1.4713\n",
      "Epoch 3117/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5102 - val_loss: 1.4737\n",
      "Epoch 3118/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3420 - val_loss: 1.6098\n",
      "Epoch 3119/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6968 - val_loss: 1.6615\n",
      "Epoch 3120/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8763 - val_loss: 1.5358\n",
      "Epoch 3121/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.5560 - val_loss: 1.4686\n",
      "Epoch 3122/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6456 - val_loss: 1.5287\n",
      "Epoch 3123/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.1418 - val_loss: 1.6173\n",
      "Epoch 3124/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9047 - val_loss: 1.6109\n",
      "Epoch 3125/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 9.8814 - val_loss: 1.6167\n",
      "Epoch 3126/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.9105 - val_loss: 1.5409\n",
      "Epoch 3127/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.4241 - val_loss: 1.5134\n",
      "Epoch 3128/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9134 - val_loss: 1.6371\n",
      "Epoch 3129/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.6233 - val_loss: 1.6270\n",
      "Epoch 3130/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4485 - val_loss: 1.5292\n",
      "Epoch 3131/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6049 - val_loss: 1.4663\n",
      "Epoch 3132/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.3862 - val_loss: 1.4664\n",
      "Epoch 3133/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.9520 - val_loss: 1.5147\n",
      "Epoch 3134/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3815 - val_loss: 1.4345\n",
      "Epoch 3135/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9914 - val_loss: 1.5089\n",
      "Epoch 3136/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5680 - val_loss: 1.4739\n",
      "Epoch 3137/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4709 - val_loss: 1.5085\n",
      "Epoch 3138/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.1773 - val_loss: 1.6926\n",
      "Epoch 3139/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.4435 - val_loss: 1.7457\n",
      "Epoch 3140/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7808 - val_loss: 1.5291\n",
      "Epoch 3141/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0328 - val_loss: 1.4824\n",
      "Epoch 3142/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.0381 - val_loss: 1.4835\n",
      "Epoch 3143/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.0774 - val_loss: 1.5489\n",
      "Epoch 3144/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4585 - val_loss: 1.4555\n",
      "Epoch 3145/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.8451 - val_loss: 1.5568\n",
      "Epoch 3146/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7122 - val_loss: 1.5492\n",
      "Epoch 3147/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 10.1420 - val_loss: 1.5072\n",
      "Epoch 3148/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4227 - val_loss: 1.4807\n",
      "Epoch 3149/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 111us/step - loss: 9.6494 - val_loss: 1.4981\n",
      "Epoch 3150/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7447 - val_loss: 1.5035\n",
      "Epoch 3151/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5389 - val_loss: 1.5174\n",
      "Epoch 3152/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7654 - val_loss: 1.5027\n",
      "Epoch 3153/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9537 - val_loss: 1.5571\n",
      "Epoch 3154/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6848 - val_loss: 1.5679\n",
      "Epoch 3155/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.9712 - val_loss: 1.5491\n",
      "Epoch 3156/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.4962 - val_loss: 1.4241\n",
      "Epoch 3157/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7548 - val_loss: 1.4801\n",
      "Epoch 3158/10000\n",
      "90/90 [==============================] - 0s 259us/step - loss: 9.6453 - val_loss: 1.5821\n",
      "Epoch 3159/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.9236 - val_loss: 1.7119\n",
      "Epoch 3160/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.2628 - val_loss: 1.6936\n",
      "Epoch 3161/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4777 - val_loss: 1.4895\n",
      "Epoch 3162/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7069 - val_loss: 1.3991\n",
      "Epoch 3163/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7101 - val_loss: 1.3700\n",
      "Epoch 3164/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6708 - val_loss: 1.4934\n",
      "Epoch 3165/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.9376 - val_loss: 1.6766\n",
      "Epoch 3166/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.0457 - val_loss: 1.6620\n",
      "Epoch 3167/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6095 - val_loss: 1.4543\n",
      "Epoch 3168/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.0466 - val_loss: 1.4852\n",
      "Epoch 3169/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5090 - val_loss: 1.4480\n",
      "Epoch 3170/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9250 - val_loss: 1.5417\n",
      "Epoch 3171/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6792 - val_loss: 1.5936\n",
      "Epoch 3172/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8768 - val_loss: 1.5273\n",
      "Epoch 3173/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8329 - val_loss: 1.4646\n",
      "Epoch 3174/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8417 - val_loss: 1.4566\n",
      "Epoch 3175/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8938 - val_loss: 1.5173\n",
      "Epoch 3176/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 10.0875 - val_loss: 1.5755\n",
      "Epoch 3177/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7535 - val_loss: 1.5350\n",
      "Epoch 3178/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.8761 - val_loss: 1.5111\n",
      "Epoch 3179/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.0699 - val_loss: 1.5506\n",
      "Epoch 3180/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3516 - val_loss: 1.3975\n",
      "Epoch 3181/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.9728 - val_loss: 1.5501\n",
      "Epoch 3182/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.1697 - val_loss: 1.6630\n",
      "Epoch 3183/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.8166 - val_loss: 1.5010\n",
      "Epoch 3184/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9065 - val_loss: 1.4270\n",
      "Epoch 3185/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7320 - val_loss: 1.4585\n",
      "Epoch 3186/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.0913 - val_loss: 1.5998\n",
      "Epoch 3187/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0574 - val_loss: 1.5642\n",
      "Epoch 3188/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.4168 - val_loss: 1.4164\n",
      "Epoch 3189/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5132 - val_loss: 1.4662\n",
      "Epoch 3190/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9827 - val_loss: 1.4884\n",
      "Epoch 3191/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6276 - val_loss: 1.5873\n",
      "Epoch 3192/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4986 - val_loss: 1.5057\n",
      "Epoch 3193/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6426 - val_loss: 1.5198\n",
      "Epoch 3194/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.9878 - val_loss: 1.5274\n",
      "Epoch 3195/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.0648 - val_loss: 1.5319\n",
      "Epoch 3196/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8261 - val_loss: 1.4903\n",
      "Epoch 3197/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4811 - val_loss: 1.5282\n",
      "Epoch 3198/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.9746 - val_loss: 1.5437\n",
      "Epoch 3199/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9711 - val_loss: 1.5820\n",
      "Epoch 3200/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5134 - val_loss: 1.4465\n",
      "Epoch 3201/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7928 - val_loss: 1.4739\n",
      "Epoch 3202/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 10.1834 - val_loss: 1.5546\n",
      "Epoch 3203/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 10.2715 - val_loss: 1.6003\n",
      "Epoch 3204/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7185 - val_loss: 1.5503\n",
      "Epoch 3205/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4924 - val_loss: 1.4416\n",
      "Epoch 3206/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.7725 - val_loss: 1.4062\n",
      "Epoch 3207/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 9.9258 - val_loss: 1.4780\n",
      "Epoch 3208/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.2907 - val_loss: 1.6194\n",
      "Epoch 3209/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.5963 - val_loss: 1.5438\n",
      "Epoch 3210/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.7421 - val_loss: 1.4379\n",
      "Epoch 3211/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.5228 - val_loss: 1.4254\n",
      "Epoch 3212/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.5969 - val_loss: 1.4583\n",
      "Epoch 3213/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 9.7069 - val_loss: 1.4734\n",
      "Epoch 3214/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.7373 - val_loss: 1.5101\n",
      "Epoch 3215/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8163 - val_loss: 1.5081\n",
      "Epoch 3216/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.9535 - val_loss: 1.5063\n",
      "Epoch 3217/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 9.8900 - val_loss: 1.4926\n",
      "Epoch 3218/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.5216 - val_loss: 1.4878\n",
      "Epoch 3219/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0875 - val_loss: 1.5649\n",
      "Epoch 3220/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9112 - val_loss: 1.5209\n",
      "Epoch 3221/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.8982 - val_loss: 1.4464\n",
      "Epoch 3222/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6411 - val_loss: 1.4530\n",
      "Epoch 3223/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9819 - val_loss: 1.4783\n",
      "Epoch 3224/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.8596 - val_loss: 1.4387\n",
      "Epoch 3225/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6138 - val_loss: 1.3998\n",
      "Epoch 3226/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0649 - val_loss: 1.4651\n",
      "Epoch 3227/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7013 - val_loss: 1.5123\n",
      "Epoch 3228/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.5802 - val_loss: 1.5325\n",
      "Epoch 3229/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 9.8943 - val_loss: 1.4699\n",
      "Epoch 3230/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4169 - val_loss: 1.3841\n",
      "Epoch 3231/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6387 - val_loss: 1.4651\n",
      "Epoch 3232/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4421 - val_loss: 1.4345\n",
      "Epoch 3233/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7835 - val_loss: 1.4811\n",
      "Epoch 3234/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6481 - val_loss: 1.5737\n",
      "Epoch 3235/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0133 - val_loss: 1.5881\n",
      "Epoch 3236/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.0598 - val_loss: 1.5506\n",
      "Epoch 3237/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.7747 - val_loss: 1.4743\n",
      "Epoch 3238/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8481 - val_loss: 1.5228\n",
      "Epoch 3239/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.5834 - val_loss: 1.5139\n",
      "Epoch 3240/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6764 - val_loss: 1.3969\n",
      "Epoch 3241/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9345 - val_loss: 1.4078\n",
      "Epoch 3242/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.3007 - val_loss: 1.5186\n",
      "Epoch 3243/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.9150 - val_loss: 1.4331\n",
      "Epoch 3244/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9810 - val_loss: 1.4197\n",
      "Epoch 3245/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6041 - val_loss: 1.4961\n",
      "Epoch 3246/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6882 - val_loss: 1.5582\n",
      "Epoch 3247/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5934 - val_loss: 1.4901\n",
      "Epoch 3248/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4643 - val_loss: 1.5048\n",
      "Epoch 3249/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.9859 - val_loss: 1.5672\n",
      "Epoch 3250/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5287 - val_loss: 1.4141\n",
      "Epoch 3251/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.7863 - val_loss: 1.4275\n",
      "Epoch 3252/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.0727 - val_loss: 1.5115\n",
      "Epoch 3253/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7821 - val_loss: 1.4525\n",
      "Epoch 3254/10000\n",
      "90/90 [==============================] - 0s 260us/step - loss: 9.5137 - val_loss: 1.4604\n",
      "Epoch 3255/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8780 - val_loss: 1.4610\n",
      "Epoch 3256/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 10.2865 - val_loss: 1.6141\n",
      "Epoch 3257/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9403 - val_loss: 1.5438\n",
      "Epoch 3258/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.8375 - val_loss: 1.4632\n",
      "Epoch 3259/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7380 - val_loss: 1.4006\n",
      "Epoch 3260/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5361 - val_loss: 1.2970\n",
      "Epoch 3261/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8163 - val_loss: 1.4169\n",
      "Epoch 3262/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8299 - val_loss: 1.6191\n",
      "Epoch 3263/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8672 - val_loss: 1.5663\n",
      "Epoch 3264/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.6257 - val_loss: 1.4609\n",
      "Epoch 3265/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7076 - val_loss: 1.4480\n",
      "Epoch 3266/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.2586 - val_loss: 1.6252\n",
      "Epoch 3267/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.1366 - val_loss: 1.5739\n",
      "Epoch 3268/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6706 - val_loss: 1.4463\n",
      "Epoch 3269/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5550 - val_loss: 1.4237\n",
      "Epoch 3270/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2707 - val_loss: 1.3988\n",
      "Epoch 3271/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4494 - val_loss: 1.4705\n",
      "Epoch 3272/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3034 - val_loss: 1.3985\n",
      "Epoch 3273/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9234 - val_loss: 1.4389\n",
      "Epoch 3274/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.6630 - val_loss: 1.4704\n",
      "Epoch 3275/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7081 - val_loss: 1.4363\n",
      "Epoch 3276/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.9503 - val_loss: 1.4794\n",
      "Epoch 3277/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7405 - val_loss: 1.5067\n",
      "Epoch 3278/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6253 - val_loss: 1.5744\n",
      "Epoch 3279/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.7104 - val_loss: 1.5009\n",
      "Epoch 3280/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8782 - val_loss: 1.4998\n",
      "Epoch 3281/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7602 - val_loss: 1.4603\n",
      "Epoch 3282/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.6426 - val_loss: 1.3261\n",
      "Epoch 3283/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.3775 - val_loss: 1.3837\n",
      "Epoch 3284/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5593 - val_loss: 1.5068\n",
      "Epoch 3285/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5991 - val_loss: 1.5651\n",
      "Epoch 3286/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8369 - val_loss: 1.4882\n",
      "Epoch 3287/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.9175 - val_loss: 1.4933\n",
      "Epoch 3288/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7921 - val_loss: 1.5519\n",
      "Epoch 3289/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8022 - val_loss: 1.5383\n",
      "Epoch 3290/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6597 - val_loss: 1.4143\n",
      "Epoch 3291/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.0127 - val_loss: 1.3835\n",
      "Epoch 3292/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8889 - val_loss: 1.5361\n",
      "Epoch 3293/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4035 - val_loss: 1.4606\n",
      "Epoch 3294/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5307 - val_loss: 1.4216\n",
      "Epoch 3295/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7077 - val_loss: 1.4432\n",
      "Epoch 3296/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4296 - val_loss: 1.4535\n",
      "Epoch 3297/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.0092 - val_loss: 1.5692\n",
      "Epoch 3298/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4620 - val_loss: 1.5099\n",
      "Epoch 3299/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7442 - val_loss: 1.4705\n",
      "Epoch 3300/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5553 - val_loss: 1.4062\n",
      "Epoch 3301/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7295 - val_loss: 1.4241\n",
      "Epoch 3302/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6115 - val_loss: 1.3943\n",
      "Epoch 3303/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 90us/step - loss: 9.8694 - val_loss: 1.3866\n",
      "Epoch 3304/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.6173 - val_loss: 1.3914\n",
      "Epoch 3305/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3011 - val_loss: 1.4060\n",
      "Epoch 3306/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.2711 - val_loss: 1.4761\n",
      "Epoch 3307/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 10.1129 - val_loss: 1.5933\n",
      "Epoch 3308/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.9948 - val_loss: 1.5048\n",
      "Epoch 3309/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.2262 - val_loss: 1.4108\n",
      "Epoch 3310/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4004 - val_loss: 1.3965\n",
      "Epoch 3311/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8787 - val_loss: 1.4195\n",
      "Epoch 3312/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.9278 - val_loss: 1.4911\n",
      "Epoch 3313/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8613 - val_loss: 1.5088\n",
      "Epoch 3314/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0161 - val_loss: 1.3838\n",
      "Epoch 3315/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9962 - val_loss: 1.3957\n",
      "Epoch 3316/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7934 - val_loss: 1.3968\n",
      "Epoch 3317/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.1874 - val_loss: 1.3281\n",
      "Epoch 3318/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 10.1832 - val_loss: 1.5234\n",
      "Epoch 3319/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5160 - val_loss: 1.4840\n",
      "Epoch 3320/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.7831 - val_loss: 1.4645\n",
      "Epoch 3321/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7847 - val_loss: 1.3834\n",
      "Epoch 3322/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7353 - val_loss: 1.3759\n",
      "Epoch 3323/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.2135 - val_loss: 1.4079\n",
      "Epoch 3324/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2238 - val_loss: 1.3959\n",
      "Epoch 3325/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4838 - val_loss: 1.4164\n",
      "Epoch 3326/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7569 - val_loss: 1.5042\n",
      "Epoch 3327/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8013 - val_loss: 1.4459\n",
      "Epoch 3328/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5188 - val_loss: 1.4950\n",
      "Epoch 3329/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 9.5712 - val_loss: 1.4914\n",
      "Epoch 3330/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7867 - val_loss: 1.4896\n",
      "Epoch 3331/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3072 - val_loss: 1.4549\n",
      "Epoch 3332/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6678 - val_loss: 1.4561\n",
      "Epoch 3333/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8457 - val_loss: 1.4678\n",
      "Epoch 3334/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.0486 - val_loss: 1.4287\n",
      "Epoch 3335/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 10.0144 - val_loss: 1.4852\n",
      "Epoch 3336/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.5949 - val_loss: 1.3721\n",
      "Epoch 3337/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8883 - val_loss: 1.3883\n",
      "Epoch 3338/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6552 - val_loss: 1.3713\n",
      "Epoch 3339/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5999 - val_loss: 1.4437\n",
      "Epoch 3340/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4163 - val_loss: 1.4440\n",
      "Epoch 3341/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.9949 - val_loss: 1.4557\n",
      "Epoch 3342/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6113 - val_loss: 1.4158\n",
      "Epoch 3343/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7168 - val_loss: 1.3852\n",
      "Epoch 3344/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.5282 - val_loss: 1.3663\n",
      "Epoch 3345/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6872 - val_loss: 1.4068\n",
      "Epoch 3346/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4100 - val_loss: 1.5320\n",
      "Epoch 3347/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7383 - val_loss: 1.5715\n",
      "Epoch 3348/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3794 - val_loss: 1.4749\n",
      "Epoch 3349/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5162 - val_loss: 1.3353\n",
      "Epoch 3350/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.0770 - val_loss: 1.3492\n",
      "Epoch 3351/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6658 - val_loss: 1.4508\n",
      "Epoch 3352/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.7356 - val_loss: 1.4486\n",
      "Epoch 3353/10000\n",
      "90/90 [==============================] - 0s 187us/step - loss: 10.0348 - val_loss: 1.5595\n",
      "Epoch 3354/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.3449 - val_loss: 1.4295\n",
      "Epoch 3355/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.6614 - val_loss: 1.3962\n",
      "Epoch 3356/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3995 - val_loss: 1.4045\n",
      "Epoch 3357/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6514 - val_loss: 1.4958\n",
      "Epoch 3358/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5763 - val_loss: 1.5362\n",
      "Epoch 3359/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.1725 - val_loss: 1.5298\n",
      "Epoch 3360/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8563 - val_loss: 1.4139\n",
      "Epoch 3361/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4336 - val_loss: 1.3643\n",
      "Epoch 3362/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.0590 - val_loss: 1.4540\n",
      "Epoch 3363/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4174 - val_loss: 1.3971\n",
      "Epoch 3364/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.2540 - val_loss: 1.3473\n",
      "Epoch 3365/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.0603 - val_loss: 1.4258\n",
      "Epoch 3366/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8043 - val_loss: 1.3820\n",
      "Epoch 3367/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8964 - val_loss: 1.3942\n",
      "Epoch 3368/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8300 - val_loss: 1.4077\n",
      "Epoch 3369/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3951 - val_loss: 1.4207\n",
      "Epoch 3370/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.3709 - val_loss: 1.6109\n",
      "Epoch 3371/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9443 - val_loss: 1.4679\n",
      "Epoch 3372/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.7834 - val_loss: 1.3364\n",
      "Epoch 3373/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.9088 - val_loss: 1.3623\n",
      "Epoch 3374/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.1102 - val_loss: 1.4225\n",
      "Epoch 3375/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6035 - val_loss: 1.4749\n",
      "Epoch 3376/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8816 - val_loss: 1.4790\n",
      "Epoch 3377/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2420 - val_loss: 1.3131\n",
      "Epoch 3378/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.9146 - val_loss: 1.3360\n",
      "Epoch 3379/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6297 - val_loss: 1.3333\n",
      "Epoch 3380/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6019 - val_loss: 1.4164\n",
      "Epoch 3381/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.6670 - val_loss: 1.4918\n",
      "Epoch 3382/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5678 - val_loss: 1.4696\n",
      "Epoch 3383/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.5559 - val_loss: 1.4282\n",
      "Epoch 3384/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8557 - val_loss: 1.4706\n",
      "Epoch 3385/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4086 - val_loss: 1.3871\n",
      "Epoch 3386/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.8408 - val_loss: 1.4932\n",
      "Epoch 3387/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3282 - val_loss: 1.4630\n",
      "Epoch 3388/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5549 - val_loss: 1.4369\n",
      "Epoch 3389/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 10.0334 - val_loss: 1.3998\n",
      "Epoch 3390/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7244 - val_loss: 1.3977\n",
      "Epoch 3391/10000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 9.9398 - val_loss: 1.4653\n",
      "Epoch 3392/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.8341 - val_loss: 1.4604\n",
      "Epoch 3393/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3221 - val_loss: 1.3889\n",
      "Epoch 3394/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5103 - val_loss: 1.4336\n",
      "Epoch 3395/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6109 - val_loss: 1.4931\n",
      "Epoch 3396/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.8059 - val_loss: 1.3771\n",
      "Epoch 3397/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5704 - val_loss: 1.3431\n",
      "Epoch 3398/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4959 - val_loss: 1.3608\n",
      "Epoch 3399/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4056 - val_loss: 1.4052\n",
      "Epoch 3400/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8938 - val_loss: 1.5210\n",
      "Epoch 3401/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6607 - val_loss: 1.4930\n",
      "Epoch 3402/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.7012 - val_loss: 1.4663\n",
      "Epoch 3403/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6833 - val_loss: 1.4530\n",
      "Epoch 3404/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0279 - val_loss: 1.3907\n",
      "Epoch 3405/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4933 - val_loss: 1.2968\n",
      "Epoch 3406/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.0108 - val_loss: 1.4352\n",
      "Epoch 3407/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4418 - val_loss: 1.4454\n",
      "Epoch 3408/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9129 - val_loss: 1.4971\n",
      "Epoch 3409/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8250 - val_loss: 1.5149\n",
      "Epoch 3410/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8964 - val_loss: 1.4460\n",
      "Epoch 3411/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 9.8553 - val_loss: 1.3821\n",
      "Epoch 3412/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.8525 - val_loss: 1.3787\n",
      "Epoch 3413/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9711 - val_loss: 1.4434\n",
      "Epoch 3414/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9669 - val_loss: 1.4833\n",
      "Epoch 3415/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5039 - val_loss: 1.3664\n",
      "Epoch 3416/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.9836 - val_loss: 1.3533\n",
      "Epoch 3417/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8347 - val_loss: 1.4378\n",
      "Epoch 3418/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8129 - val_loss: 1.3975\n",
      "Epoch 3419/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9147 - val_loss: 1.4499\n",
      "Epoch 3420/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4163 - val_loss: 1.3637\n",
      "Epoch 3421/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.3848 - val_loss: 1.2903\n",
      "Epoch 3422/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6211 - val_loss: 1.3056\n",
      "Epoch 3423/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.1148 - val_loss: 1.4232\n",
      "Epoch 3424/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3819 - val_loss: 1.3495\n",
      "Epoch 3425/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3785 - val_loss: 1.3172\n",
      "Epoch 3426/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4889 - val_loss: 1.3970\n",
      "Epoch 3427/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6804 - val_loss: 1.4698\n",
      "Epoch 3428/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2469 - val_loss: 1.4384\n",
      "Epoch 3429/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.6295 - val_loss: 1.4138\n",
      "Epoch 3430/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.5949 - val_loss: 1.4070\n",
      "Epoch 3431/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4932 - val_loss: 1.4858\n",
      "Epoch 3432/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.9177 - val_loss: 1.5083\n",
      "Epoch 3433/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5519 - val_loss: 1.3997\n",
      "Epoch 3434/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7606 - val_loss: 1.3448\n",
      "Epoch 3435/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8199 - val_loss: 1.3561\n",
      "Epoch 3436/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3692 - val_loss: 1.3236\n",
      "Epoch 3437/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8219 - val_loss: 1.4723\n",
      "Epoch 3438/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9270 - val_loss: 1.4606\n",
      "Epoch 3439/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9726 - val_loss: 1.4361\n",
      "Epoch 3440/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.3660 - val_loss: 1.3674\n",
      "Epoch 3441/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8272 - val_loss: 1.3962\n",
      "Epoch 3442/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.5757 - val_loss: 1.4476\n",
      "Epoch 3443/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6934 - val_loss: 1.4205\n",
      "Epoch 3444/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6441 - val_loss: 1.3885\n",
      "Epoch 3445/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5644 - val_loss: 1.3008\n",
      "Epoch 3446/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.0462 - val_loss: 1.5097\n",
      "Epoch 3447/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.4145 - val_loss: 1.4788\n",
      "Epoch 3448/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.6920 - val_loss: 1.3525\n",
      "Epoch 3449/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7175 - val_loss: 1.3903\n",
      "Epoch 3450/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7456 - val_loss: 1.4755\n",
      "Epoch 3451/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5830 - val_loss: 1.4734\n",
      "Epoch 3452/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8764 - val_loss: 1.4233\n",
      "Epoch 3453/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.6730 - val_loss: 1.3891\n",
      "Epoch 3454/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 9.8260 - val_loss: 1.3060\n",
      "Epoch 3455/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.9148 - val_loss: 1.3361\n",
      "Epoch 3456/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.3610 - val_loss: 1.3596\n",
      "Epoch 3457/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3782 - val_loss: 1.3887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3458/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4451 - val_loss: 1.3947\n",
      "Epoch 3459/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.9681 - val_loss: 1.5253\n",
      "Epoch 3460/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6187 - val_loss: 1.4108\n",
      "Epoch 3461/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6942 - val_loss: 1.4177\n",
      "Epoch 3462/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9530 - val_loss: 1.3867\n",
      "Epoch 3463/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8908 - val_loss: 1.4164\n",
      "Epoch 3464/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7833 - val_loss: 1.4538\n",
      "Epoch 3465/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.5712 - val_loss: 1.4261\n",
      "Epoch 3466/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.7070 - val_loss: 1.3364\n",
      "Epoch 3467/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5862 - val_loss: 1.2284\n",
      "Epoch 3468/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 9.7608 - val_loss: 1.3074\n",
      "Epoch 3469/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.5340 - val_loss: 1.4374\n",
      "Epoch 3470/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.8240 - val_loss: 1.6290\n",
      "Epoch 3471/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9387 - val_loss: 1.6183\n",
      "Epoch 3472/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.9557 - val_loss: 1.4267\n",
      "Epoch 3473/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3381 - val_loss: 1.2665\n",
      "Epoch 3474/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.4007 - val_loss: 1.2854\n",
      "Epoch 3475/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8690 - val_loss: 1.5465\n",
      "Epoch 3476/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3961 - val_loss: 1.4374\n",
      "Epoch 3477/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5628 - val_loss: 1.3615\n",
      "Epoch 3478/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9492 - val_loss: 1.3379\n",
      "Epoch 3479/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.1846 - val_loss: 1.3030\n",
      "Epoch 3480/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4540 - val_loss: 1.4535\n",
      "Epoch 3481/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9409 - val_loss: 1.5684\n",
      "Epoch 3482/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 10.0364 - val_loss: 1.5447\n",
      "Epoch 3483/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7520 - val_loss: 1.3823\n",
      "Epoch 3484/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.7330 - val_loss: 1.2458\n",
      "Epoch 3485/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8504 - val_loss: 1.3675\n",
      "Epoch 3486/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9068 - val_loss: 1.5175\n",
      "Epoch 3487/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5607 - val_loss: 1.4102\n",
      "Epoch 3488/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8332 - val_loss: 1.3430\n",
      "Epoch 3489/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.5450 - val_loss: 1.4304\n",
      "Epoch 3490/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3028 - val_loss: 1.4105\n",
      "Epoch 3491/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3291 - val_loss: 1.2655\n",
      "Epoch 3492/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7683 - val_loss: 1.3975\n",
      "Epoch 3493/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6406 - val_loss: 1.4142\n",
      "Epoch 3494/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 10.0286 - val_loss: 1.5262\n",
      "Epoch 3495/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.8802 - val_loss: 1.4202\n",
      "Epoch 3496/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.8514 - val_loss: 1.3920\n",
      "Epoch 3497/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.8539 - val_loss: 1.4348\n",
      "Epoch 3498/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.2005 - val_loss: 1.5290\n",
      "Epoch 3499/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.7814 - val_loss: 1.3904\n",
      "Epoch 3500/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3218 - val_loss: 1.2657\n",
      "Epoch 3501/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4176 - val_loss: 1.2389\n",
      "Epoch 3502/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9037 - val_loss: 1.3826\n",
      "Epoch 3503/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2803 - val_loss: 1.3317\n",
      "Epoch 3504/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.9490 - val_loss: 1.3870\n",
      "Epoch 3505/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7893 - val_loss: 1.4394\n",
      "Epoch 3506/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5262 - val_loss: 1.3699\n",
      "Epoch 3507/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.3310 - val_loss: 1.3295\n",
      "Epoch 3508/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7749 - val_loss: 1.4747\n",
      "Epoch 3509/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7015 - val_loss: 1.4849\n",
      "Epoch 3510/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7649 - val_loss: 1.4691\n",
      "Epoch 3511/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.0058 - val_loss: 1.4158\n",
      "Epoch 3512/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7752 - val_loss: 1.3615\n",
      "Epoch 3513/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.7467 - val_loss: 1.3878\n",
      "Epoch 3514/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 10.2484 - val_loss: 1.5303\n",
      "Epoch 3515/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4719 - val_loss: 1.3129\n",
      "Epoch 3516/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6555 - val_loss: 1.2067\n",
      "Epoch 3517/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.2070 - val_loss: 1.2910\n",
      "Epoch 3518/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4890 - val_loss: 1.4465\n",
      "Epoch 3519/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.5826 - val_loss: 1.4326\n",
      "Epoch 3520/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7974 - val_loss: 1.4475\n",
      "Epoch 3521/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6640 - val_loss: 1.3783\n",
      "Epoch 3522/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5807 - val_loss: 1.4681\n",
      "Epoch 3523/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7839 - val_loss: 1.5019\n",
      "Epoch 3524/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4563 - val_loss: 1.3349\n",
      "Epoch 3525/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.4845 - val_loss: 1.2852\n",
      "Epoch 3526/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4164 - val_loss: 1.3725\n",
      "Epoch 3527/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7104 - val_loss: 1.4254\n",
      "Epoch 3528/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9833 - val_loss: 1.4584\n",
      "Epoch 3529/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6104 - val_loss: 1.3281\n",
      "Epoch 3530/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7628 - val_loss: 1.3440\n",
      "Epoch 3531/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7344 - val_loss: 1.3232\n",
      "Epoch 3532/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9363 - val_loss: 1.4044\n",
      "Epoch 3533/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.7915 - val_loss: 1.4513\n",
      "Epoch 3534/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.7417 - val_loss: 1.4438\n",
      "Epoch 3535/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7092 - val_loss: 1.2880\n",
      "Epoch 3536/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.7391 - val_loss: 1.2636\n",
      "Epoch 3537/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3804 - val_loss: 1.3197\n",
      "Epoch 3538/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.3714 - val_loss: 1.4156\n",
      "Epoch 3539/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5667 - val_loss: 1.4701\n",
      "Epoch 3540/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3901 - val_loss: 1.4011\n",
      "Epoch 3541/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6091 - val_loss: 1.3322\n",
      "Epoch 3542/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7121 - val_loss: 1.3972\n",
      "Epoch 3543/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4977 - val_loss: 1.2901\n",
      "Epoch 3544/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.0656 - val_loss: 1.4560\n",
      "Epoch 3545/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4534 - val_loss: 1.4401\n",
      "Epoch 3546/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8602 - val_loss: 1.3458\n",
      "Epoch 3547/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1734 - val_loss: 1.2240\n",
      "Epoch 3548/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9486 - val_loss: 1.3495\n",
      "Epoch 3549/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4733 - val_loss: 1.4069\n",
      "Epoch 3550/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6875 - val_loss: 1.4793\n",
      "Epoch 3551/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6707 - val_loss: 1.3851\n",
      "Epoch 3552/10000\n",
      "90/90 [==============================] - 0s 217us/step - loss: 9.5291 - val_loss: 1.2993\n",
      "Epoch 3553/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4761 - val_loss: 1.3199\n",
      "Epoch 3554/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2575 - val_loss: 1.3590\n",
      "Epoch 3555/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7080 - val_loss: 1.5416\n",
      "Epoch 3556/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5621 - val_loss: 1.4503\n",
      "Epoch 3557/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7892 - val_loss: 1.3434\n",
      "Epoch 3558/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9307 - val_loss: 1.3417\n",
      "Epoch 3559/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3125 - val_loss: 1.2410\n",
      "Epoch 3560/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6626 - val_loss: 1.3120\n",
      "Epoch 3561/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5695 - val_loss: 1.4373\n",
      "Epoch 3562/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5530 - val_loss: 1.4455\n",
      "Epoch 3563/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.3630 - val_loss: 1.4305\n",
      "Epoch 3564/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.4252 - val_loss: 1.3369\n",
      "Epoch 3565/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6313 - val_loss: 1.3623\n",
      "Epoch 3566/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7190 - val_loss: 1.4111\n",
      "Epoch 3567/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8352 - val_loss: 1.3873\n",
      "Epoch 3568/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7142 - val_loss: 1.3459\n",
      "Epoch 3569/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4222 - val_loss: 1.3239\n",
      "Epoch 3570/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.5393 - val_loss: 1.2683\n",
      "Epoch 3571/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7316 - val_loss: 1.3144\n",
      "Epoch 3572/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3424 - val_loss: 1.4346\n",
      "Epoch 3573/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7971 - val_loss: 1.4320\n",
      "Epoch 3574/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8574 - val_loss: 1.3691\n",
      "Epoch 3575/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3661 - val_loss: 1.3082\n",
      "Epoch 3576/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3650 - val_loss: 1.2423\n",
      "Epoch 3577/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4631 - val_loss: 1.3354\n",
      "Epoch 3578/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7031 - val_loss: 1.5625\n",
      "Epoch 3579/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4831 - val_loss: 1.4269\n",
      "Epoch 3580/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8227 - val_loss: 1.3428\n",
      "Epoch 3581/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7296 - val_loss: 1.4069\n",
      "Epoch 3582/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.1479 - val_loss: 1.3502\n",
      "Epoch 3583/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7018 - val_loss: 1.3635\n",
      "Epoch 3584/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7176 - val_loss: 1.3639\n",
      "Epoch 3585/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.6842 - val_loss: 1.3875\n",
      "Epoch 3586/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4751 - val_loss: 1.2862\n",
      "Epoch 3587/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.9837 - val_loss: 1.3207\n",
      "Epoch 3588/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5493 - val_loss: 1.3354\n",
      "Epoch 3589/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2949 - val_loss: 1.3184\n",
      "Epoch 3590/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2969 - val_loss: 1.2926\n",
      "Epoch 3591/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.8481 - val_loss: 1.4230\n",
      "Epoch 3592/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7976 - val_loss: 1.4361\n",
      "Epoch 3593/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3981 - val_loss: 1.3775\n",
      "Epoch 3594/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6428 - val_loss: 1.3383\n",
      "Epoch 3595/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3759 - val_loss: 1.3470\n",
      "Epoch 3596/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5824 - val_loss: 1.3182\n",
      "Epoch 3597/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4738 - val_loss: 1.3871\n",
      "Epoch 3598/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6703 - val_loss: 1.4862\n",
      "Epoch 3599/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6660 - val_loss: 1.4293\n",
      "Epoch 3600/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8231 - val_loss: 1.3494\n",
      "Epoch 3601/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.3859 - val_loss: 1.2639\n",
      "Epoch 3602/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5250 - val_loss: 1.3534\n",
      "Epoch 3603/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7322 - val_loss: 1.4375\n",
      "Epoch 3604/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4389 - val_loss: 1.3582\n",
      "Epoch 3605/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.6994 - val_loss: 1.2858\n",
      "Epoch 3606/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5470 - val_loss: 1.3551\n",
      "Epoch 3607/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.0977 - val_loss: 1.3143\n",
      "Epoch 3608/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3503 - val_loss: 1.2967\n",
      "Epoch 3609/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6219 - val_loss: 1.4288\n",
      "Epoch 3610/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.8161 - val_loss: 1.5850\n",
      "Epoch 3611/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.0763 - val_loss: 1.4645\n",
      "Epoch 3612/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8615 - val_loss: 1.2762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3613/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.2545 - val_loss: 1.3640\n",
      "Epoch 3614/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4010 - val_loss: 1.3102\n",
      "Epoch 3615/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3524 - val_loss: 1.3122\n",
      "Epoch 3616/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.4556 - val_loss: 1.2759\n",
      "Epoch 3617/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.2340 - val_loss: 1.3690\n",
      "Epoch 3618/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7955 - val_loss: 1.3627\n",
      "Epoch 3619/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3016 - val_loss: 1.3144\n",
      "Epoch 3620/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5392 - val_loss: 1.3628\n",
      "Epoch 3621/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5478 - val_loss: 1.3631\n",
      "Epoch 3622/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.9583 - val_loss: 1.4937\n",
      "Epoch 3623/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.3950 - val_loss: 1.5105\n",
      "Epoch 3624/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6721 - val_loss: 1.3206\n",
      "Epoch 3625/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.9314 - val_loss: 1.2627\n",
      "Epoch 3626/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.5943 - val_loss: 1.3097\n",
      "Epoch 3627/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.4973 - val_loss: 1.3519\n",
      "Epoch 3628/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7203 - val_loss: 1.4497\n",
      "Epoch 3629/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8509 - val_loss: 1.3549\n",
      "Epoch 3630/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3975 - val_loss: 1.2446\n",
      "Epoch 3631/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.8953 - val_loss: 1.2720\n",
      "Epoch 3632/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7555 - val_loss: 1.3752\n",
      "Epoch 3633/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.1707 - val_loss: 1.4988\n",
      "Epoch 3634/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8794 - val_loss: 1.4191\n",
      "Epoch 3635/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4996 - val_loss: 1.2726\n",
      "Epoch 3636/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 10.1997 - val_loss: 1.3477\n",
      "Epoch 3637/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7756 - val_loss: 1.2851\n",
      "Epoch 3638/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6148 - val_loss: 1.3852\n",
      "Epoch 3639/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5610 - val_loss: 1.2917\n",
      "Epoch 3640/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 10.1486 - val_loss: 1.4238\n",
      "Epoch 3641/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6644 - val_loss: 1.3680\n",
      "Epoch 3642/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7981 - val_loss: 1.3612\n",
      "Epoch 3643/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.2425 - val_loss: 1.2872\n",
      "Epoch 3644/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6109 - val_loss: 1.4065\n",
      "Epoch 3645/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4765 - val_loss: 1.5203\n",
      "Epoch 3646/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4680 - val_loss: 1.3494\n",
      "Epoch 3647/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2574 - val_loss: 1.2835\n",
      "Epoch 3648/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6397 - val_loss: 1.2239\n",
      "Epoch 3649/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.8026 - val_loss: 1.2934\n",
      "Epoch 3650/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.6237 - val_loss: 1.3850\n",
      "Epoch 3651/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4998 - val_loss: 1.4436\n",
      "Epoch 3652/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7485 - val_loss: 1.3805\n",
      "Epoch 3653/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4970 - val_loss: 1.2659\n",
      "Epoch 3654/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0100 - val_loss: 1.2583\n",
      "Epoch 3655/10000\n",
      "90/90 [==============================] - 0s 241us/step - loss: 9.9898 - val_loss: 1.4518\n",
      "Epoch 3656/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0970 - val_loss: 1.5223\n",
      "Epoch 3657/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6837 - val_loss: 1.3648\n",
      "Epoch 3658/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9299 - val_loss: 1.3021\n",
      "Epoch 3659/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6029 - val_loss: 1.2656\n",
      "Epoch 3660/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3529 - val_loss: 1.2190\n",
      "Epoch 3661/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7174 - val_loss: 1.4553\n",
      "Epoch 3662/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9821 - val_loss: 1.5402\n",
      "Epoch 3663/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6827 - val_loss: 1.2944\n",
      "Epoch 3664/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6572 - val_loss: 1.2104\n",
      "Epoch 3665/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5352 - val_loss: 1.2644\n",
      "Epoch 3666/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9690 - val_loss: 1.4426\n",
      "Epoch 3667/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8828 - val_loss: 1.6007\n",
      "Epoch 3668/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3685 - val_loss: 1.3982\n",
      "Epoch 3669/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5532 - val_loss: 1.2931\n",
      "Epoch 3670/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.3060 - val_loss: 1.3840\n",
      "Epoch 3671/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9103 - val_loss: 1.4725\n",
      "Epoch 3672/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4976 - val_loss: 1.3552\n",
      "Epoch 3673/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3524 - val_loss: 1.2692\n",
      "Epoch 3674/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8225 - val_loss: 1.1897\n",
      "Epoch 3675/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5334 - val_loss: 1.3199\n",
      "Epoch 3676/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3921 - val_loss: 1.3278\n",
      "Epoch 3677/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.9002 - val_loss: 1.3483\n",
      "Epoch 3678/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4957 - val_loss: 1.2709\n",
      "Epoch 3679/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7184 - val_loss: 1.3542\n",
      "Epoch 3680/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.0305 - val_loss: 1.3780\n",
      "Epoch 3681/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8346 - val_loss: 1.4430\n",
      "Epoch 3682/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6531 - val_loss: 1.3967\n",
      "Epoch 3683/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6563 - val_loss: 1.3038\n",
      "Epoch 3684/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4803 - val_loss: 1.2511\n",
      "Epoch 3685/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0603 - val_loss: 1.3407\n",
      "Epoch 3686/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7882 - val_loss: 1.5381\n",
      "Epoch 3687/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4121 - val_loss: 1.3011\n",
      "Epoch 3688/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5890 - val_loss: 1.2497\n",
      "Epoch 3689/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6440 - val_loss: 1.2847\n",
      "Epoch 3690/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.8092 - val_loss: 1.3386\n",
      "Epoch 3691/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4324 - val_loss: 1.3521\n",
      "Epoch 3692/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9453 - val_loss: 1.4369\n",
      "Epoch 3693/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5979 - val_loss: 1.4443\n",
      "Epoch 3694/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6408 - val_loss: 1.2647\n",
      "Epoch 3695/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6893 - val_loss: 1.2621\n",
      "Epoch 3696/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7512 - val_loss: 1.3838\n",
      "Epoch 3697/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.4805 - val_loss: 1.4401\n",
      "Epoch 3698/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5240 - val_loss: 1.3361\n",
      "Epoch 3699/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7721 - val_loss: 1.3384\n",
      "Epoch 3700/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8002 - val_loss: 1.3174\n",
      "Epoch 3701/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.2871 - val_loss: 1.3877\n",
      "Epoch 3702/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.0090 - val_loss: 1.2741\n",
      "Epoch 3703/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8049 - val_loss: 1.2304\n",
      "Epoch 3704/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3802 - val_loss: 1.2912\n",
      "Epoch 3705/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4803 - val_loss: 1.4656\n",
      "Epoch 3706/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9615 - val_loss: 1.5290\n",
      "Epoch 3707/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5893 - val_loss: 1.4043\n",
      "Epoch 3708/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8848 - val_loss: 1.3563\n",
      "Epoch 3709/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7995 - val_loss: 1.2353\n",
      "Epoch 3710/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7917 - val_loss: 1.2958\n",
      "Epoch 3711/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4056 - val_loss: 1.3134\n",
      "Epoch 3712/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4365 - val_loss: 1.3981\n",
      "Epoch 3713/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5408 - val_loss: 1.3538\n",
      "Epoch 3714/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.8125 - val_loss: 1.2678\n",
      "Epoch 3715/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7166 - val_loss: 1.2165\n",
      "Epoch 3716/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6580 - val_loss: 1.2486\n",
      "Epoch 3717/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0894 - val_loss: 1.3720\n",
      "Epoch 3718/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6517 - val_loss: 1.3659\n",
      "Epoch 3719/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 9.4624 - val_loss: 1.3447\n",
      "Epoch 3720/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2643 - val_loss: 1.2276\n",
      "Epoch 3721/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.8201 - val_loss: 1.3057\n",
      "Epoch 3722/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.7615 - val_loss: 1.4808\n",
      "Epoch 3723/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.8639 - val_loss: 1.5147\n",
      "Epoch 3724/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.6126 - val_loss: 1.3499\n",
      "Epoch 3725/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.5143 - val_loss: 1.2902\n",
      "Epoch 3726/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7518 - val_loss: 1.3267\n",
      "Epoch 3727/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7532 - val_loss: 1.5081\n",
      "Epoch 3728/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6247 - val_loss: 1.4568\n",
      "Epoch 3729/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0757 - val_loss: 1.4282\n",
      "Epoch 3730/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.6052 - val_loss: 1.2174\n",
      "Epoch 3731/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.7696 - val_loss: 1.2440\n",
      "Epoch 3732/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6679 - val_loss: 1.2877\n",
      "Epoch 3733/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.9086 - val_loss: 1.4019\n",
      "Epoch 3734/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4230 - val_loss: 1.3432\n",
      "Epoch 3735/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6220 - val_loss: 1.4027\n",
      "Epoch 3736/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.9140 - val_loss: 1.3945\n",
      "Epoch 3737/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.9120 - val_loss: 1.3251\n",
      "Epoch 3738/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.9257 - val_loss: 1.3617\n",
      "Epoch 3739/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5185 - val_loss: 1.3361\n",
      "Epoch 3740/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.5952 - val_loss: 1.2994\n",
      "Epoch 3741/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4062 - val_loss: 1.1964\n",
      "Epoch 3742/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7966 - val_loss: 1.2902\n",
      "Epoch 3743/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5277 - val_loss: 1.3300\n",
      "Epoch 3744/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3896 - val_loss: 1.3088\n",
      "Epoch 3745/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.1097 - val_loss: 1.2390\n",
      "Epoch 3746/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.5245 - val_loss: 1.3424\n",
      "Epoch 3747/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.1644 - val_loss: 1.4619\n",
      "Epoch 3748/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3307 - val_loss: 1.2885\n",
      "Epoch 3749/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2799 - val_loss: 1.2149\n",
      "Epoch 3750/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.7707 - val_loss: 1.3168\n",
      "Epoch 3751/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.6712 - val_loss: 1.3789\n",
      "Epoch 3752/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.6172 - val_loss: 1.4915\n",
      "Epoch 3753/10000\n",
      "90/90 [==============================] - 0s 225us/step - loss: 9.3240 - val_loss: 1.3345\n",
      "Epoch 3754/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.1544 - val_loss: 1.3165\n",
      "Epoch 3755/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6369 - val_loss: 1.2812\n",
      "Epoch 3756/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7972 - val_loss: 1.3458\n",
      "Epoch 3757/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5068 - val_loss: 1.3777\n",
      "Epoch 3758/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.4643 - val_loss: 1.3332\n",
      "Epoch 3759/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9435 - val_loss: 1.4230\n",
      "Epoch 3760/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7045 - val_loss: 1.3258\n",
      "Epoch 3761/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6553 - val_loss: 1.2556\n",
      "Epoch 3762/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7241 - val_loss: 1.2741\n",
      "Epoch 3763/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.7158 - val_loss: 1.2663\n",
      "Epoch 3764/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9313 - val_loss: 1.4256\n",
      "Epoch 3765/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.1120 - val_loss: 1.2933\n",
      "Epoch 3766/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.1210 - val_loss: 1.3692\n",
      "Epoch 3767/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 102us/step - loss: 9.8189 - val_loss: 1.3517\n",
      "Epoch 3768/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.5614 - val_loss: 1.2826\n",
      "Epoch 3769/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4361 - val_loss: 1.2897\n",
      "Epoch 3770/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7698 - val_loss: 1.3252\n",
      "Epoch 3771/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 10.2427 - val_loss: 1.4246\n",
      "Epoch 3772/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.7665 - val_loss: 1.4623\n",
      "Epoch 3773/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6899 - val_loss: 1.3615\n",
      "Epoch 3774/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.1042 - val_loss: 1.2868\n",
      "Epoch 3775/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3164 - val_loss: 1.2405\n",
      "Epoch 3776/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6220 - val_loss: 1.2957\n",
      "Epoch 3777/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5808 - val_loss: 1.4636\n",
      "Epoch 3778/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5786 - val_loss: 1.4015\n",
      "Epoch 3779/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6964 - val_loss: 1.3094\n",
      "Epoch 3780/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8281 - val_loss: 1.2746\n",
      "Epoch 3781/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5027 - val_loss: 1.1757\n",
      "Epoch 3782/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5037 - val_loss: 1.3041\n",
      "Epoch 3783/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5807 - val_loss: 1.4738\n",
      "Epoch 3784/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7595 - val_loss: 1.4274\n",
      "Epoch 3785/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7400 - val_loss: 1.3445\n",
      "Epoch 3786/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.6465 - val_loss: 1.2441\n",
      "Epoch 3787/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6385 - val_loss: 1.2952\n",
      "Epoch 3788/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9828 - val_loss: 1.3640\n",
      "Epoch 3789/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2371 - val_loss: 1.3085\n",
      "Epoch 3790/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8464 - val_loss: 1.2804\n",
      "Epoch 3791/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6774 - val_loss: 1.2947\n",
      "Epoch 3792/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3597 - val_loss: 1.2425\n",
      "Epoch 3793/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.0947 - val_loss: 1.3598\n",
      "Epoch 3794/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.0718 - val_loss: 1.4701\n",
      "Epoch 3795/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5001 - val_loss: 1.3520\n",
      "Epoch 3796/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8391 - val_loss: 1.3128\n",
      "Epoch 3797/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4002 - val_loss: 1.2662\n",
      "Epoch 3798/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7672 - val_loss: 1.2677\n",
      "Epoch 3799/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.591 - 0s 102us/step - loss: 9.6870 - val_loss: 1.2879\n",
      "Epoch 3800/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5219 - val_loss: 1.4259\n",
      "Epoch 3801/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3336 - val_loss: 1.3683\n",
      "Epoch 3802/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4381 - val_loss: 1.3467\n",
      "Epoch 3803/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7028 - val_loss: 1.2440\n",
      "Epoch 3804/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6169 - val_loss: 1.2185\n",
      "Epoch 3805/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2696 - val_loss: 1.3163\n",
      "Epoch 3806/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5604 - val_loss: 1.4266\n",
      "Epoch 3807/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7593 - val_loss: 1.5805\n",
      "Epoch 3808/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6189 - val_loss: 1.4937\n",
      "Epoch 3809/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5806 - val_loss: 1.2779\n",
      "Epoch 3810/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3011 - val_loss: 1.1950\n",
      "Epoch 3811/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.9090 - val_loss: 1.2695\n",
      "Epoch 3812/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6902 - val_loss: 1.3981\n",
      "Epoch 3813/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.7643 - val_loss: 1.3305\n",
      "Epoch 3814/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5962 - val_loss: 1.2239\n",
      "Epoch 3815/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.9073 - val_loss: 1.2481\n",
      "Epoch 3816/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.8047 - val_loss: 1.2310\n",
      "Epoch 3817/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4947 - val_loss: 1.3346\n",
      "Epoch 3818/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6684 - val_loss: 1.4078\n",
      "Epoch 3819/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5630 - val_loss: 1.4379\n",
      "Epoch 3820/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4343 - val_loss: 1.2661\n",
      "Epoch 3821/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7331 - val_loss: 1.2032\n",
      "Epoch 3822/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3722 - val_loss: 1.2385\n",
      "Epoch 3823/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6786 - val_loss: 1.4185\n",
      "Epoch 3824/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6038 - val_loss: 1.4417\n",
      "Epoch 3825/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6590 - val_loss: 1.3598\n",
      "Epoch 3826/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7777 - val_loss: 1.2835\n",
      "Epoch 3827/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6821 - val_loss: 1.3082\n",
      "Epoch 3828/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3176 - val_loss: 1.2301\n",
      "Epoch 3829/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.0805 - val_loss: 1.4658\n",
      "Epoch 3830/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9821 - val_loss: 1.4551\n",
      "Epoch 3831/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8005 - val_loss: 1.2799\n",
      "Epoch 3832/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6706 - val_loss: 1.2250\n",
      "Epoch 3833/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8721 - val_loss: 1.3339\n",
      "Epoch 3834/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8727 - val_loss: 1.4215\n",
      "Epoch 3835/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.9466 - val_loss: 1.3198\n",
      "Epoch 3836/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6658 - val_loss: 1.1865\n",
      "Epoch 3837/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9007 - val_loss: 1.2338\n",
      "Epoch 3838/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7799 - val_loss: 1.3843\n",
      "Epoch 3839/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.4370 - val_loss: 1.3704\n",
      "Epoch 3840/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.8292 - val_loss: 1.3876\n",
      "Epoch 3841/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6993 - val_loss: 1.3386\n",
      "Epoch 3842/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6910 - val_loss: 1.3085\n",
      "Epoch 3843/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7689 - val_loss: 1.3610\n",
      "Epoch 3844/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7009 - val_loss: 1.3636\n",
      "Epoch 3845/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.9916 - val_loss: 1.4412\n",
      "Epoch 3846/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.5224 - val_loss: 1.2148\n",
      "Epoch 3847/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9682 - val_loss: 1.2689\n",
      "Epoch 3848/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6143 - val_loss: 1.2051\n",
      "Epoch 3849/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7370 - val_loss: 1.3862\n",
      "Epoch 3850/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4857 - val_loss: 1.3752\n",
      "Epoch 3851/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.6726 - val_loss: 1.3594\n",
      "Epoch 3852/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8001 - val_loss: 1.3686\n",
      "Epoch 3853/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7596 - val_loss: 1.3467\n",
      "Epoch 3854/10000\n",
      "90/90 [==============================] - 0s 199us/step - loss: 9.7235 - val_loss: 1.2465\n",
      "Epoch 3855/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2899 - val_loss: 1.3017\n",
      "Epoch 3856/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4229 - val_loss: 1.3529\n",
      "Epoch 3857/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9400 - val_loss: 1.3736\n",
      "Epoch 3858/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9822 - val_loss: 1.3002\n",
      "Epoch 3859/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8109 - val_loss: 1.2469\n",
      "Epoch 3860/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7423 - val_loss: 1.1783\n",
      "Epoch 3861/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6655 - val_loss: 1.3849\n",
      "Epoch 3862/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.1664 - val_loss: 1.3724\n",
      "Epoch 3863/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.5535 - val_loss: 1.3531\n",
      "Epoch 3864/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6294 - val_loss: 1.3400\n",
      "Epoch 3865/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4367 - val_loss: 1.2899\n",
      "Epoch 3866/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4260 - val_loss: 1.3488\n",
      "Epoch 3867/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4088 - val_loss: 1.3465\n",
      "Epoch 3868/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.0212 - val_loss: 1.4854\n",
      "Epoch 3869/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3573 - val_loss: 1.3722\n",
      "Epoch 3870/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6383 - val_loss: 1.3761\n",
      "Epoch 3871/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6927 - val_loss: 1.3175\n",
      "Epoch 3872/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3867 - val_loss: 1.2046\n",
      "Epoch 3873/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7893 - val_loss: 1.3597\n",
      "Epoch 3874/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6213 - val_loss: 1.3185\n",
      "Epoch 3875/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9895 - val_loss: 1.3137\n",
      "Epoch 3876/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 10.0345 - val_loss: 1.3871\n",
      "Epoch 3877/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5345 - val_loss: 1.2658\n",
      "Epoch 3878/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 9.8138 - val_loss: 1.3195\n",
      "Epoch 3879/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2295 - val_loss: 1.1935\n",
      "Epoch 3880/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8375 - val_loss: 1.3175\n",
      "Epoch 3881/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6174 - val_loss: 1.4129\n",
      "Epoch 3882/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3308 - val_loss: 1.3607\n",
      "Epoch 3883/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4396 - val_loss: 1.3383\n",
      "Epoch 3884/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8921 - val_loss: 1.3479\n",
      "Epoch 3885/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5432 - val_loss: 1.3094\n",
      "Epoch 3886/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8646 - val_loss: 1.3233\n",
      "Epoch 3887/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1748 - val_loss: 1.2270\n",
      "Epoch 3888/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6295 - val_loss: 1.3066\n",
      "Epoch 3889/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8428 - val_loss: 1.2633\n",
      "Epoch 3890/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4973 - val_loss: 1.2909\n",
      "Epoch 3891/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6015 - val_loss: 1.2885\n",
      "Epoch 3892/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.6110 - val_loss: 1.3244\n",
      "Epoch 3893/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6480 - val_loss: 1.2958\n",
      "Epoch 3894/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4824 - val_loss: 1.1895\n",
      "Epoch 3895/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 10.0477 - val_loss: 1.4791\n",
      "Epoch 3896/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7873 - val_loss: 1.5153\n",
      "Epoch 3897/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6439 - val_loss: 1.3158\n",
      "Epoch 3898/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6670 - val_loss: 1.2300\n",
      "Epoch 3899/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7616 - val_loss: 1.2219\n",
      "Epoch 3900/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4316 - val_loss: 1.2384\n",
      "Epoch 3901/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7266 - val_loss: 1.3203\n",
      "Epoch 3902/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.8565 - val_loss: 1.3307\n",
      "Epoch 3903/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8889 - val_loss: 1.2839\n",
      "Epoch 3904/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7191 - val_loss: 1.2368\n",
      "Epoch 3905/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.7366 - val_loss: 1.2691\n",
      "Epoch 3906/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9062 - val_loss: 1.4191\n",
      "Epoch 3907/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6385 - val_loss: 1.4159\n",
      "Epoch 3908/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2908 - val_loss: 1.2649\n",
      "Epoch 3909/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5699 - val_loss: 1.2837\n",
      "Epoch 3910/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6019 - val_loss: 1.3723\n",
      "Epoch 3911/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5847 - val_loss: 1.2294\n",
      "Epoch 3912/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6534 - val_loss: 1.3355\n",
      "Epoch 3913/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.9058 - val_loss: 1.3794\n",
      "Epoch 3914/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5584 - val_loss: 1.3113\n",
      "Epoch 3915/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9009 - val_loss: 1.3425\n",
      "Epoch 3916/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8606 - val_loss: 1.2938\n",
      "Epoch 3917/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2707 - val_loss: 1.2052\n",
      "Epoch 3918/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7883 - val_loss: 1.3468\n",
      "Epoch 3919/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7018 - val_loss: 1.3986\n",
      "Epoch 3920/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6234 - val_loss: 1.3253\n",
      "Epoch 3921/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 97us/step - loss: 9.5806 - val_loss: 1.3152\n",
      "Epoch 3922/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3414 - val_loss: 1.2845\n",
      "Epoch 3923/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3696 - val_loss: 1.2457\n",
      "Epoch 3924/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.0997 - val_loss: 1.2654\n",
      "Epoch 3925/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8438 - val_loss: 1.4432\n",
      "Epoch 3926/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1849 - val_loss: 1.2822\n",
      "Epoch 3927/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1897 - val_loss: 1.2052\n",
      "Epoch 3928/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6507 - val_loss: 1.3185\n",
      "Epoch 3929/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7971 - val_loss: 1.4239\n",
      "Epoch 3930/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4753 - val_loss: 1.3335\n",
      "Epoch 3931/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.5426 - val_loss: 1.2096\n",
      "Epoch 3932/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4109 - val_loss: 1.2936\n",
      "Epoch 3933/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7193 - val_loss: 1.4001\n",
      "Epoch 3934/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5391 - val_loss: 1.4059\n",
      "Epoch 3935/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9602 - val_loss: 1.4182\n",
      "Epoch 3936/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6128 - val_loss: 1.2397\n",
      "Epoch 3937/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4069 - val_loss: 1.1608\n",
      "Epoch 3938/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3761 - val_loss: 1.4490\n",
      "Epoch 3939/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9566 - val_loss: 1.5731\n",
      "Epoch 3940/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8508 - val_loss: 1.4267\n",
      "Epoch 3941/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7516 - val_loss: 1.2609\n",
      "Epoch 3942/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9423 - val_loss: 1.2650\n",
      "Epoch 3943/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7988 - val_loss: 1.4217\n",
      "Epoch 3944/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4868 - val_loss: 1.3269\n",
      "Epoch 3945/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5557 - val_loss: 1.2838\n",
      "Epoch 3946/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7926 - val_loss: 1.2165\n",
      "Epoch 3947/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4602 - val_loss: 1.2826\n",
      "Epoch 3948/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8097 - val_loss: 1.3358\n",
      "Epoch 3949/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6311 - val_loss: 1.3145\n",
      "Epoch 3950/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7035 - val_loss: 1.2249\n",
      "Epoch 3951/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6603 - val_loss: 1.2506\n",
      "Epoch 3952/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 9.6629 - val_loss: 1.2882\n",
      "Epoch 3953/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 9.4030 - val_loss: 1.3672\n",
      "Epoch 3954/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.4126 - val_loss: 1.3447\n",
      "Epoch 3955/10000\n",
      "90/90 [==============================] - 0s 204us/step - loss: 9.8331 - val_loss: 1.3782\n",
      "Epoch 3956/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.3727 - val_loss: 1.3600\n",
      "Epoch 3957/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3803 - val_loss: 1.2566\n",
      "Epoch 3958/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8465 - val_loss: 1.3847\n",
      "Epoch 3959/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5876 - val_loss: 1.3956\n",
      "Epoch 3960/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4264 - val_loss: 1.2456\n",
      "Epoch 3961/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7568 - val_loss: 1.1672\n",
      "Epoch 3962/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8380 - val_loss: 1.3017\n",
      "Epoch 3963/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.1189 - val_loss: 1.4822\n",
      "Epoch 3964/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6147 - val_loss: 1.4133\n",
      "Epoch 3965/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5597 - val_loss: 1.3333\n",
      "Epoch 3966/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5409 - val_loss: 1.3060\n",
      "Epoch 3967/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6007 - val_loss: 1.2996\n",
      "Epoch 3968/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6187 - val_loss: 1.3708\n",
      "Epoch 3969/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5538 - val_loss: 1.1621\n",
      "Epoch 3970/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7079 - val_loss: 1.2272\n",
      "Epoch 3971/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.5953 - val_loss: 1.2496\n",
      "Epoch 3972/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.2473 - val_loss: 1.2539\n",
      "Epoch 3973/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4366 - val_loss: 1.3651\n",
      "Epoch 3974/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5418 - val_loss: 1.3807\n",
      "Epoch 3975/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5903 - val_loss: 1.4360\n",
      "Epoch 3976/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.9143 - val_loss: 1.3388\n",
      "Epoch 3977/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8201 - val_loss: 1.3413\n",
      "Epoch 3978/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8685 - val_loss: 1.2511\n",
      "Epoch 3979/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7205 - val_loss: 1.1467\n",
      "Epoch 3980/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7185 - val_loss: 1.1658\n",
      "Epoch 3981/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6054 - val_loss: 1.3310\n",
      "Epoch 3982/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 10.2103 - val_loss: 1.4988\n",
      "Epoch 3983/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4521 - val_loss: 1.2685\n",
      "Epoch 3984/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.3783 - val_loss: 1.1920\n",
      "Epoch 3985/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.5567 - val_loss: 1.2972\n",
      "Epoch 3986/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.7597 - val_loss: 1.4744\n",
      "Epoch 3987/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5045 - val_loss: 1.4855\n",
      "Epoch 3988/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8963 - val_loss: 1.4234\n",
      "Epoch 3989/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0136 - val_loss: 1.2415\n",
      "Epoch 3990/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9105 - val_loss: 1.2636\n",
      "Epoch 3991/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6600 - val_loss: 1.4031\n",
      "Epoch 3992/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.2649 - val_loss: 1.2913\n",
      "Epoch 3993/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6941 - val_loss: 1.2046\n",
      "Epoch 3994/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5723 - val_loss: 1.2202\n",
      "Epoch 3995/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5699 - val_loss: 1.2958\n",
      "Epoch 3996/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8199 - val_loss: 1.3653\n",
      "Epoch 3997/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3085 - val_loss: 1.2884\n",
      "Epoch 3998/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4205 - val_loss: 1.3035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3999/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7391 - val_loss: 1.3547\n",
      "Epoch 4000/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3565 - val_loss: 1.3252\n",
      "Epoch 4001/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7464 - val_loss: 1.3056\n",
      "Epoch 4002/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.0822 - val_loss: 1.3126\n",
      "Epoch 4003/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5252 - val_loss: 1.2588\n",
      "Epoch 4004/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8455 - val_loss: 1.2661\n",
      "Epoch 4005/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8100 - val_loss: 1.3185\n",
      "Epoch 4006/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8449 - val_loss: 1.3028\n",
      "Epoch 4007/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5640 - val_loss: 1.3652\n",
      "Epoch 4008/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3815 - val_loss: 1.2594\n",
      "Epoch 4009/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5199 - val_loss: 1.3211\n",
      "Epoch 4010/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.2115 - val_loss: 1.4027\n",
      "Epoch 4011/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2554 - val_loss: 1.3327\n",
      "Epoch 4012/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6759 - val_loss: 1.2082\n",
      "Epoch 4013/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9725 - val_loss: 1.3634\n",
      "Epoch 4014/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5655 - val_loss: 1.3520\n",
      "Epoch 4015/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4604 - val_loss: 1.2276\n",
      "Epoch 4016/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3162 - val_loss: 1.3226\n",
      "Epoch 4017/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7977 - val_loss: 1.3941\n",
      "Epoch 4018/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6237 - val_loss: 1.3181\n",
      "Epoch 4019/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7350 - val_loss: 1.2085\n",
      "Epoch 4020/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8134 - val_loss: 1.2716\n",
      "Epoch 4021/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7527 - val_loss: 1.3534\n",
      "Epoch 4022/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4653 - val_loss: 1.3557\n",
      "Epoch 4023/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2702 - val_loss: 1.3445\n",
      "Epoch 4024/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7177 - val_loss: 1.3524\n",
      "Epoch 4025/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5169 - val_loss: 1.3573\n",
      "Epoch 4026/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6879 - val_loss: 1.2986\n",
      "Epoch 4027/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7186 - val_loss: 1.3200\n",
      "Epoch 4028/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3301 - val_loss: 1.2427\n",
      "Epoch 4029/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6155 - val_loss: 1.2442\n",
      "Epoch 4030/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6291 - val_loss: 1.3393\n",
      "Epoch 4031/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 9.3083 - val_loss: 1.2290\n",
      "Epoch 4032/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4817 - val_loss: 1.2986\n",
      "Epoch 4033/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5905 - val_loss: 1.3709\n",
      "Epoch 4034/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6396 - val_loss: 1.3641\n",
      "Epoch 4035/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3742 - val_loss: 1.2211\n",
      "Epoch 4036/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8349 - val_loss: 1.3048\n",
      "Epoch 4037/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6956 - val_loss: 1.3627\n",
      "Epoch 4038/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.2390 - val_loss: 1.2310\n",
      "Epoch 4039/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.0242 - val_loss: 1.3642\n",
      "Epoch 4040/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7968 - val_loss: 1.3587\n",
      "Epoch 4041/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6513 - val_loss: 1.2367\n",
      "Epoch 4042/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9937 - val_loss: 1.3028\n",
      "Epoch 4043/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.5638 - val_loss: 1.3669\n",
      "Epoch 4044/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8458 - val_loss: 1.3819\n",
      "Epoch 4045/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8532 - val_loss: 1.3862\n",
      "Epoch 4046/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5351 - val_loss: 1.3381\n",
      "Epoch 4047/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6447 - val_loss: 1.3141\n",
      "Epoch 4048/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6335 - val_loss: 1.2485\n",
      "Epoch 4049/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8430 - val_loss: 1.2151\n",
      "Epoch 4050/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4305 - val_loss: 1.2462\n",
      "Epoch 4051/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4589 - val_loss: 1.2511\n",
      "Epoch 4052/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3020 - val_loss: 1.2870\n",
      "Epoch 4053/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4410 - val_loss: 1.3620\n",
      "Epoch 4054/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5237 - val_loss: 1.2731\n",
      "Epoch 4055/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8734 - val_loss: 1.3441\n",
      "Epoch 4056/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6146 - val_loss: 1.3616\n",
      "Epoch 4057/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2888 - val_loss: 1.3503\n",
      "Epoch 4058/10000\n",
      "90/90 [==============================] - 0s 241us/step - loss: 9.2425 - val_loss: 1.2094\n",
      "Epoch 4059/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.4498 - val_loss: 1.2276\n",
      "Epoch 4060/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.1104 - val_loss: 1.5465\n",
      "Epoch 4061/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6792 - val_loss: 1.4042\n",
      "Epoch 4062/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4755 - val_loss: 1.2429\n",
      "Epoch 4063/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8399 - val_loss: 1.3439\n",
      "Epoch 4064/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.2308 - val_loss: 1.2848\n",
      "Epoch 4065/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.6050 - val_loss: 1.2782\n",
      "Epoch 4066/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6212 - val_loss: 1.4154\n",
      "Epoch 4067/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4760 - val_loss: 1.3930\n",
      "Epoch 4068/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.3913 - val_loss: 1.3000\n",
      "Epoch 4069/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6328 - val_loss: 1.2529\n",
      "Epoch 4070/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6053 - val_loss: 1.2332\n",
      "Epoch 4071/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4213 - val_loss: 1.3448\n",
      "Epoch 4072/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7741 - val_loss: 1.4341\n",
      "Epoch 4073/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9378 - val_loss: 1.2857\n",
      "Epoch 4074/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8524 - val_loss: 1.1714\n",
      "Epoch 4075/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8694 - val_loss: 1.1604\n",
      "Epoch 4076/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.8262 - val_loss: 1.3117\n",
      "Epoch 4077/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5668 - val_loss: 1.2486\n",
      "Epoch 4078/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5955 - val_loss: 1.2602\n",
      "Epoch 4079/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.3071 - val_loss: 1.4072\n",
      "Epoch 4080/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.8365 - val_loss: 1.4637\n",
      "Epoch 4081/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5854 - val_loss: 1.3693\n",
      "Epoch 4082/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6243 - val_loss: 1.2507\n",
      "Epoch 4083/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.3054 - val_loss: 1.1403\n",
      "Epoch 4084/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.2608 - val_loss: 1.2024\n",
      "Epoch 4085/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5317 - val_loss: 1.3273\n",
      "Epoch 4086/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7351 - val_loss: 1.4791\n",
      "Epoch 4087/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5372 - val_loss: 1.4291\n",
      "Epoch 4088/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.3568 - val_loss: 1.2169\n",
      "Epoch 4089/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4518 - val_loss: 1.0966\n",
      "Epoch 4090/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9074 - val_loss: 1.3116\n",
      "Epoch 4091/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 9.2765 - val_loss: 1.4027\n",
      "Epoch 4092/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6371 - val_loss: 1.3890\n",
      "Epoch 4093/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.8654 - val_loss: 1.3792\n",
      "Epoch 4094/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5911 - val_loss: 1.3120\n",
      "Epoch 4095/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.4798 - val_loss: 1.2013\n",
      "Epoch 4096/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7752 - val_loss: 1.3602\n",
      "Epoch 4097/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6647 - val_loss: 1.5005\n",
      "Epoch 4098/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.2334 - val_loss: 1.4573\n",
      "Epoch 4099/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4279 - val_loss: 1.1072\n",
      "Epoch 4100/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9438 - val_loss: 1.1412\n",
      "Epoch 4101/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7150 - val_loss: 1.3061\n",
      "Epoch 4102/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6963 - val_loss: 1.3648\n",
      "Epoch 4103/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6760 - val_loss: 1.4172\n",
      "Epoch 4104/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.3735 - val_loss: 1.2953\n",
      "Epoch 4105/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6163 - val_loss: 1.2012\n",
      "Epoch 4106/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1264 - val_loss: 1.2103\n",
      "Epoch 4107/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6042 - val_loss: 1.3317\n",
      "Epoch 4108/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0102 - val_loss: 1.4359\n",
      "Epoch 4109/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7348 - val_loss: 1.2485\n",
      "Epoch 4110/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5572 - val_loss: 1.1969\n",
      "Epoch 4111/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6265 - val_loss: 1.2003\n",
      "Epoch 4112/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8820 - val_loss: 1.3145\n",
      "Epoch 4113/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7233 - val_loss: 1.4097\n",
      "Epoch 4114/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 10.0298 - val_loss: 1.4006\n",
      "Epoch 4115/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6950 - val_loss: 1.2177\n",
      "Epoch 4116/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6093 - val_loss: 1.1381\n",
      "Epoch 4117/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9100 - val_loss: 1.2332\n",
      "Epoch 4118/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0322 - val_loss: 1.4360\n",
      "Epoch 4119/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6582 - val_loss: 1.3916\n",
      "Epoch 4120/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.1080 - val_loss: 1.1321\n",
      "Epoch 4121/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6977 - val_loss: 1.1217\n",
      "Epoch 4122/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6067 - val_loss: 1.2443\n",
      "Epoch 4123/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6577 - val_loss: 1.3730\n",
      "Epoch 4124/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6988 - val_loss: 1.2966\n",
      "Epoch 4125/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4692 - val_loss: 1.1929\n",
      "Epoch 4126/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6118 - val_loss: 1.2229\n",
      "Epoch 4127/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6622 - val_loss: 1.3249\n",
      "Epoch 4128/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4846 - val_loss: 1.3070\n",
      "Epoch 4129/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3902 - val_loss: 1.3016\n",
      "Epoch 4130/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4881 - val_loss: 1.2801\n",
      "Epoch 4131/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8896 - val_loss: 1.3612\n",
      "Epoch 4132/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4046 - val_loss: 1.3368\n",
      "Epoch 4133/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2818 - val_loss: 1.3156\n",
      "Epoch 4134/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.5202 - val_loss: 1.3132\n",
      "Epoch 4135/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6514 - val_loss: 1.3156\n",
      "Epoch 4136/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4787 - val_loss: 1.1764\n",
      "Epoch 4137/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3304 - val_loss: 1.0737\n",
      "Epoch 4138/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8509 - val_loss: 1.3454\n",
      "Epoch 4139/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7280 - val_loss: 1.4533\n",
      "Epoch 4140/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5915 - val_loss: 1.3565\n",
      "Epoch 4141/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4596 - val_loss: 1.2833\n",
      "Epoch 4142/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8941 - val_loss: 1.2967\n",
      "Epoch 4143/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.9435 - val_loss: 1.3826\n",
      "Epoch 4144/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4597 - val_loss: 1.2627\n",
      "Epoch 4145/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6637 - val_loss: 1.2662\n",
      "Epoch 4146/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9146 - val_loss: 1.3324\n",
      "Epoch 4147/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9608 - val_loss: 1.2642\n",
      "Epoch 4148/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5480 - val_loss: 1.2750\n",
      "Epoch 4149/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5319 - val_loss: 1.2431\n",
      "Epoch 4150/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2708 - val_loss: 1.3357\n",
      "Epoch 4151/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8923 - val_loss: 1.4412\n",
      "Epoch 4152/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6905 - val_loss: 1.3376\n",
      "Epoch 4153/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4335 - val_loss: 1.2069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4154/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3375 - val_loss: 1.2256\n",
      "Epoch 4155/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9028 - val_loss: 1.3278\n",
      "Epoch 4156/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9146 - val_loss: 1.3958\n",
      "Epoch 4157/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5317 - val_loss: 1.2394\n",
      "Epoch 4158/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.0051 - val_loss: 1.3160\n",
      "Epoch 4159/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6130 - val_loss: 1.1998\n",
      "Epoch 4160/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9020 - val_loss: 1.2749\n",
      "Epoch 4161/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 9.4460 - val_loss: 1.3745\n",
      "Epoch 4162/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.7929 - val_loss: 1.3218\n",
      "Epoch 4163/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.6168 - val_loss: 1.2877\n",
      "Epoch 4164/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5835 - val_loss: 1.1136\n",
      "Epoch 4165/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6053 - val_loss: 1.2154\n",
      "Epoch 4166/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6033 - val_loss: 1.3399\n",
      "Epoch 4167/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4978 - val_loss: 1.3080\n",
      "Epoch 4168/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 9.3887 - val_loss: 1.3822\n",
      "Epoch 4169/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6935 - val_loss: 1.3035\n",
      "Epoch 4170/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3126 - val_loss: 1.1789\n",
      "Epoch 4171/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.8437 - val_loss: 1.3459\n",
      "Epoch 4172/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7720 - val_loss: 1.3622\n",
      "Epoch 4173/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.0269 - val_loss: 1.2911\n",
      "Epoch 4174/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7208 - val_loss: 1.2992\n",
      "Epoch 4175/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.8573 - val_loss: 1.3129\n",
      "Epoch 4176/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5926 - val_loss: 1.2804\n",
      "Epoch 4177/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4033 - val_loss: 1.2577\n",
      "Epoch 4178/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7872 - val_loss: 1.3624\n",
      "Epoch 4179/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8628 - val_loss: 1.3635\n",
      "Epoch 4180/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.9650 - val_loss: 1.2776\n",
      "Epoch 4181/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4611 - val_loss: 1.1952\n",
      "Epoch 4182/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3804 - val_loss: 1.2134\n",
      "Epoch 4183/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3333 - val_loss: 1.3498\n",
      "Epoch 4184/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1339 - val_loss: 1.2735\n",
      "Epoch 4185/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5287 - val_loss: 1.2571\n",
      "Epoch 4186/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3658 - val_loss: 1.3532\n",
      "Epoch 4187/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.9517 - val_loss: 1.3058\n",
      "Epoch 4188/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4284 - val_loss: 1.2905\n",
      "Epoch 4189/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.6867 - val_loss: 1.2661\n",
      "Epoch 4190/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6410 - val_loss: 1.2781\n",
      "Epoch 4191/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.5704 - val_loss: 1.3236\n",
      "Epoch 4192/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7025 - val_loss: 1.3518\n",
      "Epoch 4193/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7957 - val_loss: 1.3918\n",
      "Epoch 4194/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6109 - val_loss: 1.3019\n",
      "Epoch 4195/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3826 - val_loss: 1.2216\n",
      "Epoch 4196/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7538 - val_loss: 1.2128\n",
      "Epoch 4197/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4583 - val_loss: 1.2646\n",
      "Epoch 4198/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9545 - val_loss: 1.3635\n",
      "Epoch 4199/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.5548 - val_loss: 1.3429\n",
      "Epoch 4200/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6722 - val_loss: 1.1369\n",
      "Epoch 4201/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7587 - val_loss: 1.1936\n",
      "Epoch 4202/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8133 - val_loss: 1.2754\n",
      "Epoch 4203/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4433 - val_loss: 1.3302\n",
      "Epoch 4204/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5155 - val_loss: 1.3253\n",
      "Epoch 4205/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4148 - val_loss: 1.2953\n",
      "Epoch 4206/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9002 - val_loss: 1.3083\n",
      "Epoch 4207/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4201 - val_loss: 1.3026\n",
      "Epoch 4208/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4857 - val_loss: 1.3385\n",
      "Epoch 4209/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.3932 - val_loss: 1.2600\n",
      "Epoch 4210/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.9857 - val_loss: 1.3624\n",
      "Epoch 4211/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2695 - val_loss: 1.2450\n",
      "Epoch 4212/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7957 - val_loss: 1.2088\n",
      "Epoch 4213/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.3898 - val_loss: 1.2289\n",
      "Epoch 4214/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7137 - val_loss: 1.4500\n",
      "Epoch 4215/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2587 - val_loss: 1.3486\n",
      "Epoch 4216/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5184 - val_loss: 1.2791\n",
      "Epoch 4217/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4798 - val_loss: 1.2852\n",
      "Epoch 4218/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4509 - val_loss: 1.2151\n",
      "Epoch 4219/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.9804 - val_loss: 1.3160\n",
      "Epoch 4220/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7795 - val_loss: 1.4050\n",
      "Epoch 4221/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6718 - val_loss: 1.3994\n",
      "Epoch 4222/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7640 - val_loss: 1.2879\n",
      "Epoch 4223/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.3669 - val_loss: 1.1282\n",
      "Epoch 4224/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6950 - val_loss: 1.3070\n",
      "Epoch 4225/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3973 - val_loss: 1.3068\n",
      "Epoch 4226/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4800 - val_loss: 1.3317\n",
      "Epoch 4227/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1181 - val_loss: 1.2734\n",
      "Epoch 4228/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6675 - val_loss: 1.3039\n",
      "Epoch 4229/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8601 - val_loss: 1.3276\n",
      "Epoch 4230/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 9.4324 - val_loss: 1.1905\n",
      "Epoch 4231/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8476 - val_loss: 1.2271\n",
      "Epoch 4232/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3731 - val_loss: 1.2527\n",
      "Epoch 4233/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4004 - val_loss: 1.2568\n",
      "Epoch 4234/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1435 - val_loss: 1.1992\n",
      "Epoch 4235/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0084 - val_loss: 1.2280\n",
      "Epoch 4236/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4216 - val_loss: 1.3908\n",
      "Epoch 4237/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4943 - val_loss: 1.4066\n",
      "Epoch 4238/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5489 - val_loss: 1.3847\n",
      "Epoch 4239/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6817 - val_loss: 1.3821\n",
      "Epoch 4240/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2180 - val_loss: 1.2740\n",
      "Epoch 4241/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6204 - val_loss: 1.2777\n",
      "Epoch 4242/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5068 - val_loss: 1.2958\n",
      "Epoch 4243/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3568 - val_loss: 1.2198\n",
      "Epoch 4244/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6578 - val_loss: 1.3097\n",
      "Epoch 4245/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6451 - val_loss: 1.3397\n",
      "Epoch 4246/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5772 - val_loss: 1.2315\n",
      "Epoch 4247/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.8698 - val_loss: 1.2869\n",
      "Epoch 4248/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5547 - val_loss: 1.2171\n",
      "Epoch 4249/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7096 - val_loss: 1.3574\n",
      "Epoch 4250/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 10.1988 - val_loss: 1.5543\n",
      "Epoch 4251/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7390 - val_loss: 1.3695\n",
      "Epoch 4252/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.3590 - val_loss: 1.2446\n",
      "Epoch 4253/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5196 - val_loss: 1.1407\n",
      "Epoch 4254/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5795 - val_loss: 1.2279\n",
      "Epoch 4255/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4254 - val_loss: 1.3410\n",
      "Epoch 4256/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5619 - val_loss: 1.3530\n",
      "Epoch 4257/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7604 - val_loss: 1.2845\n",
      "Epoch 4258/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8796 - val_loss: 1.3362\n",
      "Epoch 4259/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0087 - val_loss: 1.2766\n",
      "Epoch 4260/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5947 - val_loss: 1.2390\n",
      "Epoch 4261/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6471 - val_loss: 1.2766\n",
      "Epoch 4262/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4905 - val_loss: 1.3310\n",
      "Epoch 4263/10000\n",
      "90/90 [==============================] - 0s 230us/step - loss: 9.5388 - val_loss: 1.2217\n",
      "Epoch 4264/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6253 - val_loss: 1.1746\n",
      "Epoch 4265/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5862 - val_loss: 1.1701\n",
      "Epoch 4266/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5885 - val_loss: 1.3008\n",
      "Epoch 4267/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6569 - val_loss: 1.3703\n",
      "Epoch 4268/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.2131 - val_loss: 1.2846\n",
      "Epoch 4269/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.9693 - val_loss: 1.3339\n",
      "Epoch 4270/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3006 - val_loss: 1.2008\n",
      "Epoch 4271/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4693 - val_loss: 1.2445\n",
      "Epoch 4272/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4002 - val_loss: 1.3195\n",
      "Epoch 4273/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.6636 - val_loss: 1.3337\n",
      "Epoch 4274/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7358 - val_loss: 1.3345\n",
      "Epoch 4275/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4711 - val_loss: 1.2866\n",
      "Epoch 4276/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4195 - val_loss: 1.3500\n",
      "Epoch 4277/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7909 - val_loss: 1.3244\n",
      "Epoch 4278/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.2930 - val_loss: 1.2879\n",
      "Epoch 4279/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6158 - val_loss: 1.2804\n",
      "Epoch 4280/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3308 - val_loss: 1.2529\n",
      "Epoch 4281/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7298 - val_loss: 1.3008\n",
      "Epoch 4282/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3792 - val_loss: 1.2779\n",
      "Epoch 4283/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8830 - val_loss: 1.3398\n",
      "Epoch 4284/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4656 - val_loss: 1.1780\n",
      "Epoch 4285/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0912 - val_loss: 1.1472\n",
      "Epoch 4286/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7800 - val_loss: 1.2616\n",
      "Epoch 4287/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5375 - val_loss: 1.3165\n",
      "Epoch 4288/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5274 - val_loss: 1.3158\n",
      "Epoch 4289/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2950 - val_loss: 1.2966\n",
      "Epoch 4290/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.6829 - val_loss: 1.2608\n",
      "Epoch 4291/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6710 - val_loss: 1.4404\n",
      "Epoch 4292/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.4631 - val_loss: 1.3281\n",
      "Epoch 4293/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8448 - val_loss: 1.3323\n",
      "Epoch 4294/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5778 - val_loss: 1.2919\n",
      "Epoch 4295/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5276 - val_loss: 1.1416\n",
      "Epoch 4296/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7429 - val_loss: 1.2030\n",
      "Epoch 4297/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3629 - val_loss: 1.1998\n",
      "Epoch 4298/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3932 - val_loss: 1.2573\n",
      "Epoch 4299/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3282 - val_loss: 1.3669\n",
      "Epoch 4300/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8462 - val_loss: 1.4580\n",
      "Epoch 4301/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2558 - val_loss: 1.3465\n",
      "Epoch 4302/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6230 - val_loss: 1.3398\n",
      "Epoch 4303/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6175 - val_loss: 1.2601\n",
      "Epoch 4304/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0100 - val_loss: 1.2027\n",
      "Epoch 4305/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4828 - val_loss: 1.3394\n",
      "Epoch 4306/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6828 - val_loss: 1.4308\n",
      "Epoch 4307/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5295 - val_loss: 1.3139\n",
      "Epoch 4308/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6291 - val_loss: 1.1197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4309/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 10.1557 - val_loss: 1.2446\n",
      "Epoch 4310/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.1115 - val_loss: 1.2551\n",
      "Epoch 4311/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.6989 - val_loss: 1.4241\n",
      "Epoch 4312/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 10.0632 - val_loss: 1.4807\n",
      "Epoch 4313/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.5554 - val_loss: 1.3348\n",
      "Epoch 4314/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6948 - val_loss: 1.2405\n",
      "Epoch 4315/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.5082 - val_loss: 1.2504\n",
      "Epoch 4316/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7862 - val_loss: 1.3894\n",
      "Epoch 4317/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4514 - val_loss: 1.3016\n",
      "Epoch 4318/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3900 - val_loss: 1.2204\n",
      "Epoch 4319/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6328 - val_loss: 1.2155\n",
      "Epoch 4320/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2193 - val_loss: 1.2042\n",
      "Epoch 4321/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7810 - val_loss: 1.2520\n",
      "Epoch 4322/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5091 - val_loss: 1.3405\n",
      "Epoch 4323/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9795 - val_loss: 1.3529\n",
      "Epoch 4324/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5572 - val_loss: 1.1892\n",
      "Epoch 4325/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6280 - val_loss: 1.2184\n",
      "Epoch 4326/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4276 - val_loss: 1.3247\n",
      "Epoch 4327/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3477 - val_loss: 1.3283\n",
      "Epoch 4328/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9007 - val_loss: 1.3129\n",
      "Epoch 4329/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5239 - val_loss: 1.2709\n",
      "Epoch 4330/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9025 - val_loss: 1.3763\n",
      "Epoch 4331/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8652 - val_loss: 1.3932\n",
      "Epoch 4332/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.2680 - val_loss: 1.3150\n",
      "Epoch 4333/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5298 - val_loss: 1.2654\n",
      "Epoch 4334/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7491 - val_loss: 1.2223\n",
      "Epoch 4335/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.4024 - val_loss: 1.1934\n",
      "Epoch 4336/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6684 - val_loss: 1.2739\n",
      "Epoch 4337/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4987 - val_loss: 1.3220\n",
      "Epoch 4338/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8432 - val_loss: 1.2768\n",
      "Epoch 4339/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.6130 - val_loss: 1.2755\n",
      "Epoch 4340/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4270 - val_loss: 1.1201\n",
      "Epoch 4341/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5872 - val_loss: 1.3838\n",
      "Epoch 4342/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.1498 - val_loss: 1.4183\n",
      "Epoch 4343/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8264 - val_loss: 1.4540\n",
      "Epoch 4344/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5010 - val_loss: 1.1881\n",
      "Epoch 4345/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5654 - val_loss: 1.2247\n",
      "Epoch 4346/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.1732 - val_loss: 1.3572\n",
      "Epoch 4347/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7781 - val_loss: 1.3742\n",
      "Epoch 4348/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6847 - val_loss: 1.3291\n",
      "Epoch 4349/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4965 - val_loss: 1.2830\n",
      "Epoch 4350/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0931 - val_loss: 1.2863\n",
      "Epoch 4351/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4119 - val_loss: 1.2408\n",
      "Epoch 4352/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.5131 - val_loss: 1.2589\n",
      "Epoch 4353/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.2664 - val_loss: 1.2236\n",
      "Epoch 4354/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5884 - val_loss: 1.2995\n",
      "Epoch 4355/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.4771 - val_loss: 1.3029\n",
      "Epoch 4356/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2344 - val_loss: 1.1960\n",
      "Epoch 4357/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.5390 - val_loss: 1.3062\n",
      "Epoch 4358/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7208 - val_loss: 1.3217\n",
      "Epoch 4359/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5195 - val_loss: 1.3227\n",
      "Epoch 4360/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6587 - val_loss: 1.2245\n",
      "Epoch 4361/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3502 - val_loss: 1.3622\n",
      "Epoch 4362/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7290 - val_loss: 1.3930\n",
      "Epoch 4363/10000\n",
      "90/90 [==============================] - 0s 212us/step - loss: 9.4466 - val_loss: 1.2721\n",
      "Epoch 4364/10000\n",
      "90/90 [==============================] - 0s 191us/step - loss: 10.1489 - val_loss: 1.2767\n",
      "Epoch 4365/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2607 - val_loss: 1.1814\n",
      "Epoch 4366/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6650 - val_loss: 1.2886\n",
      "Epoch 4367/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7465 - val_loss: 1.2886\n",
      "Epoch 4368/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7736 - val_loss: 1.3294\n",
      "Epoch 4369/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5418 - val_loss: 1.2614\n",
      "Epoch 4370/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4124 - val_loss: 1.2490\n",
      "Epoch 4371/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.3187 - val_loss: 1.2007\n",
      "Epoch 4372/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7909 - val_loss: 1.4275\n",
      "Epoch 4373/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7709 - val_loss: 1.5082\n",
      "Epoch 4374/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 9.5899 - val_loss: 1.3801\n",
      "Epoch 4375/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7812 - val_loss: 1.2306\n",
      "Epoch 4376/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8410 - val_loss: 1.1961\n",
      "Epoch 4377/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8513 - val_loss: 1.2399\n",
      "Epoch 4378/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6093 - val_loss: 1.2914\n",
      "Epoch 4379/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1246 - val_loss: 1.2450\n",
      "Epoch 4380/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5104 - val_loss: 1.2180\n",
      "Epoch 4381/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.6688 - val_loss: 1.2442\n",
      "Epoch 4382/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.3481 - val_loss: 1.2357\n",
      "Epoch 4383/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.7522 - val_loss: 1.1827\n",
      "Epoch 4384/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6617 - val_loss: 1.4366\n",
      "Epoch 4385/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1933 - val_loss: 1.3720\n",
      "Epoch 4386/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2939 - val_loss: 1.2642\n",
      "Epoch 4387/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.3536 - val_loss: 1.3692\n",
      "Epoch 4388/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3687 - val_loss: 1.4006\n",
      "Epoch 4389/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5450 - val_loss: 1.3693\n",
      "Epoch 4390/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5327 - val_loss: 1.3400\n",
      "Epoch 4391/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5553 - val_loss: 1.2100\n",
      "Epoch 4392/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.8180 - val_loss: 1.2401\n",
      "Epoch 4393/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6466 - val_loss: 1.2585\n",
      "Epoch 4394/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6435 - val_loss: 1.2561\n",
      "Epoch 4395/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.0897 - val_loss: 1.2509\n",
      "Epoch 4396/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3838 - val_loss: 1.2213\n",
      "Epoch 4397/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8705 - val_loss: 1.3779\n",
      "Epoch 4398/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7987 - val_loss: 1.3662\n",
      "Epoch 4399/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8604 - val_loss: 1.2772\n",
      "Epoch 4400/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5333 - val_loss: 1.1548\n",
      "Epoch 4401/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7622 - val_loss: 1.2304\n",
      "Epoch 4402/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5434 - val_loss: 1.3717\n",
      "Epoch 4403/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4482 - val_loss: 1.3117\n",
      "Epoch 4404/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5002 - val_loss: 1.3013\n",
      "Epoch 4405/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9527 - val_loss: 1.3366\n",
      "Epoch 4406/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5110 - val_loss: 1.3248\n",
      "Epoch 4407/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9517 - val_loss: 1.3415\n",
      "Epoch 4408/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4484 - val_loss: 1.2469\n",
      "Epoch 4409/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5758 - val_loss: 1.1797\n",
      "Epoch 4410/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5668 - val_loss: 1.2534\n",
      "Epoch 4411/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3395 - val_loss: 1.3371\n",
      "Epoch 4412/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4837 - val_loss: 1.3028\n",
      "Epoch 4413/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3035 - val_loss: 1.2622\n",
      "Epoch 4414/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4369 - val_loss: 1.2584\n",
      "Epoch 4415/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8845 - val_loss: 1.2675\n",
      "Epoch 4416/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4577 - val_loss: 1.2353\n",
      "Epoch 4417/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5177 - val_loss: 1.3415\n",
      "Epoch 4418/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4467 - val_loss: 1.3510\n",
      "Epoch 4419/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.9120 - val_loss: 1.3411\n",
      "Epoch 4420/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1352 - val_loss: 1.1875\n",
      "Epoch 4421/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.9411 - val_loss: 1.3826\n",
      "Epoch 4422/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.3001 - val_loss: 1.3831\n",
      "Epoch 4423/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9605 - val_loss: 1.3824\n",
      "Epoch 4424/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6475 - val_loss: 1.3194\n",
      "Epoch 4425/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5827 - val_loss: 1.2566\n",
      "Epoch 4426/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5823 - val_loss: 1.1882\n",
      "Epoch 4427/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2662 - val_loss: 1.2736\n",
      "Epoch 4428/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5566 - val_loss: 1.3340\n",
      "Epoch 4429/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.7406 - val_loss: 1.3447\n",
      "Epoch 4430/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4411 - val_loss: 1.2523\n",
      "Epoch 4431/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4607 - val_loss: 1.2262\n",
      "Epoch 4432/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8331 - val_loss: 1.2673\n",
      "Epoch 4433/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7427 - val_loss: 1.3301\n",
      "Epoch 4434/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3153 - val_loss: 1.3497\n",
      "Epoch 4435/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5419 - val_loss: 1.2632\n",
      "Epoch 4436/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5907 - val_loss: 1.2800\n",
      "Epoch 4437/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5059 - val_loss: 1.2170\n",
      "Epoch 4438/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1760 - val_loss: 1.1984\n",
      "Epoch 4439/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8948 - val_loss: 1.3723\n",
      "Epoch 4440/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5373 - val_loss: 1.3526\n",
      "Epoch 4441/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3967 - val_loss: 1.2421\n",
      "Epoch 4442/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6341 - val_loss: 1.1592\n",
      "Epoch 4443/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3058 - val_loss: 1.2341\n",
      "Epoch 4444/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3831 - val_loss: 1.4465\n",
      "Epoch 4445/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6544 - val_loss: 1.4188\n",
      "Epoch 4446/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4174 - val_loss: 1.2014\n",
      "Epoch 4447/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3846 - val_loss: 1.1676\n",
      "Epoch 4448/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5745 - val_loss: 1.2939\n",
      "Epoch 4449/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5568 - val_loss: 1.3919\n",
      "Epoch 4450/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8531 - val_loss: 1.3468\n",
      "Epoch 4451/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6222 - val_loss: 1.2329\n",
      "Epoch 4452/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6650 - val_loss: 1.2877\n",
      "Epoch 4453/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6403 - val_loss: 1.2548\n",
      "Epoch 4454/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4255 - val_loss: 1.1221\n",
      "Epoch 4455/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.5925 - val_loss: 1.2505\n",
      "Epoch 4456/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.9058 - val_loss: 1.4466\n",
      "Epoch 4457/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6031 - val_loss: 1.4444\n",
      "Epoch 4458/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.2645 - val_loss: 1.2544\n",
      "Epoch 4459/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6968 - val_loss: 1.1714\n",
      "Epoch 4460/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7153 - val_loss: 1.3922\n",
      "Epoch 4461/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7730 - val_loss: 1.5100\n",
      "Epoch 4462/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4479 - val_loss: 1.3025\n",
      "Epoch 4463/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.4802 - val_loss: 1.1828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4464/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6246 - val_loss: 1.1656\n",
      "Epoch 4465/10000\n",
      "90/90 [==============================] - 0s 231us/step - loss: 9.3651 - val_loss: 1.1846\n",
      "Epoch 4466/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3590 - val_loss: 1.2612\n",
      "Epoch 4467/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6475 - val_loss: 1.3202\n",
      "Epoch 4468/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.9143 - val_loss: 1.3480\n",
      "Epoch 4469/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5564 - val_loss: 1.2802\n",
      "Epoch 4470/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4642 - val_loss: 1.1934\n",
      "Epoch 4471/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4649 - val_loss: 1.3058\n",
      "Epoch 4472/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7784 - val_loss: 1.3267\n",
      "Epoch 4473/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.6531 - val_loss: 1.3630\n",
      "Epoch 4474/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7607 - val_loss: 1.2892\n",
      "Epoch 4475/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0396 - val_loss: 1.2763\n",
      "Epoch 4476/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7148 - val_loss: 1.2814\n",
      "Epoch 4477/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.8252 - val_loss: 1.4483\n",
      "Epoch 4478/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2466 - val_loss: 1.3572\n",
      "Epoch 4479/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6379 - val_loss: 1.2359\n",
      "Epoch 4480/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.4166 - val_loss: 1.1938\n",
      "Epoch 4481/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3423 - val_loss: 1.2907\n",
      "Epoch 4482/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.7718 - val_loss: 1.4504\n",
      "Epoch 4483/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5405 - val_loss: 1.3510\n",
      "Epoch 4484/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5749 - val_loss: 1.2529\n",
      "Epoch 4485/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 9.7692 - val_loss: 1.2738\n",
      "Epoch 4486/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5538 - val_loss: 1.2598\n",
      "Epoch 4487/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5172 - val_loss: 1.1629\n",
      "Epoch 4488/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5594 - val_loss: 1.2149\n",
      "Epoch 4489/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1636 - val_loss: 1.2075\n",
      "Epoch 4490/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3092 - val_loss: 1.2855\n",
      "Epoch 4491/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.6198 - val_loss: 1.3967\n",
      "Epoch 4492/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.8396 - val_loss: 1.3639\n",
      "Epoch 4493/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3556 - val_loss: 1.2011\n",
      "Epoch 4494/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6290 - val_loss: 1.2091\n",
      "Epoch 4495/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 10.3153 - val_loss: 1.3971\n",
      "Epoch 4496/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7514 - val_loss: 1.3252\n",
      "Epoch 4497/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.9290 - val_loss: 1.2906\n",
      "Epoch 4498/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3374 - val_loss: 1.2085\n",
      "Epoch 4499/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.1317 - val_loss: 1.1820\n",
      "Epoch 4500/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3969 - val_loss: 1.2334\n",
      "Epoch 4501/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5358 - val_loss: 1.3473\n",
      "Epoch 4502/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5292 - val_loss: 1.4013\n",
      "Epoch 4503/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5511 - val_loss: 1.3021\n",
      "Epoch 4504/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2910 - val_loss: 1.2632\n",
      "Epoch 4505/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8471 - val_loss: 1.2471\n",
      "Epoch 4506/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8856 - val_loss: 1.3555\n",
      "Epoch 4507/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5795 - val_loss: 1.3268\n",
      "Epoch 4508/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.5899 - val_loss: 1.3625\n",
      "Epoch 4509/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1084 - val_loss: 1.2422\n",
      "Epoch 4510/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3811 - val_loss: 1.2112\n",
      "Epoch 4511/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.8802 - val_loss: 1.2275\n",
      "Epoch 4512/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3712 - val_loss: 1.2519\n",
      "Epoch 4513/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.2099 - val_loss: 1.2623\n",
      "Epoch 4514/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5873 - val_loss: 1.2872\n",
      "Epoch 4515/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5048 - val_loss: 1.3681\n",
      "Epoch 4516/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8996 - val_loss: 1.3905\n",
      "Epoch 4517/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7430 - val_loss: 1.3224\n",
      "Epoch 4518/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6492 - val_loss: 1.2730\n",
      "Epoch 4519/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 9.6270 - val_loss: 1.2493\n",
      "Epoch 4520/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1514 - val_loss: 1.3074\n",
      "Epoch 4521/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4564 - val_loss: 1.2214\n",
      "Epoch 4522/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5123 - val_loss: 1.2708\n",
      "Epoch 4523/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9730 - val_loss: 1.1551\n",
      "Epoch 4524/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2098 - val_loss: 1.1498\n",
      "Epoch 4525/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4222 - val_loss: 1.3701\n",
      "Epoch 4526/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2415 - val_loss: 1.4269\n",
      "Epoch 4527/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4950 - val_loss: 1.4263\n",
      "Epoch 4528/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5301 - val_loss: 1.3533\n",
      "Epoch 4529/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8538 - val_loss: 1.2983\n",
      "Epoch 4530/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1997 - val_loss: 1.2722\n",
      "Epoch 4531/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.4845 - val_loss: 1.3929\n",
      "Epoch 4532/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6412 - val_loss: 1.4515\n",
      "Epoch 4533/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5101 - val_loss: 1.3197\n",
      "Epoch 4534/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.3251 - val_loss: 1.0785\n",
      "Epoch 4535/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5186 - val_loss: 1.1795\n",
      "Epoch 4536/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2607 - val_loss: 1.2905\n",
      "Epoch 4537/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2751 - val_loss: 1.3272\n",
      "Epoch 4538/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 5.342 - 0s 103us/step - loss: 9.4872 - val_loss: 1.3434\n",
      "Epoch 4539/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7564 - val_loss: 1.3431\n",
      "Epoch 4540/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2732 - val_loss: 1.2882\n",
      "Epoch 4541/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5475 - val_loss: 1.2985\n",
      "Epoch 4542/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3104 - val_loss: 1.2115\n",
      "Epoch 4543/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4823 - val_loss: 1.3503\n",
      "Epoch 4544/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1566 - val_loss: 1.3304\n",
      "Epoch 4545/10000\n",
      "90/90 [==============================] - 0s 82us/step - loss: 9.8388 - val_loss: 1.3923\n",
      "Epoch 4546/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5310 - val_loss: 1.3567\n",
      "Epoch 4547/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3529 - val_loss: 1.2462\n",
      "Epoch 4548/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5844 - val_loss: 1.2407\n",
      "Epoch 4549/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7181 - val_loss: 1.3243\n",
      "Epoch 4550/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4632 - val_loss: 1.3324\n",
      "Epoch 4551/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.1419 - val_loss: 1.1999\n",
      "Epoch 4552/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.6360 - val_loss: 1.2122\n",
      "Epoch 4553/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.9388 - val_loss: 1.3928\n",
      "Epoch 4554/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.1657 - val_loss: 1.3398\n",
      "Epoch 4555/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5533 - val_loss: 1.3269\n",
      "Epoch 4556/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5083 - val_loss: 1.2617\n",
      "Epoch 4557/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2368 - val_loss: 1.2347\n",
      "Epoch 4558/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3672 - val_loss: 1.3189\n",
      "Epoch 4559/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6548 - val_loss: 1.3217\n",
      "Epoch 4560/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6051 - val_loss: 1.2534\n",
      "Epoch 4561/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2463 - val_loss: 1.1517\n",
      "Epoch 4562/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5684 - val_loss: 1.3047\n",
      "Epoch 4563/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.9630 - val_loss: 1.3595\n",
      "Epoch 4564/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5780 - val_loss: 1.3931\n",
      "Epoch 4565/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.2521 - val_loss: 1.3833\n",
      "Epoch 4566/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8232 - val_loss: 1.3968\n",
      "Epoch 4567/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6207 - val_loss: 1.2525\n",
      "Epoch 4568/10000\n",
      "90/90 [==============================] - 0s 220us/step - loss: 9.7365 - val_loss: 1.2107\n",
      "Epoch 4569/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8845 - val_loss: 1.4315\n",
      "Epoch 4570/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8613 - val_loss: 1.4237\n",
      "Epoch 4571/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7169 - val_loss: 1.2704\n",
      "Epoch 4572/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6466 - val_loss: 1.1613\n",
      "Epoch 4573/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3959 - val_loss: 1.1581\n",
      "Epoch 4574/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9181 - val_loss: 1.4096\n",
      "Epoch 4575/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6733 - val_loss: 1.3819\n",
      "Epoch 4576/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4720 - val_loss: 1.2515\n",
      "Epoch 4577/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5604 - val_loss: 1.2174\n",
      "Epoch 4578/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0167 - val_loss: 1.2818\n",
      "Epoch 4579/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4751 - val_loss: 1.3090\n",
      "Epoch 4580/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5543 - val_loss: 1.3624\n",
      "Epoch 4581/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7345 - val_loss: 1.4001\n",
      "Epoch 4582/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.9060 - val_loss: 1.3059\n",
      "Epoch 4583/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3546 - val_loss: 1.1007\n",
      "Epoch 4584/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3743 - val_loss: 1.1892\n",
      "Epoch 4585/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5379 - val_loss: 1.4361\n",
      "Epoch 4586/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6223 - val_loss: 1.4135\n",
      "Epoch 4587/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1498 - val_loss: 1.2400\n",
      "Epoch 4588/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5692 - val_loss: 1.1458\n",
      "Epoch 4589/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2901 - val_loss: 1.2109\n",
      "Epoch 4590/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.0012 - val_loss: 1.4361\n",
      "Epoch 4591/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.2040 - val_loss: 1.4316\n",
      "Epoch 4592/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4276 - val_loss: 1.3167\n",
      "Epoch 4593/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2185 - val_loss: 1.1638\n",
      "Epoch 4594/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.8802 - val_loss: 1.1403\n",
      "Epoch 4595/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3509 - val_loss: 1.2713\n",
      "Epoch 4596/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3676 - val_loss: 1.4193\n",
      "Epoch 4597/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4520 - val_loss: 1.4426\n",
      "Epoch 4598/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6585 - val_loss: 1.3194\n",
      "Epoch 4599/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.2615 - val_loss: 1.1869\n",
      "Epoch 4600/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3824 - val_loss: 1.0842\n",
      "Epoch 4601/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6795 - val_loss: 1.3089\n",
      "Epoch 4602/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6594 - val_loss: 1.4030\n",
      "Epoch 4603/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9745 - val_loss: 1.5518\n",
      "Epoch 4604/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4437 - val_loss: 1.3574\n",
      "Epoch 4605/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6787 - val_loss: 1.2342\n",
      "Epoch 4606/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7088 - val_loss: 1.2563\n",
      "Epoch 4607/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.1294 - val_loss: 1.1585\n",
      "Epoch 4608/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5550 - val_loss: 1.2208\n",
      "Epoch 4609/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.3537 - val_loss: 1.2969\n",
      "Epoch 4610/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2916 - val_loss: 1.3113\n",
      "Epoch 4611/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.0115 - val_loss: 1.3714\n",
      "Epoch 4612/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7704 - val_loss: 1.3496\n",
      "Epoch 4613/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6334 - val_loss: 1.3068\n",
      "Epoch 4614/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.8453 - val_loss: 1.2843\n",
      "Epoch 4615/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7112 - val_loss: 1.2624\n",
      "Epoch 4616/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5247 - val_loss: 1.2863\n",
      "Epoch 4617/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1732 - val_loss: 1.2071\n",
      "Epoch 4618/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5772 - val_loss: 1.2089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4619/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7401 - val_loss: 1.2917\n",
      "Epoch 4620/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4024 - val_loss: 1.3584\n",
      "Epoch 4621/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6533 - val_loss: 1.2979\n",
      "Epoch 4622/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3895 - val_loss: 1.2158\n",
      "Epoch 4623/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8046 - val_loss: 1.3742\n",
      "Epoch 4624/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.9875 - val_loss: 1.3859\n",
      "Epoch 4625/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7790 - val_loss: 1.3080\n",
      "Epoch 4626/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4787 - val_loss: 1.2375\n",
      "Epoch 4627/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0180 - val_loss: 1.3353\n",
      "Epoch 4628/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.8835 - val_loss: 1.1911\n",
      "Epoch 4629/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6796 - val_loss: 1.2939\n",
      "Epoch 4630/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6211 - val_loss: 1.2797\n",
      "Epoch 4631/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8745 - val_loss: 1.1961\n",
      "Epoch 4632/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3252 - val_loss: 1.2745\n",
      "Epoch 4633/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4748 - val_loss: 1.3590\n",
      "Epoch 4634/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4370 - val_loss: 1.3691\n",
      "Epoch 4635/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5348 - val_loss: 1.2786\n",
      "Epoch 4636/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.3912 - val_loss: 1.2936\n",
      "Epoch 4637/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4925 - val_loss: 1.2991\n",
      "Epoch 4638/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3691 - val_loss: 1.2712\n",
      "Epoch 4639/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1792 - val_loss: 1.2786\n",
      "Epoch 4640/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7626 - val_loss: 1.3088\n",
      "Epoch 4641/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4903 - val_loss: 1.3060\n",
      "Epoch 4642/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2426 - val_loss: 1.3487\n",
      "Epoch 4643/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.2545 - val_loss: 1.3750\n",
      "Epoch 4644/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4361 - val_loss: 1.2548\n",
      "Epoch 4645/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6720 - val_loss: 1.1980\n",
      "Epoch 4646/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6605 - val_loss: 1.2921\n",
      "Epoch 4647/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8291 - val_loss: 1.3975\n",
      "Epoch 4648/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.4615 - val_loss: 1.2905\n",
      "Epoch 4649/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3011 - val_loss: 1.3199\n",
      "Epoch 4650/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7195 - val_loss: 1.3503\n",
      "Epoch 4651/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8336 - val_loss: 1.2883\n",
      "Epoch 4652/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5165 - val_loss: 1.2770\n",
      "Epoch 4653/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.9872 - val_loss: 1.4276\n",
      "Epoch 4654/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8582 - val_loss: 1.4086\n",
      "Epoch 4655/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6197 - val_loss: 1.2181\n",
      "Epoch 4656/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4410 - val_loss: 1.0816\n",
      "Epoch 4657/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 10.3652 - val_loss: 1.3648\n",
      "Epoch 4658/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4836 - val_loss: 1.4397\n",
      "Epoch 4659/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5449 - val_loss: 1.2659\n",
      "Epoch 4660/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.3874 - val_loss: 1.3314\n",
      "Epoch 4661/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3826 - val_loss: 1.3048\n",
      "Epoch 4662/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6446 - val_loss: 1.3841\n",
      "Epoch 4663/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4103 - val_loss: 1.3938\n",
      "Epoch 4664/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4409 - val_loss: 1.2835\n",
      "Epoch 4665/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5218 - val_loss: 1.2654\n",
      "Epoch 4666/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7943 - val_loss: 1.3297\n",
      "Epoch 4667/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0707 - val_loss: 1.2429\n",
      "Epoch 4668/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3922 - val_loss: 1.2203\n",
      "Epoch 4669/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7073 - val_loss: 1.2531\n",
      "Epoch 4670/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5148 - val_loss: 1.3289\n",
      "Epoch 4671/10000\n",
      "90/90 [==============================] - 0s 184us/step - loss: 9.3470 - val_loss: 1.2372\n",
      "Epoch 4672/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.0130 - val_loss: 1.1594\n",
      "Epoch 4673/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5614 - val_loss: 1.3481\n",
      "Epoch 4674/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4198 - val_loss: 1.4106\n",
      "Epoch 4675/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.2232 - val_loss: 1.4372\n",
      "Epoch 4676/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4178 - val_loss: 1.3734\n",
      "Epoch 4677/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8293 - val_loss: 1.3313\n",
      "Epoch 4678/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3252 - val_loss: 1.3124\n",
      "Epoch 4679/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6313 - val_loss: 1.3058\n",
      "Epoch 4680/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6000 - val_loss: 1.3430\n",
      "Epoch 4681/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4699 - val_loss: 1.1741\n",
      "Epoch 4682/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5000 - val_loss: 1.2204\n",
      "Epoch 4683/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7076 - val_loss: 1.3173\n",
      "Epoch 4684/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5346 - val_loss: 1.3518\n",
      "Epoch 4685/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3512 - val_loss: 1.3462\n",
      "Epoch 4686/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4388 - val_loss: 1.2919\n",
      "Epoch 4687/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3588 - val_loss: 1.3321\n",
      "Epoch 4688/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 10.0083 - val_loss: 1.4098\n",
      "Epoch 4689/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7256 - val_loss: 1.3117\n",
      "Epoch 4690/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4882 - val_loss: 1.1500\n",
      "Epoch 4691/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1586 - val_loss: 1.1846\n",
      "Epoch 4692/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6261 - val_loss: 1.3477\n",
      "Epoch 4693/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4425 - val_loss: 1.3295\n",
      "Epoch 4694/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2890 - val_loss: 1.2635\n",
      "Epoch 4695/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7942 - val_loss: 1.3425\n",
      "Epoch 4696/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5091 - val_loss: 1.3033\n",
      "Epoch 4697/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7624 - val_loss: 1.4117\n",
      "Epoch 4698/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5848 - val_loss: 1.3715\n",
      "Epoch 4699/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.9874 - val_loss: 1.4388\n",
      "Epoch 4700/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4521 - val_loss: 1.2983\n",
      "Epoch 4701/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3982 - val_loss: 1.2665\n",
      "Epoch 4702/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3798 - val_loss: 1.1776\n",
      "Epoch 4703/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5331 - val_loss: 1.3277\n",
      "Epoch 4704/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.9275 - val_loss: 1.4133\n",
      "Epoch 4705/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3046 - val_loss: 1.2414\n",
      "Epoch 4706/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.2402 - val_loss: 1.2760\n",
      "Epoch 4707/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.2744 - val_loss: 1.3041\n",
      "Epoch 4708/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.9885 - val_loss: 1.3448\n",
      "Epoch 4709/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.8386 - val_loss: 1.4071\n",
      "Epoch 4710/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.4988 - val_loss: 1.2701\n",
      "Epoch 4711/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6045 - val_loss: 1.2779\n",
      "Epoch 4712/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8725 - val_loss: 1.3839\n",
      "Epoch 4713/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4780 - val_loss: 1.4312\n",
      "Epoch 4714/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.4603 - val_loss: 1.3898\n",
      "Epoch 4715/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7864 - val_loss: 1.2124\n",
      "Epoch 4716/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3747 - val_loss: 1.2347\n",
      "Epoch 4717/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7227 - val_loss: 1.3272\n",
      "Epoch 4718/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5551 - val_loss: 1.3255\n",
      "Epoch 4719/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.1559 - val_loss: 1.2794\n",
      "Epoch 4720/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5360 - val_loss: 1.3416\n",
      "Epoch 4721/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3479 - val_loss: 1.2890\n",
      "Epoch 4722/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.1598 - val_loss: 1.2459\n",
      "Epoch 4723/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5964 - val_loss: 1.2550\n",
      "Epoch 4724/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6330 - val_loss: 1.3277\n",
      "Epoch 4725/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4906 - val_loss: 1.4184\n",
      "Epoch 4726/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.9072 - val_loss: 1.5106\n",
      "Epoch 4727/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6290 - val_loss: 1.3263\n",
      "Epoch 4728/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5018 - val_loss: 1.2127\n",
      "Epoch 4729/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.1738 - val_loss: 1.1957\n",
      "Epoch 4730/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.2460 - val_loss: 1.3121\n",
      "Epoch 4731/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3798 - val_loss: 1.4575\n",
      "Epoch 4732/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2980 - val_loss: 1.3953\n",
      "Epoch 4733/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5480 - val_loss: 1.2838\n",
      "Epoch 4734/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3505 - val_loss: 1.1961\n",
      "Epoch 4735/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4712 - val_loss: 1.2911\n",
      "Epoch 4736/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3365 - val_loss: 1.4307\n",
      "Epoch 4737/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4362 - val_loss: 1.4638\n",
      "Epoch 4738/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.1701 - val_loss: 1.2917\n",
      "Epoch 4739/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.3437 - val_loss: 1.2675\n",
      "Epoch 4740/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.1783 - val_loss: 1.2796\n",
      "Epoch 4741/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0476 - val_loss: 1.4450\n",
      "Epoch 4742/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6359 - val_loss: 1.3803\n",
      "Epoch 4743/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.4953 - val_loss: 1.2505\n",
      "Epoch 4744/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3285 - val_loss: 1.2803\n",
      "Epoch 4745/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.5712 - val_loss: 1.4327\n",
      "Epoch 4746/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4803 - val_loss: 1.3680\n",
      "Epoch 4747/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8371 - val_loss: 1.2889\n",
      "Epoch 4748/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2837 - val_loss: 1.1975\n",
      "Epoch 4749/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.0539 - val_loss: 1.2175\n",
      "Epoch 4750/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5160 - val_loss: 1.3491\n",
      "Epoch 4751/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3834 - val_loss: 1.4275\n",
      "Epoch 4752/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3912 - val_loss: 1.3879\n",
      "Epoch 4753/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.8652 - val_loss: 1.3764\n",
      "Epoch 4754/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3869 - val_loss: 1.2138\n",
      "Epoch 4755/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8004 - val_loss: 1.2578\n",
      "Epoch 4756/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5710 - val_loss: 1.4463\n",
      "Epoch 4757/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.1041 - val_loss: 1.3809\n",
      "Epoch 4758/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.5246 - val_loss: 1.3920\n",
      "Epoch 4759/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3006 - val_loss: 1.3667\n",
      "Epoch 4760/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4035 - val_loss: 1.2266\n",
      "Epoch 4761/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5524 - val_loss: 1.2631\n",
      "Epoch 4762/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.9556 - val_loss: 1.3916\n",
      "Epoch 4763/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.8945 - val_loss: 1.4022\n",
      "Epoch 4764/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2612 - val_loss: 1.1831\n",
      "Epoch 4765/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7888 - val_loss: 1.2540\n",
      "Epoch 4766/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3614 - val_loss: 1.3175\n",
      "Epoch 4767/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3333 - val_loss: 1.2668\n",
      "Epoch 4768/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4741 - val_loss: 1.3357\n",
      "Epoch 4769/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.4519 - val_loss: 1.3037\n",
      "Epoch 4770/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6854 - val_loss: 1.3647\n",
      "Epoch 4771/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.2897 - val_loss: 1.3280\n",
      "Epoch 4772/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5989 - val_loss: 1.3942\n",
      "Epoch 4773/10000\n",
      "90/90 [==============================] - 0s 227us/step - loss: 9.7490 - val_loss: 1.4186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4774/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4180 - val_loss: 1.3011\n",
      "Epoch 4775/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7322 - val_loss: 1.2755\n",
      "Epoch 4776/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.8936 - val_loss: 1.2110\n",
      "Epoch 4777/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6921 - val_loss: 1.3781\n",
      "Epoch 4778/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3423 - val_loss: 1.4306\n",
      "Epoch 4779/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6695 - val_loss: 1.4354\n",
      "Epoch 4780/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6464 - val_loss: 1.2266\n",
      "Epoch 4781/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.8260 - val_loss: 1.3175\n",
      "Epoch 4782/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5553 - val_loss: 1.3236\n",
      "Epoch 4783/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.3231 - val_loss: 1.3388\n",
      "Epoch 4784/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5186 - val_loss: 1.3473\n",
      "Epoch 4785/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3363 - val_loss: 1.3343\n",
      "Epoch 4786/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4011 - val_loss: 1.2844\n",
      "Epoch 4787/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4987 - val_loss: 1.3473\n",
      "Epoch 4788/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8860 - val_loss: 1.4074\n",
      "Epoch 4789/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.6007 - val_loss: 1.3362\n",
      "Epoch 4790/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 9.4521 - val_loss: 1.1447\n",
      "Epoch 4791/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.1545 - val_loss: 1.1838\n",
      "Epoch 4792/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2746 - val_loss: 1.3803\n",
      "Epoch 4793/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.1431 - val_loss: 1.4192\n",
      "Epoch 4794/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3796 - val_loss: 1.3605\n",
      "Epoch 4795/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6055 - val_loss: 1.3411\n",
      "Epoch 4796/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5347 - val_loss: 1.2600\n",
      "Epoch 4797/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8904 - val_loss: 1.2598\n",
      "Epoch 4798/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.1036 - val_loss: 1.2421\n",
      "Epoch 4799/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.3658 - val_loss: 1.3642\n",
      "Epoch 4800/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.8802 - val_loss: 1.4626\n",
      "Epoch 4801/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.7073 - val_loss: 1.3248\n",
      "Epoch 4802/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.4211 - val_loss: 1.2033\n",
      "Epoch 4803/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3169 - val_loss: 1.1714\n",
      "Epoch 4804/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2951 - val_loss: 1.3239\n",
      "Epoch 4805/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1403 - val_loss: 1.3945\n",
      "Epoch 4806/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3687 - val_loss: 1.4205\n",
      "Epoch 4807/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3789 - val_loss: 1.3474\n",
      "Epoch 4808/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.6611 - val_loss: 1.4033\n",
      "Epoch 4809/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.5742 - val_loss: 1.3685\n",
      "Epoch 4810/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3519 - val_loss: 1.3714\n",
      "Epoch 4811/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5265 - val_loss: 1.3524\n",
      "Epoch 4812/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.0980 - val_loss: 1.2354\n",
      "Epoch 4813/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5152 - val_loss: 1.2718\n",
      "Epoch 4814/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4363 - val_loss: 1.3075\n",
      "Epoch 4815/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7397 - val_loss: 1.3751\n",
      "Epoch 4816/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5147 - val_loss: 1.4033\n",
      "Epoch 4817/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3621 - val_loss: 1.2929\n",
      "Epoch 4818/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2832 - val_loss: 1.3283\n",
      "Epoch 4819/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2491 - val_loss: 1.3156\n",
      "Epoch 4820/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6891 - val_loss: 1.4278\n",
      "Epoch 4821/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6764 - val_loss: 1.4248\n",
      "Epoch 4822/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5320 - val_loss: 1.3243\n",
      "Epoch 4823/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7727 - val_loss: 1.1425\n",
      "Epoch 4824/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5849 - val_loss: 1.2929\n",
      "Epoch 4825/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5334 - val_loss: 1.3075\n",
      "Epoch 4826/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 10.2149 - val_loss: 1.5173\n",
      "Epoch 4827/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.5385 - val_loss: 1.3365\n",
      "Epoch 4828/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.5077 - val_loss: 1.3329\n",
      "Epoch 4829/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 9.2319 - val_loss: 1.3872\n",
      "Epoch 4830/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6034 - val_loss: 1.3851\n",
      "Epoch 4831/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.0814 - val_loss: 1.2771\n",
      "Epoch 4832/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7321 - val_loss: 1.3281\n",
      "Epoch 4833/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 9.7918 - val_loss: 1.3865\n",
      "Epoch 4834/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.1746 - val_loss: 1.2838\n",
      "Epoch 4835/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.3985 - val_loss: 1.2482\n",
      "Epoch 4836/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.8392 - val_loss: 1.3531\n",
      "Epoch 4837/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.8031 - val_loss: 1.3718\n",
      "Epoch 4838/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.3880 - val_loss: 1.3487\n",
      "Epoch 4839/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.1705 - val_loss: 1.2720\n",
      "Epoch 4840/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6400 - val_loss: 1.2676\n",
      "Epoch 4841/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.2018 - val_loss: 1.2422\n",
      "Epoch 4842/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5346 - val_loss: 1.4038\n",
      "Epoch 4843/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4924 - val_loss: 1.3887\n",
      "Epoch 4844/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.3881 - val_loss: 1.2896\n",
      "Epoch 4845/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4073 - val_loss: 1.3159\n",
      "Epoch 4846/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3936 - val_loss: 1.2927\n",
      "Epoch 4847/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3114 - val_loss: 1.3623\n",
      "Epoch 4848/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5523 - val_loss: 1.3505\n",
      "Epoch 4849/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6689 - val_loss: 1.3786\n",
      "Epoch 4850/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7469 - val_loss: 1.4536\n",
      "Epoch 4851/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6510 - val_loss: 1.3098\n",
      "Epoch 4852/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2603 - val_loss: 1.1994\n",
      "Epoch 4853/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.0725 - val_loss: 1.2515\n",
      "Epoch 4854/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.7125 - val_loss: 1.4289\n",
      "Epoch 4855/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4632 - val_loss: 1.4168\n",
      "Epoch 4856/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4655 - val_loss: 1.3601\n",
      "Epoch 4857/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7389 - val_loss: 1.3251\n",
      "Epoch 4858/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2268 - val_loss: 1.2066\n",
      "Epoch 4859/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6434 - val_loss: 1.3919\n",
      "Epoch 4860/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4404 - val_loss: 1.3670\n",
      "Epoch 4861/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5033 - val_loss: 1.2690\n",
      "Epoch 4862/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 9.6192 - val_loss: 1.3286\n",
      "Epoch 4863/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.1733 - val_loss: 1.2766\n",
      "Epoch 4864/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4817 - val_loss: 1.3610\n",
      "Epoch 4865/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3388 - val_loss: 1.3542\n",
      "Epoch 4866/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4810 - val_loss: 1.2331\n",
      "Epoch 4867/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6695 - val_loss: 1.3605\n",
      "Epoch 4868/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4997 - val_loss: 1.4025\n",
      "Epoch 4869/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 9.8266 - val_loss: 1.4246\n",
      "Epoch 4870/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1678 - val_loss: 1.2874\n",
      "Epoch 4871/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7450 - val_loss: 1.2816\n",
      "Epoch 4872/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.2685 - val_loss: 1.2283\n",
      "Epoch 4873/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2454 - val_loss: 1.3304\n",
      "Epoch 4874/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0911 - val_loss: 1.3251\n",
      "Epoch 4875/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4742 - val_loss: 1.3436\n",
      "Epoch 4876/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2145 - val_loss: 1.2610\n",
      "Epoch 4877/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.9280 - val_loss: 1.4237\n",
      "Epoch 4878/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4645 - val_loss: 1.4348\n",
      "Epoch 4879/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3925 - val_loss: 1.2864\n",
      "Epoch 4880/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2016 - val_loss: 1.3456\n",
      "Epoch 4881/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2235 - val_loss: 1.3811\n",
      "Epoch 4882/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1440 - val_loss: 1.3463\n",
      "Epoch 4883/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3210 - val_loss: 1.3352\n",
      "Epoch 4884/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3146 - val_loss: 1.3240\n",
      "Epoch 4885/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5473 - val_loss: 1.3539\n",
      "Epoch 4886/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5587 - val_loss: 1.3782\n",
      "Epoch 4887/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3008 - val_loss: 1.3330\n",
      "Epoch 4888/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3488 - val_loss: 1.4031\n",
      "Epoch 4889/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 10.0149 - val_loss: 1.5036\n",
      "Epoch 4890/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4015 - val_loss: 1.2239\n",
      "Epoch 4891/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4711 - val_loss: 1.2025\n",
      "Epoch 4892/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4448 - val_loss: 1.2598\n",
      "Epoch 4893/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4831 - val_loss: 1.3142\n",
      "Epoch 4894/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.1953 - val_loss: 1.2506\n",
      "Epoch 4895/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3574 - val_loss: 1.3053\n",
      "Epoch 4896/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4203 - val_loss: 1.3098\n",
      "Epoch 4897/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.0533 - val_loss: 1.3704\n",
      "Epoch 4898/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.9807 - val_loss: 1.5067\n",
      "Epoch 4899/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.9970 - val_loss: 1.4307\n",
      "Epoch 4900/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.7189 - val_loss: 1.2942\n",
      "Epoch 4901/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5958 - val_loss: 1.2150\n",
      "Epoch 4902/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6223 - val_loss: 1.3210\n",
      "Epoch 4903/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.7656 - val_loss: 1.4332\n",
      "Epoch 4904/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.6080 - val_loss: 1.3655\n",
      "Epoch 4905/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4545 - val_loss: 1.3005\n",
      "Epoch 4906/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.6523 - val_loss: 1.3218\n",
      "Epoch 4907/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3592 - val_loss: 1.3126\n",
      "Epoch 4908/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2538 - val_loss: 1.2662\n",
      "Epoch 4909/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3161 - val_loss: 1.3174\n",
      "Epoch 4910/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.3008 - val_loss: 1.3454\n",
      "Epoch 4911/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6509 - val_loss: 1.2547\n",
      "Epoch 4912/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7445 - val_loss: 1.2699\n",
      "Epoch 4913/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 11.84 - 0s 103us/step - loss: 9.5747 - val_loss: 1.3251\n",
      "Epoch 4914/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.6163 - val_loss: 1.3840\n",
      "Epoch 4915/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.1030 - val_loss: 1.2854\n",
      "Epoch 4916/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.8197 - val_loss: 1.4278\n",
      "Epoch 4917/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5159 - val_loss: 1.3717\n",
      "Epoch 4918/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.8253 - val_loss: 1.3499\n",
      "Epoch 4919/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.5018 - val_loss: 1.2270\n",
      "Epoch 4920/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3283 - val_loss: 1.2090\n",
      "Epoch 4921/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.3823 - val_loss: 1.3520\n",
      "Epoch 4922/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2811 - val_loss: 1.3943\n",
      "Epoch 4923/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4230 - val_loss: 1.3924\n",
      "Epoch 4924/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.5905 - val_loss: 1.3082\n",
      "Epoch 4925/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.1623 - val_loss: 1.3427\n",
      "Epoch 4926/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7071 - val_loss: 1.4972\n",
      "Epoch 4927/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8387 - val_loss: 1.5022\n",
      "Epoch 4928/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 90us/step - loss: 9.3550 - val_loss: 1.3003\n",
      "Epoch 4929/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.1564 - val_loss: 1.1738\n",
      "Epoch 4930/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.3488 - val_loss: 1.1834\n",
      "Epoch 4931/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6186 - val_loss: 1.4009\n",
      "Epoch 4932/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.9752 - val_loss: 1.5595\n",
      "Epoch 4933/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5796 - val_loss: 1.3720\n",
      "Epoch 4934/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7210 - val_loss: 1.2609\n",
      "Epoch 4935/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4157 - val_loss: 1.2479\n",
      "Epoch 4936/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6423 - val_loss: 1.3259\n",
      "Epoch 4937/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2388 - val_loss: 1.3682\n",
      "Epoch 4938/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6391 - val_loss: 1.3678\n",
      "Epoch 4939/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6779 - val_loss: 1.2927\n",
      "Epoch 4940/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4202 - val_loss: 1.2274\n",
      "Epoch 4941/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0160 - val_loss: 1.2065\n",
      "Epoch 4942/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5626 - val_loss: 1.3806\n",
      "Epoch 4943/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.1078 - val_loss: 1.4814\n",
      "Epoch 4944/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6638 - val_loss: 1.4888\n",
      "Epoch 4945/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4901 - val_loss: 1.3855\n",
      "Epoch 4946/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4883 - val_loss: 1.2889\n",
      "Epoch 4947/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3099 - val_loss: 1.2164\n",
      "Epoch 4948/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.9948 - val_loss: 1.2702\n",
      "Epoch 4949/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6432 - val_loss: 1.4748\n",
      "Epoch 4950/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.2533 - val_loss: 1.4000\n",
      "Epoch 4951/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5437 - val_loss: 1.2215\n",
      "Epoch 4952/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7232 - val_loss: 1.2646\n",
      "Epoch 4953/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4922 - val_loss: 1.3389\n",
      "Epoch 4954/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5510 - val_loss: 1.4486\n",
      "Epoch 4955/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5329 - val_loss: 1.3908\n",
      "Epoch 4956/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5863 - val_loss: 1.3418\n",
      "Epoch 4957/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0561 - val_loss: 1.2964\n",
      "Epoch 4958/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.6056 - val_loss: 1.3013\n",
      "Epoch 4959/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6629 - val_loss: 1.3872\n",
      "Epoch 4960/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4717 - val_loss: 1.3208\n",
      "Epoch 4961/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.9725 - val_loss: 1.2080\n",
      "Epoch 4962/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4697 - val_loss: 1.2424\n",
      "Epoch 4963/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.2097 - val_loss: 1.2973\n",
      "Epoch 4964/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7856 - val_loss: 1.4039\n",
      "Epoch 4965/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5556 - val_loss: 1.4063\n",
      "Epoch 4966/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 10.0534 - val_loss: 1.4688\n",
      "Epoch 4967/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4364 - val_loss: 1.3612\n",
      "Epoch 4968/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7178 - val_loss: 1.3258\n",
      "Epoch 4969/10000\n",
      "90/90 [==============================] - 0s 212us/step - loss: 9.4652 - val_loss: 1.2174\n",
      "Epoch 4970/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5248 - val_loss: 1.1512\n",
      "Epoch 4971/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5436 - val_loss: 1.2307\n",
      "Epoch 4972/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.2147 - val_loss: 1.3060\n",
      "Epoch 4973/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2091 - val_loss: 1.3107\n",
      "Epoch 4974/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5904 - val_loss: 1.3693\n",
      "Epoch 4975/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.4659 - val_loss: 1.4390\n",
      "Epoch 4976/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3544 - val_loss: 1.4046\n",
      "Epoch 4977/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3503 - val_loss: 1.3449\n",
      "Epoch 4978/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4444 - val_loss: 1.3900\n",
      "Epoch 4979/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.8005 - val_loss: 1.4013\n",
      "Epoch 4980/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5603 - val_loss: 1.3065\n",
      "Epoch 4981/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.0336 - val_loss: 1.1448\n",
      "Epoch 4982/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.7091 - val_loss: 1.1575\n",
      "Epoch 4983/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.7901 - val_loss: 1.3260\n",
      "Epoch 4984/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 10.3040 - val_loss: 1.5774\n",
      "Epoch 4985/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5078 - val_loss: 1.4076\n",
      "Epoch 4986/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.1078 - val_loss: 1.2976\n",
      "Epoch 4987/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3553 - val_loss: 1.2856\n",
      "Epoch 4988/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5159 - val_loss: 1.2760\n",
      "Epoch 4989/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6883 - val_loss: 1.4998\n",
      "Epoch 4990/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.9159 - val_loss: 1.5020\n",
      "Epoch 4991/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.0837 - val_loss: 1.4103\n",
      "Epoch 4992/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2654 - val_loss: 1.2248\n",
      "Epoch 4993/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4683 - val_loss: 1.2093\n",
      "Epoch 4994/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0135 - val_loss: 1.2665\n",
      "Epoch 4995/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3930 - val_loss: 1.4073\n",
      "Epoch 4996/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4928 - val_loss: 1.3798\n",
      "Epoch 4997/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.9606 - val_loss: 1.2813\n",
      "Epoch 4998/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2769 - val_loss: 1.2362\n",
      "Epoch 4999/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.2621 - val_loss: 1.2814\n",
      "Epoch 5000/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.7010 - val_loss: 1.3206\n",
      "Epoch 5001/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6611 - val_loss: 1.4824\n",
      "Epoch 5002/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6574 - val_loss: 1.4461\n",
      "Epoch 5003/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5191 - val_loss: 1.3901\n",
      "Epoch 5004/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5635 - val_loss: 1.3356\n",
      "Epoch 5005/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1021 - val_loss: 1.3278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5006/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4679 - val_loss: 1.4752\n",
      "Epoch 5007/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4254 - val_loss: 1.4129\n",
      "Epoch 5008/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2417 - val_loss: 1.3009\n",
      "Epoch 5009/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5415 - val_loss: 1.1878\n",
      "Epoch 5010/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3625 - val_loss: 1.2546\n",
      "Epoch 5011/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2772 - val_loss: 1.4287\n",
      "Epoch 5012/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7563 - val_loss: 1.4643\n",
      "Epoch 5013/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.5288 - val_loss: 1.4322\n",
      "Epoch 5014/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7578 - val_loss: 1.4326\n",
      "Epoch 5015/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5409 - val_loss: 1.3570\n",
      "Epoch 5016/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5758 - val_loss: 1.3950\n",
      "Epoch 5017/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7242 - val_loss: 1.4444\n",
      "Epoch 5018/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4979 - val_loss: 1.3280\n",
      "Epoch 5019/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.6277 - val_loss: 1.2112\n",
      "Epoch 5020/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.3735 - val_loss: 1.2268\n",
      "Epoch 5021/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.6693 - val_loss: 1.2930\n",
      "Epoch 5022/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4269 - val_loss: 1.3214\n",
      "Epoch 5023/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5342 - val_loss: 1.3473\n",
      "Epoch 5024/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3842 - val_loss: 1.3141\n",
      "Epoch 5025/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2334 - val_loss: 1.3385\n",
      "Epoch 5026/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7071 - val_loss: 1.3920\n",
      "Epoch 5027/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.2561 - val_loss: 1.3856\n",
      "Epoch 5028/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.5960 - val_loss: 1.3743\n",
      "Epoch 5029/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5049 - val_loss: 1.4342\n",
      "Epoch 5030/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7405 - val_loss: 1.4326\n",
      "Epoch 5031/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2993 - val_loss: 1.3988\n",
      "Epoch 5032/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4199 - val_loss: 1.2991\n",
      "Epoch 5033/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1708 - val_loss: 1.3044\n",
      "Epoch 5034/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4776 - val_loss: 1.3026\n",
      "Epoch 5035/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4268 - val_loss: 1.2568\n",
      "Epoch 5036/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9414 - val_loss: 1.2080\n",
      "Epoch 5037/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3922 - val_loss: 1.4338\n",
      "Epoch 5038/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.8175 - val_loss: 1.5527\n",
      "Epoch 5039/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3957 - val_loss: 1.4408\n",
      "Epoch 5040/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1940 - val_loss: 1.3417\n",
      "Epoch 5041/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4359 - val_loss: 1.3247\n",
      "Epoch 5042/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5069 - val_loss: 1.3888\n",
      "Epoch 5043/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1719 - val_loss: 1.3090\n",
      "Epoch 5044/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3771 - val_loss: 1.3783\n",
      "Epoch 5045/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2506 - val_loss: 1.2377\n",
      "Epoch 5046/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.7983 - val_loss: 1.3527\n",
      "Epoch 5047/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6679 - val_loss: 1.4569\n",
      "Epoch 5048/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.1782 - val_loss: 1.5444\n",
      "Epoch 5049/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3069 - val_loss: 1.3623\n",
      "Epoch 5050/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3159 - val_loss: 1.3169\n",
      "Epoch 5051/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.0100 - val_loss: 1.2935\n",
      "Epoch 5052/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6312 - val_loss: 1.3337\n",
      "Epoch 5053/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1197 - val_loss: 1.3121\n",
      "Epoch 5054/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4434 - val_loss: 1.3474\n",
      "Epoch 5055/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5532 - val_loss: 1.4109\n",
      "Epoch 5056/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1946 - val_loss: 1.2397\n",
      "Epoch 5057/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4274 - val_loss: 1.3494\n",
      "Epoch 5058/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.9108 - val_loss: 1.5505\n",
      "Epoch 5059/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6893 - val_loss: 1.5003\n",
      "Epoch 5060/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.3200 - val_loss: 1.3329\n",
      "Epoch 5061/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7593 - val_loss: 1.2808\n",
      "Epoch 5062/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2589 - val_loss: 1.2764\n",
      "Epoch 5063/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.7475 - val_loss: 1.3466\n",
      "Epoch 5064/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4659 - val_loss: 1.3245\n",
      "Epoch 5065/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5198 - val_loss: 1.3867\n",
      "Epoch 5066/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4955 - val_loss: 1.3062\n",
      "Epoch 5067/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7576 - val_loss: 1.2408\n",
      "Epoch 5068/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.7057 - val_loss: 1.3203\n",
      "Epoch 5069/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3776 - val_loss: 1.3874\n",
      "Epoch 5070/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6665 - val_loss: 1.4125\n",
      "Epoch 5071/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.2195 - val_loss: 1.4781\n",
      "Epoch 5072/10000\n",
      "90/90 [==============================] - 0s 205us/step - loss: 9.3417 - val_loss: 1.3370\n",
      "Epoch 5073/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7219 - val_loss: 1.2754\n",
      "Epoch 5074/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.1373 - val_loss: 1.1461\n",
      "Epoch 5075/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8978 - val_loss: 1.3814\n",
      "Epoch 5076/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3725 - val_loss: 1.4079\n",
      "Epoch 5077/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6717 - val_loss: 1.3697\n",
      "Epoch 5078/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5693 - val_loss: 1.3782\n",
      "Epoch 5079/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5974 - val_loss: 1.2833\n",
      "Epoch 5080/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5633 - val_loss: 1.2845\n",
      "Epoch 5081/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5954 - val_loss: 1.3836\n",
      "Epoch 5082/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.0673 - val_loss: 1.3977\n",
      "Epoch 5083/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3489 - val_loss: 1.3388\n",
      "Epoch 5084/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2850 - val_loss: 1.3038\n",
      "Epoch 5085/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4023 - val_loss: 1.3294\n",
      "Epoch 5086/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.4632 - val_loss: 1.3680\n",
      "Epoch 5087/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.9908 - val_loss: 1.3662\n",
      "Epoch 5088/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7097 - val_loss: 1.4001\n",
      "Epoch 5089/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.2735 - val_loss: 1.3040\n",
      "Epoch 5090/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6052 - val_loss: 1.3949\n",
      "Epoch 5091/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.1859 - val_loss: 1.4495\n",
      "Epoch 5092/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3787 - val_loss: 1.4925\n",
      "Epoch 5093/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6864 - val_loss: 1.4466\n",
      "Epoch 5094/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.4546 - val_loss: 1.2675\n",
      "Epoch 5095/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5645 - val_loss: 1.1842\n",
      "Epoch 5096/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4362 - val_loss: 1.2270\n",
      "Epoch 5097/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.1941 - val_loss: 1.3194\n",
      "Epoch 5098/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 10.1579 - val_loss: 1.4808\n",
      "Epoch 5099/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4805 - val_loss: 1.4212\n",
      "Epoch 5100/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 10.0124 - val_loss: 1.3712\n",
      "Epoch 5101/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.3031 - val_loss: 1.3178\n",
      "Epoch 5102/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9858 - val_loss: 1.4244\n",
      "Epoch 5103/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.3398 - val_loss: 1.3240\n",
      "Epoch 5104/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.2974 - val_loss: 1.3063\n",
      "Epoch 5105/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2781 - val_loss: 1.3252\n",
      "Epoch 5106/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5291 - val_loss: 1.4222\n",
      "Epoch 5107/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7178 - val_loss: 1.3784\n",
      "Epoch 5108/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2406 - val_loss: 1.2946\n",
      "Epoch 5109/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4505 - val_loss: 1.3232\n",
      "Epoch 5110/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.6031 - val_loss: 1.3721\n",
      "Epoch 5111/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.6566 - val_loss: 1.3395\n",
      "Epoch 5112/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4964 - val_loss: 1.3979\n",
      "Epoch 5113/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.0070 - val_loss: 1.3057\n",
      "Epoch 5114/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3207 - val_loss: 1.3405\n",
      "Epoch 5115/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5146 - val_loss: 1.3897\n",
      "Epoch 5116/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4215 - val_loss: 1.4368\n",
      "Epoch 5117/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4560 - val_loss: 1.3551\n",
      "Epoch 5118/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.7685 - val_loss: 1.4522\n",
      "Epoch 5119/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2832 - val_loss: 1.4009\n",
      "Epoch 5120/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5161 - val_loss: 1.3214\n",
      "Epoch 5121/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2138 - val_loss: 1.2970\n",
      "Epoch 5122/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.0302 - val_loss: 1.2737\n",
      "Epoch 5123/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0162 - val_loss: 1.3291\n",
      "Epoch 5124/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.7567 - val_loss: 1.4653\n",
      "Epoch 5125/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4926 - val_loss: 1.3458\n",
      "Epoch 5126/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.5520 - val_loss: 1.2922\n",
      "Epoch 5127/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.0660 - val_loss: 1.2495\n",
      "Epoch 5128/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.5834 - val_loss: 1.3827\n",
      "Epoch 5129/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.2540 - val_loss: 1.4507\n",
      "Epoch 5130/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3470 - val_loss: 1.4034\n",
      "Epoch 5131/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3542 - val_loss: 1.4746\n",
      "Epoch 5132/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.3845 - val_loss: 1.3530\n",
      "Epoch 5133/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3718 - val_loss: 1.3839\n",
      "Epoch 5134/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4944 - val_loss: 1.4273\n",
      "Epoch 5135/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2176 - val_loss: 1.3542\n",
      "Epoch 5136/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.0618 - val_loss: 1.2511\n",
      "Epoch 5137/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.9371 - val_loss: 1.2901\n",
      "Epoch 5138/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2269 - val_loss: 1.3162\n",
      "Epoch 5139/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2622 - val_loss: 1.4406\n",
      "Epoch 5140/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1701 - val_loss: 1.4268\n",
      "Epoch 5141/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6743 - val_loss: 1.4349\n",
      "Epoch 5142/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5217 - val_loss: 1.4401\n",
      "Epoch 5143/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4721 - val_loss: 1.4432\n",
      "Epoch 5144/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4095 - val_loss: 1.3950\n",
      "Epoch 5145/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4150 - val_loss: 1.2939\n",
      "Epoch 5146/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7202 - val_loss: 1.4492\n",
      "Epoch 5147/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 10.1016 - val_loss: 1.5217\n",
      "Epoch 5148/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7537 - val_loss: 1.3433\n",
      "Epoch 5149/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3879 - val_loss: 1.2433\n",
      "Epoch 5150/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.7181 - val_loss: 1.3036\n",
      "Epoch 5151/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.7128 - val_loss: 1.4118\n",
      "Epoch 5152/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.3826 - val_loss: 1.3336\n",
      "Epoch 5153/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.2635 - val_loss: 1.2686\n",
      "Epoch 5154/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2580 - val_loss: 1.3665\n",
      "Epoch 5155/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1135 - val_loss: 1.4440\n",
      "Epoch 5156/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5947 - val_loss: 1.4368\n",
      "Epoch 5157/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.8311 - val_loss: 1.5278\n",
      "Epoch 5158/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9248 - val_loss: 1.4151\n",
      "Epoch 5159/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1043 - val_loss: 1.2804\n",
      "Epoch 5160/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4868 - val_loss: 1.4357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5161/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.0763 - val_loss: 1.4988\n",
      "Epoch 5162/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5239 - val_loss: 1.4297\n",
      "Epoch 5163/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.8821 - val_loss: 1.3833\n",
      "Epoch 5164/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3951 - val_loss: 1.2380\n",
      "Epoch 5165/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3546 - val_loss: 1.1463\n",
      "Epoch 5166/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.8852 - val_loss: 1.2068\n",
      "Epoch 5167/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3989 - val_loss: 1.3816\n",
      "Epoch 5168/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3465 - val_loss: 1.5169\n",
      "Epoch 5169/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.1855 - val_loss: 1.4168\n",
      "Epoch 5170/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5276 - val_loss: 1.3933\n",
      "Epoch 5171/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.0622 - val_loss: 1.3782\n",
      "Epoch 5172/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9227 - val_loss: 1.5115\n",
      "Epoch 5173/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6717 - val_loss: 1.4870\n",
      "Epoch 5174/10000\n",
      "90/90 [==============================] - 0s 204us/step - loss: 9.1513 - val_loss: 1.3597\n",
      "Epoch 5175/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7144 - val_loss: 1.3787\n",
      "Epoch 5176/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4118 - val_loss: 1.3398\n",
      "Epoch 5177/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0400 - val_loss: 1.2483\n",
      "Epoch 5178/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4426 - val_loss: 1.2792\n",
      "Epoch 5179/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3834 - val_loss: 1.3183\n",
      "Epoch 5180/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6553 - val_loss: 1.4716\n",
      "Epoch 5181/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4488 - val_loss: 1.4782\n",
      "Epoch 5182/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.0401 - val_loss: 1.3921\n",
      "Epoch 5183/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.5888 - val_loss: 1.2840\n",
      "Epoch 5184/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3060 - val_loss: 1.3001\n",
      "Epoch 5185/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4876 - val_loss: 1.4924\n",
      "Epoch 5186/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5801 - val_loss: 1.5457\n",
      "Epoch 5187/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6702 - val_loss: 1.4860\n",
      "Epoch 5188/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.2314 - val_loss: 1.3176\n",
      "Epoch 5189/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1191 - val_loss: 1.2951\n",
      "Epoch 5190/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.7889 - val_loss: 1.3509\n",
      "Epoch 5191/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5678 - val_loss: 1.4268\n",
      "Epoch 5192/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6794 - val_loss: 1.3383\n",
      "Epoch 5193/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1594 - val_loss: 1.3891\n",
      "Epoch 5194/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5145 - val_loss: 1.4082\n",
      "Epoch 5195/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7254 - val_loss: 1.4131\n",
      "Epoch 5196/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6114 - val_loss: 1.3970\n",
      "Epoch 5197/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4669 - val_loss: 1.3354\n",
      "Epoch 5198/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2882 - val_loss: 1.3996\n",
      "Epoch 5199/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6238 - val_loss: 1.4440\n",
      "Epoch 5200/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1353 - val_loss: 1.3071\n",
      "Epoch 5201/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4975 - val_loss: 1.3688\n",
      "Epoch 5202/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5176 - val_loss: 1.5054\n",
      "Epoch 5203/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6893 - val_loss: 1.4828\n",
      "Epoch 5204/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3327 - val_loss: 1.3805\n",
      "Epoch 5205/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.3913 - val_loss: 1.3280\n",
      "Epoch 5206/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.2091 - val_loss: 1.2829\n",
      "Epoch 5207/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5211 - val_loss: 1.3191\n",
      "Epoch 5208/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.2723 - val_loss: 1.3510\n",
      "Epoch 5209/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3657 - val_loss: 1.3375\n",
      "Epoch 5210/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3846 - val_loss: 1.3830\n",
      "Epoch 5211/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2385 - val_loss: 1.4471\n",
      "Epoch 5212/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5106 - val_loss: 1.5217\n",
      "Epoch 5213/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.4396 - val_loss: 1.4648\n",
      "Epoch 5214/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.8346 - val_loss: 1.3807\n",
      "Epoch 5215/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0880 - val_loss: 1.2934\n",
      "Epoch 5216/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 9.1266 - val_loss: 1.3517\n",
      "Epoch 5217/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 9.9352 - val_loss: 1.4795\n",
      "Epoch 5218/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 9.3655 - val_loss: 1.5495\n",
      "Epoch 5219/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.2075 - val_loss: 1.3577\n",
      "Epoch 5220/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.5617 - val_loss: 1.4307\n",
      "Epoch 5221/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 9.1843 - val_loss: 1.3188\n",
      "Epoch 5222/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.4875 - val_loss: 1.3275\n",
      "Epoch 5223/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.1732 - val_loss: 1.3563\n",
      "Epoch 5224/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.8275 - val_loss: 1.4667\n",
      "Epoch 5225/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.6228 - val_loss: 1.4344\n",
      "Epoch 5226/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.1393 - val_loss: 1.2710\n",
      "Epoch 5227/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.2269 - val_loss: 1.2842\n",
      "Epoch 5228/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.0611 - val_loss: 1.4272\n",
      "Epoch 5229/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 9.5370 - val_loss: 1.4895\n",
      "Epoch 5230/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.0167 - val_loss: 1.4157\n",
      "Epoch 5231/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.6505 - val_loss: 1.3707\n",
      "Epoch 5232/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.8006 - val_loss: 1.3066\n",
      "Epoch 5233/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5115 - val_loss: 1.4403\n",
      "Epoch 5234/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.1570 - val_loss: 1.4160\n",
      "Epoch 5235/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.8071 - val_loss: 1.5647\n",
      "Epoch 5236/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2901 - val_loss: 1.4282\n",
      "Epoch 5237/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.7274 - val_loss: 1.3954\n",
      "Epoch 5238/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3074 - val_loss: 1.3458\n",
      "Epoch 5239/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6584 - val_loss: 1.3967\n",
      "Epoch 5240/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.2634 - val_loss: 1.2669\n",
      "Epoch 5241/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4213 - val_loss: 1.3011\n",
      "Epoch 5242/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.7171 - val_loss: 1.3691\n",
      "Epoch 5243/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2617 - val_loss: 1.3684\n",
      "Epoch 5244/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.8835 - val_loss: 1.3945\n",
      "Epoch 5245/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5332 - val_loss: 1.4372\n",
      "Epoch 5246/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.0736 - val_loss: 1.3867\n",
      "Epoch 5247/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.9103 - val_loss: 1.4281\n",
      "Epoch 5248/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6106 - val_loss: 1.4143\n",
      "Epoch 5249/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5437 - val_loss: 1.3250\n",
      "Epoch 5250/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3963 - val_loss: 1.2997\n",
      "Epoch 5251/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7093 - val_loss: 1.4672\n",
      "Epoch 5252/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3720 - val_loss: 1.4350\n",
      "Epoch 5253/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.3135 - val_loss: 1.4016\n",
      "Epoch 5254/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.0875 - val_loss: 1.2925\n",
      "Epoch 5255/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.0556 - val_loss: 1.2366\n",
      "Epoch 5256/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.4049 - val_loss: 1.3822\n",
      "Epoch 5257/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3930 - val_loss: 1.4679\n",
      "Epoch 5258/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3352 - val_loss: 1.5541\n",
      "Epoch 5259/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6799 - val_loss: 1.5674\n",
      "Epoch 5260/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2385 - val_loss: 1.4456\n",
      "Epoch 5261/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2829 - val_loss: 1.3408\n",
      "Epoch 5262/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.1614 - val_loss: 1.2869\n",
      "Epoch 5263/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5810 - val_loss: 1.4176\n",
      "Epoch 5264/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.7117 - val_loss: 1.5235\n",
      "Epoch 5265/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2108 - val_loss: 1.4123\n",
      "Epoch 5266/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1368 - val_loss: 1.1861\n",
      "Epoch 5267/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5438 - val_loss: 1.1871\n",
      "Epoch 5268/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.6160 - val_loss: 1.4333\n",
      "Epoch 5269/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.1368 - val_loss: 1.5342\n",
      "Epoch 5270/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5669 - val_loss: 1.6007\n",
      "Epoch 5271/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.7798 - val_loss: 1.5407\n",
      "Epoch 5272/10000\n",
      "90/90 [==============================] - 0s 272us/step - loss: 9.3727 - val_loss: 1.3609\n",
      "Epoch 5273/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.5362 - val_loss: 1.2950\n",
      "Epoch 5274/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 9.1183 - val_loss: 1.2201\n",
      "Epoch 5275/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3721 - val_loss: 1.4349\n",
      "Epoch 5276/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.3999 - val_loss: 1.5731\n",
      "Epoch 5277/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5471 - val_loss: 1.5125\n",
      "Epoch 5278/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4932 - val_loss: 1.4193\n",
      "Epoch 5279/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1959 - val_loss: 1.3105\n",
      "Epoch 5280/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.0212 - val_loss: 1.1575\n",
      "Epoch 5281/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.0974 - val_loss: 1.2892\n",
      "Epoch 5282/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.8466 - val_loss: 1.6000\n",
      "Epoch 5283/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.2567 - val_loss: 1.5063\n",
      "Epoch 5284/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3008 - val_loss: 1.3966\n",
      "Epoch 5285/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2265 - val_loss: 1.2663\n",
      "Epoch 5286/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6603 - val_loss: 1.4568\n",
      "Epoch 5287/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2008 - val_loss: 1.5261\n",
      "Epoch 5288/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6247 - val_loss: 1.4655\n",
      "Epoch 5289/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3736 - val_loss: 1.3921\n",
      "Epoch 5290/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.1246 - val_loss: 1.3100\n",
      "Epoch 5291/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.7707 - val_loss: 1.4015\n",
      "Epoch 5292/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2818 - val_loss: 1.4880\n",
      "Epoch 5293/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.9964 - val_loss: 1.3964\n",
      "Epoch 5294/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.6491 - val_loss: 1.4326\n",
      "Epoch 5295/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.1761 - val_loss: 1.3230\n",
      "Epoch 5296/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1530 - val_loss: 1.3376\n",
      "Epoch 5297/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4307 - val_loss: 1.3999\n",
      "Epoch 5298/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6017 - val_loss: 1.4721\n",
      "Epoch 5299/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1782 - val_loss: 1.4547\n",
      "Epoch 5300/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5380 - val_loss: 1.3945\n",
      "Epoch 5301/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6853 - val_loss: 1.4367\n",
      "Epoch 5302/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.6045 - val_loss: 1.4112\n",
      "Epoch 5303/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.1357 - val_loss: 1.3246\n",
      "Epoch 5304/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2476 - val_loss: 1.3423\n",
      "Epoch 5305/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.0645 - val_loss: 1.3802\n",
      "Epoch 5306/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2560 - val_loss: 1.3915\n",
      "Epoch 5307/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5680 - val_loss: 1.5196\n",
      "Epoch 5308/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2950 - val_loss: 1.4795\n",
      "Epoch 5309/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3489 - val_loss: 1.4445\n",
      "Epoch 5310/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.8573 - val_loss: 1.4316\n",
      "Epoch 5311/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2321 - val_loss: 1.3535\n",
      "Epoch 5312/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4224 - val_loss: 1.4195\n",
      "Epoch 5313/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5911 - val_loss: 1.4263\n",
      "Epoch 5314/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.7235 - val_loss: 1.4810\n",
      "Epoch 5315/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.2522 - val_loss: 1.3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5316/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.0482 - val_loss: 1.3605\n",
      "Epoch 5317/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3266 - val_loss: 1.4256\n",
      "Epoch 5318/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.1132 - val_loss: 1.4976\n",
      "Epoch 5319/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3739 - val_loss: 1.4657\n",
      "Epoch 5320/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5427 - val_loss: 1.4224\n",
      "Epoch 5321/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2612 - val_loss: 1.3382\n",
      "Epoch 5322/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4698 - val_loss: 1.3849\n",
      "Epoch 5323/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3243 - val_loss: 1.4162\n",
      "Epoch 5324/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4628 - val_loss: 1.4273\n",
      "Epoch 5325/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.4456 - val_loss: 1.3952\n",
      "Epoch 5326/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3830 - val_loss: 1.3336\n",
      "Epoch 5327/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0705 - val_loss: 1.4025\n",
      "Epoch 5328/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4111 - val_loss: 1.4361\n",
      "Epoch 5329/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.5702 - val_loss: 1.5053\n",
      "Epoch 5330/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3032 - val_loss: 1.3956\n",
      "Epoch 5331/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.4458 - val_loss: 1.4443\n",
      "Epoch 5332/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.9806 - val_loss: 1.3790\n",
      "Epoch 5333/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1845 - val_loss: 1.4116\n",
      "Epoch 5334/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3066 - val_loss: 1.4303\n",
      "Epoch 5335/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4256 - val_loss: 1.4032\n",
      "Epoch 5336/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5531 - val_loss: 1.4990\n",
      "Epoch 5337/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1204 - val_loss: 1.4626\n",
      "Epoch 5338/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5004 - val_loss: 1.4369\n",
      "Epoch 5339/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1765 - val_loss: 1.3508\n",
      "Epoch 5340/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1781 - val_loss: 1.3166\n",
      "Epoch 5341/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.8688 - val_loss: 1.2955\n",
      "Epoch 5342/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4419 - val_loss: 1.3479\n",
      "Epoch 5343/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4587 - val_loss: 1.4805\n",
      "Epoch 5344/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3883 - val_loss: 1.5191\n",
      "Epoch 5345/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.6441 - val_loss: 1.4616\n",
      "Epoch 5346/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1122 - val_loss: 1.3918\n",
      "Epoch 5347/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.1550 - val_loss: 1.2914\n",
      "Epoch 5348/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3849 - val_loss: 1.4049\n",
      "Epoch 5349/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.7388 - val_loss: 1.5204\n",
      "Epoch 5350/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3449 - val_loss: 1.4160\n",
      "Epoch 5351/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2715 - val_loss: 1.3321\n",
      "Epoch 5352/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.7114 - val_loss: 1.4320\n",
      "Epoch 5353/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 8.9842 - val_loss: 1.4252\n",
      "Epoch 5354/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 9.3004 - val_loss: 1.3971\n",
      "Epoch 5355/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5196 - val_loss: 1.3672\n",
      "Epoch 5356/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.5896 - val_loss: 1.3556\n",
      "Epoch 5357/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2525 - val_loss: 1.4114\n",
      "Epoch 5358/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 9.2244 - val_loss: 1.4626\n",
      "Epoch 5359/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2996 - val_loss: 1.6157\n",
      "Epoch 5360/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3658 - val_loss: 1.5051\n",
      "Epoch 5361/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.1352 - val_loss: 1.4012\n",
      "Epoch 5362/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4025 - val_loss: 1.4789\n",
      "Epoch 5363/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2407 - val_loss: 1.4464\n",
      "Epoch 5364/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.0124 - val_loss: 1.3218\n",
      "Epoch 5365/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.0834 - val_loss: 1.2921\n",
      "Epoch 5366/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.4905 - val_loss: 1.4921\n",
      "Epoch 5367/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0792 - val_loss: 1.5022\n",
      "Epoch 5368/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6826 - val_loss: 1.5669\n",
      "Epoch 5369/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.8361 - val_loss: 1.4178\n",
      "Epoch 5370/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3893 - val_loss: 1.3577\n",
      "Epoch 5371/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2339 - val_loss: 1.3848\n",
      "Epoch 5372/10000\n",
      "90/90 [==============================] - 0s 206us/step - loss: 9.2337 - val_loss: 1.4819\n",
      "Epoch 5373/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.4273 - val_loss: 1.4928\n",
      "Epoch 5374/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5731 - val_loss: 1.4684\n",
      "Epoch 5375/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4739 - val_loss: 1.3655\n",
      "Epoch 5376/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4214 - val_loss: 1.3446\n",
      "Epoch 5377/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2346 - val_loss: 1.2909\n",
      "Epoch 5378/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.3926 - val_loss: 1.3401\n",
      "Epoch 5379/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1053 - val_loss: 1.4526\n",
      "Epoch 5380/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.4974 - val_loss: 1.5263\n",
      "Epoch 5381/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8505 - val_loss: 1.4243\n",
      "Epoch 5382/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1359 - val_loss: 1.3791\n",
      "Epoch 5383/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6653 - val_loss: 1.4645\n",
      "Epoch 5384/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.9744 - val_loss: 1.3882\n",
      "Epoch 5385/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1898 - val_loss: 1.4123\n",
      "Epoch 5386/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.4587 - val_loss: 1.4607\n",
      "Epoch 5387/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5618 - val_loss: 1.4367\n",
      "Epoch 5388/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6845 - val_loss: 1.4383\n",
      "Epoch 5389/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.1912 - val_loss: 1.3270\n",
      "Epoch 5390/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5252 - val_loss: 1.3723\n",
      "Epoch 5391/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.6851 - val_loss: 1.4108\n",
      "Epoch 5392/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.6159 - val_loss: 1.5416\n",
      "Epoch 5393/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.2811 - val_loss: 1.4776\n",
      "Epoch 5394/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.7370 - val_loss: 1.5512\n",
      "Epoch 5395/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0737 - val_loss: 1.3736\n",
      "Epoch 5396/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.8857 - val_loss: 1.3036\n",
      "Epoch 5397/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4418 - val_loss: 1.4125\n",
      "Epoch 5398/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5644 - val_loss: 1.5136\n",
      "Epoch 5399/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3286 - val_loss: 1.5613\n",
      "Epoch 5400/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7087 - val_loss: 1.5318\n",
      "Epoch 5401/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.3483 - val_loss: 1.4073\n",
      "Epoch 5402/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.2323 - val_loss: 1.3264\n",
      "Epoch 5403/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 9.3402 - val_loss: 1.4923\n",
      "Epoch 5404/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.2405 - val_loss: 1.5888\n",
      "Epoch 5405/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3739 - val_loss: 1.5750\n",
      "Epoch 5406/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.1217 - val_loss: 1.4781\n",
      "Epoch 5407/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4937 - val_loss: 1.3835\n",
      "Epoch 5408/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3892 - val_loss: 1.2956\n",
      "Epoch 5409/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5510 - val_loss: 1.2884\n",
      "Epoch 5410/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3763 - val_loss: 1.4572\n",
      "Epoch 5411/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.9247 - val_loss: 1.4554\n",
      "Epoch 5412/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6970 - val_loss: 1.4776\n",
      "Epoch 5413/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.0543 - val_loss: 1.4328\n",
      "Epoch 5414/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.7402 - val_loss: 1.4069\n",
      "Epoch 5415/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.1303 - val_loss: 1.3625\n",
      "Epoch 5416/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.6519 - val_loss: 1.4723\n",
      "Epoch 5417/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.9349 - val_loss: 1.5746\n",
      "Epoch 5418/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.0430 - val_loss: 1.5205\n",
      "Epoch 5419/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.9538 - val_loss: 1.3340\n",
      "Epoch 5420/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6003 - val_loss: 1.4568\n",
      "Epoch 5421/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.1748 - val_loss: 1.4177\n",
      "Epoch 5422/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.9891 - val_loss: 1.4255\n",
      "Epoch 5423/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5637 - val_loss: 1.6099\n",
      "Epoch 5424/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.7385 - val_loss: 1.4906\n",
      "Epoch 5425/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.4849 - val_loss: 1.3864\n",
      "Epoch 5426/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5665 - val_loss: 1.4057\n",
      "Epoch 5427/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3205 - val_loss: 1.3797\n",
      "Epoch 5428/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7035 - val_loss: 1.3995\n",
      "Epoch 5429/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3329 - val_loss: 1.4139\n",
      "Epoch 5430/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.3822 - val_loss: 1.5443\n",
      "Epoch 5431/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2793 - val_loss: 1.5750\n",
      "Epoch 5432/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4831 - val_loss: 1.5323\n",
      "Epoch 5433/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4189 - val_loss: 1.4551\n",
      "Epoch 5434/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3213 - val_loss: 1.5581\n",
      "Epoch 5435/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.8972 - val_loss: 1.5625\n",
      "Epoch 5436/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.6267 - val_loss: 1.4251\n",
      "Epoch 5437/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1428 - val_loss: 1.3312\n",
      "Epoch 5438/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.9608 - val_loss: 1.3352\n",
      "Epoch 5439/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5296 - val_loss: 1.4713\n",
      "Epoch 5440/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2701 - val_loss: 1.4021\n",
      "Epoch 5441/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.9296 - val_loss: 1.4737\n",
      "Epoch 5442/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.1735 - val_loss: 1.3549\n",
      "Epoch 5443/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3608 - val_loss: 1.4629\n",
      "Epoch 5444/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3701 - val_loss: 1.5607\n",
      "Epoch 5445/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2714 - val_loss: 1.5611\n",
      "Epoch 5446/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4398 - val_loss: 1.4862\n",
      "Epoch 5447/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.0942 - val_loss: 1.3465\n",
      "Epoch 5448/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.1893 - val_loss: 1.4083\n",
      "Epoch 5449/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.6920 - val_loss: 1.5978\n",
      "Epoch 5450/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.9013 - val_loss: 1.6507\n",
      "Epoch 5451/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.5975 - val_loss: 1.5535\n",
      "Epoch 5452/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.6170 - val_loss: 1.4076\n",
      "Epoch 5453/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4047 - val_loss: 1.3120\n",
      "Epoch 5454/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.9181 - val_loss: 1.3418\n",
      "Epoch 5455/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.4271 - val_loss: 1.4997\n",
      "Epoch 5456/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.7574 - val_loss: 1.5952\n",
      "Epoch 5457/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0243 - val_loss: 1.3459\n",
      "Epoch 5458/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.0897 - val_loss: 1.3069\n",
      "Epoch 5459/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4854 - val_loss: 1.4510\n",
      "Epoch 5460/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.8530 - val_loss: 1.3643\n",
      "Epoch 5461/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.0188 - val_loss: 1.4794\n",
      "Epoch 5462/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1732 - val_loss: 1.5502\n",
      "Epoch 5463/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.0849 - val_loss: 1.5362\n",
      "Epoch 5464/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2931 - val_loss: 1.4661\n",
      "Epoch 5465/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2484 - val_loss: 1.4271\n",
      "Epoch 5466/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6518 - val_loss: 1.5654\n",
      "Epoch 5467/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2161 - val_loss: 1.5313\n",
      "Epoch 5468/10000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 9.5046 - val_loss: 1.4644\n",
      "Epoch 5469/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6398 - val_loss: 1.5091\n",
      "Epoch 5470/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.0767 - val_loss: 1.4165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5471/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.1545 - val_loss: 1.4130\n",
      "Epoch 5472/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4836 - val_loss: 1.4379\n",
      "Epoch 5473/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.1076 - val_loss: 1.5669\n",
      "Epoch 5474/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1348 - val_loss: 1.5416\n",
      "Epoch 5475/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 9.2146 - val_loss: 1.3848\n",
      "Epoch 5476/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2662 - val_loss: 1.4451\n",
      "Epoch 5477/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 9.1841 - val_loss: 1.5582\n",
      "Epoch 5478/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.1535 - val_loss: 1.4742\n",
      "Epoch 5479/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.2659 - val_loss: 1.3761\n",
      "Epoch 5480/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2451 - val_loss: 1.3947\n",
      "Epoch 5481/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.6222 - val_loss: 1.5447\n",
      "Epoch 5482/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2243 - val_loss: 1.4783\n",
      "Epoch 5483/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.5811 - val_loss: 1.4572\n",
      "Epoch 5484/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3904 - val_loss: 1.4717\n",
      "Epoch 5485/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3403 - val_loss: 1.4362\n",
      "Epoch 5486/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1074 - val_loss: 1.4834\n",
      "Epoch 5487/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0740 - val_loss: 1.4874\n",
      "Epoch 5488/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.5514 - val_loss: 1.4774\n",
      "Epoch 5489/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9053 - val_loss: 1.4405\n",
      "Epoch 5490/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3912 - val_loss: 1.4216\n",
      "Epoch 5491/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1625 - val_loss: 1.4581\n",
      "Epoch 5492/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.4321 - val_loss: 1.4106\n",
      "Epoch 5493/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1255 - val_loss: 1.4645\n",
      "Epoch 5494/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5789 - val_loss: 1.5702\n",
      "Epoch 5495/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3675 - val_loss: 1.5610\n",
      "Epoch 5496/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5516 - val_loss: 1.4896\n",
      "Epoch 5497/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.0939 - val_loss: 1.5183\n",
      "Epoch 5498/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.3102 - val_loss: 1.3904\n",
      "Epoch 5499/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.2448 - val_loss: 1.4192\n",
      "Epoch 5500/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0738 - val_loss: 1.4010\n",
      "Epoch 5501/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 9.1918 - val_loss: 1.4386\n",
      "Epoch 5502/10000\n",
      "90/90 [==============================] - 0s 164us/step - loss: 8.9241 - val_loss: 1.3930\n",
      "Epoch 5503/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 9.2646 - val_loss: 1.4848\n",
      "Epoch 5504/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.2892 - val_loss: 1.5482\n",
      "Epoch 5505/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 9.3867 - val_loss: 1.5357\n",
      "Epoch 5506/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.6723 - val_loss: 1.5472\n",
      "Epoch 5507/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1119 - val_loss: 1.5043\n",
      "Epoch 5508/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3725 - val_loss: 1.5203\n",
      "Epoch 5509/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2685 - val_loss: 1.4717\n",
      "Epoch 5510/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.8374 - val_loss: 1.3347\n",
      "Epoch 5511/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.8832 - val_loss: 1.3928\n",
      "Epoch 5512/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.4414 - val_loss: 1.4706\n",
      "Epoch 5513/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.9024 - val_loss: 1.4339\n",
      "Epoch 5514/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3578 - val_loss: 1.4759\n",
      "Epoch 5515/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4177 - val_loss: 1.5354\n",
      "Epoch 5516/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.3386 - val_loss: 1.5073\n",
      "Epoch 5517/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0927 - val_loss: 1.5320\n",
      "Epoch 5518/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3004 - val_loss: 1.5370\n",
      "Epoch 5519/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4237 - val_loss: 1.5905\n",
      "Epoch 5520/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5354 - val_loss: 1.5377\n",
      "Epoch 5521/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2838 - val_loss: 1.4231\n",
      "Epoch 5522/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.8158 - val_loss: 1.3425\n",
      "Epoch 5523/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2836 - val_loss: 1.4156\n",
      "Epoch 5524/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4177 - val_loss: 1.4909\n",
      "Epoch 5525/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.0614 - val_loss: 1.4563\n",
      "Epoch 5526/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.3954 - val_loss: 1.5156\n",
      "Epoch 5527/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.9354 - val_loss: 1.4304\n",
      "Epoch 5528/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4069 - val_loss: 1.4883\n",
      "Epoch 5529/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2448 - val_loss: 1.5500\n",
      "Epoch 5530/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.4195 - val_loss: 1.5215\n",
      "Epoch 5531/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3193 - val_loss: 1.5484\n",
      "Epoch 5532/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5324 - val_loss: 1.5019\n",
      "Epoch 5533/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3079 - val_loss: 1.4510\n",
      "Epoch 5534/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1008 - val_loss: 1.4146\n",
      "Epoch 5535/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4443 - val_loss: 1.3849\n",
      "Epoch 5536/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.0416 - val_loss: 1.4620\n",
      "Epoch 5537/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4460 - val_loss: 1.4787\n",
      "Epoch 5538/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3789 - val_loss: 1.5866\n",
      "Epoch 5539/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2605 - val_loss: 1.6384\n",
      "Epoch 5540/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0392 - val_loss: 1.5651\n",
      "Epoch 5541/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.0289 - val_loss: 1.4386\n",
      "Epoch 5542/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4005 - val_loss: 1.4402\n",
      "Epoch 5543/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.2383 - val_loss: 1.4346\n",
      "Epoch 5544/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.7573 - val_loss: 1.5168\n",
      "Epoch 5545/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.8845 - val_loss: 1.4682\n",
      "Epoch 5546/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.9033 - val_loss: 1.5561\n",
      "Epoch 5547/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.1402 - val_loss: 1.4584\n",
      "Epoch 5548/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1590 - val_loss: 1.4891\n",
      "Epoch 5549/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2910 - val_loss: 1.5874\n",
      "Epoch 5550/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4648 - val_loss: 1.6031\n",
      "Epoch 5551/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.9299 - val_loss: 1.4547\n",
      "Epoch 5552/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.1036 - val_loss: 1.5061\n",
      "Epoch 5553/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.1996 - val_loss: 1.3802\n",
      "Epoch 5554/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3818 - val_loss: 1.5007\n",
      "Epoch 5555/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.6516 - val_loss: 1.5940\n",
      "Epoch 5556/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2898 - val_loss: 1.4862\n",
      "Epoch 5557/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6154 - val_loss: 1.4769\n",
      "Epoch 5558/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2679 - val_loss: 1.4521\n",
      "Epoch 5559/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.5420 - val_loss: 1.5137\n",
      "Epoch 5560/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.1165 - val_loss: 1.4384\n",
      "Epoch 5561/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1367 - val_loss: 1.4435\n",
      "Epoch 5562/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.4784 - val_loss: 1.4645\n",
      "Epoch 5563/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5985 - val_loss: 1.4715\n",
      "Epoch 5564/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.1818 - val_loss: 1.5097\n",
      "Epoch 5565/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1340 - val_loss: 1.4829\n",
      "Epoch 5566/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.9887 - val_loss: 1.4282\n",
      "Epoch 5567/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.4893 - val_loss: 1.4893\n",
      "Epoch 5568/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.2165 - val_loss: 1.5530\n",
      "Epoch 5569/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3970 - val_loss: 1.5878\n",
      "Epoch 5570/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1585 - val_loss: 1.6259\n",
      "Epoch 5571/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5573 - val_loss: 1.6537\n",
      "Epoch 5572/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.1486 - val_loss: 1.4475\n",
      "Epoch 5573/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.9473 - val_loss: 1.3979\n",
      "Epoch 5574/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0290 - val_loss: 1.4448\n",
      "Epoch 5575/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3265 - val_loss: 1.5754\n",
      "Epoch 5576/10000\n",
      "90/90 [==============================] - 0s 213us/step - loss: 9.3841 - val_loss: 1.5681\n",
      "Epoch 5577/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.1575 - val_loss: 1.4358\n",
      "Epoch 5578/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.9796 - val_loss: 1.4687\n",
      "Epoch 5579/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 10.0167 - val_loss: 1.6631\n",
      "Epoch 5580/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3366 - val_loss: 1.5661\n",
      "Epoch 5581/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.9386 - val_loss: 1.5056\n",
      "Epoch 5582/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.7564 - val_loss: 1.5859\n",
      "Epoch 5583/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4080 - val_loss: 1.5119\n",
      "Epoch 5584/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0787 - val_loss: 1.5137\n",
      "Epoch 5585/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.0117 - val_loss: 1.4786\n",
      "Epoch 5586/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.2200 - val_loss: 1.4537\n",
      "Epoch 5587/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.0061 - val_loss: 1.4988\n",
      "Epoch 5588/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.4410 - val_loss: 1.5296\n",
      "Epoch 5589/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.8843 - val_loss: 1.5152\n",
      "Epoch 5590/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.2876 - val_loss: 1.4693\n",
      "Epoch 5591/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 9.5619 - val_loss: 1.5075\n",
      "Epoch 5592/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 10.0233 - val_loss: 1.6832\n",
      "Epoch 5593/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.1667 - val_loss: 1.5498\n",
      "Epoch 5594/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9679 - val_loss: 1.5548\n",
      "Epoch 5595/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.9593 - val_loss: 1.3885\n",
      "Epoch 5596/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.9164 - val_loss: 1.5100\n",
      "Epoch 5597/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.0956 - val_loss: 1.5314\n",
      "Epoch 5598/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5475 - val_loss: 1.5133\n",
      "Epoch 5599/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.5641 - val_loss: 1.5511\n",
      "Epoch 5600/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.5333 - val_loss: 1.5638\n",
      "Epoch 5601/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3783 - val_loss: 1.5214\n",
      "Epoch 5602/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0649 - val_loss: 1.4017\n",
      "Epoch 5603/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.0137 - val_loss: 1.4368\n",
      "Epoch 5604/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3535 - val_loss: 1.5660\n",
      "Epoch 5605/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4026 - val_loss: 1.5807\n",
      "Epoch 5606/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.1959 - val_loss: 1.4653\n",
      "Epoch 5607/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.2799 - val_loss: 1.4277\n",
      "Epoch 5608/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.3144 - val_loss: 1.4191\n",
      "Epoch 5609/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1733 - val_loss: 1.4995\n",
      "Epoch 5610/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.1270 - val_loss: 1.4863\n",
      "Epoch 5611/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.0790 - val_loss: 1.5874\n",
      "Epoch 5612/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.7548 - val_loss: 1.5590\n",
      "Epoch 5613/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.0381 - val_loss: 1.4857\n",
      "Epoch 5614/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.1355 - val_loss: 1.4986\n",
      "Epoch 5615/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1147 - val_loss: 1.5545\n",
      "Epoch 5616/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1552 - val_loss: 1.5970\n",
      "Epoch 5617/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.3959 - val_loss: 1.4922\n",
      "Epoch 5618/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9758 - val_loss: 1.4877\n",
      "Epoch 5619/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.4704 - val_loss: 1.5448\n",
      "Epoch 5620/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4316 - val_loss: 1.5834\n",
      "Epoch 5621/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 9.3111 - val_loss: 1.5534\n",
      "Epoch 5622/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2701 - val_loss: 1.5317\n",
      "Epoch 5623/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.3612 - val_loss: 1.4494\n",
      "Epoch 5624/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.1022 - val_loss: 1.4991\n",
      "Epoch 5625/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.5376 - val_loss: 1.5899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5626/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2599 - val_loss: 1.5880\n",
      "Epoch 5627/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4213 - val_loss: 1.5004\n",
      "Epoch 5628/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.0485 - val_loss: 1.5587\n",
      "Epoch 5629/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0750 - val_loss: 1.5494\n",
      "Epoch 5630/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.9182 - val_loss: 1.5124\n",
      "Epoch 5631/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.3101 - val_loss: 1.5029\n",
      "Epoch 5632/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 9.4434 - val_loss: 1.4978\n",
      "Epoch 5633/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.0626 - val_loss: 1.4455\n",
      "Epoch 5634/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3993 - val_loss: 1.5345\n",
      "Epoch 5635/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.4008 - val_loss: 1.5748\n",
      "Epoch 5636/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2659 - val_loss: 1.5519\n",
      "Epoch 5637/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.6647 - val_loss: 1.6237\n",
      "Epoch 5638/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.4199 - val_loss: 1.5922\n",
      "Epoch 5639/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.9992 - val_loss: 1.5004\n",
      "Epoch 5640/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.2215 - val_loss: 1.4806\n",
      "Epoch 5641/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0653 - val_loss: 1.4785\n",
      "Epoch 5642/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.6018 - val_loss: 1.5074\n",
      "Epoch 5643/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.9621 - val_loss: 1.5203\n",
      "Epoch 5644/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5995 - val_loss: 1.5088\n",
      "Epoch 5645/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.2598 - val_loss: 1.6026\n",
      "Epoch 5646/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0215 - val_loss: 1.5737\n",
      "Epoch 5647/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.9671 - val_loss: 1.6071\n",
      "Epoch 5648/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2373 - val_loss: 1.5767\n",
      "Epoch 5649/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.0226 - val_loss: 1.5329\n",
      "Epoch 5650/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.5659 - val_loss: 1.5611\n",
      "Epoch 5651/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0442 - val_loss: 1.5100\n",
      "Epoch 5652/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.5981 - val_loss: 1.5374\n",
      "Epoch 5653/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.9140 - val_loss: 1.5856\n",
      "Epoch 5654/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.2773 - val_loss: 1.5882\n",
      "Epoch 5655/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.2514 - val_loss: 1.5581\n",
      "Epoch 5656/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4383 - val_loss: 1.5674\n",
      "Epoch 5657/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.6248 - val_loss: 1.5635\n",
      "Epoch 5658/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.3214 - val_loss: 1.4604\n",
      "Epoch 5659/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.0843 - val_loss: 1.4395\n",
      "Epoch 5660/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.7270 - val_loss: 1.3440\n",
      "Epoch 5661/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.0499 - val_loss: 1.5080\n",
      "Epoch 5662/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.1926 - val_loss: 1.5640\n",
      "Epoch 5663/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.2499 - val_loss: 1.6206\n",
      "Epoch 5664/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.9534 - val_loss: 1.5455\n",
      "Epoch 5665/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.7396 - val_loss: 1.6334\n",
      "Epoch 5666/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4036 - val_loss: 1.5706\n",
      "Epoch 5667/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3363 - val_loss: 1.5379\n",
      "Epoch 5668/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.7321 - val_loss: 1.5340\n",
      "Epoch 5669/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.1240 - val_loss: 1.4591\n",
      "Epoch 5670/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.2427 - val_loss: 1.4897\n",
      "Epoch 5671/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.2707 - val_loss: 1.4931\n",
      "Epoch 5672/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.9554 - val_loss: 1.4882\n",
      "Epoch 5673/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.5684 - val_loss: 1.5988\n",
      "Epoch 5674/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.2780 - val_loss: 1.5633\n",
      "Epoch 5675/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.1878 - val_loss: 1.5159\n",
      "Epoch 5676/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.4466 - val_loss: 1.5301\n",
      "Epoch 5677/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.1217 - val_loss: 1.5311\n",
      "Epoch 5678/10000\n",
      "90/90 [==============================] - 0s 275us/step - loss: 9.1618 - val_loss: 1.5906\n",
      "Epoch 5679/10000\n",
      "90/90 [==============================] - 0s 291us/step - loss: 8.9078 - val_loss: 1.6029\n",
      "Epoch 5680/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 9.0910 - val_loss: 1.5785\n",
      "Epoch 5681/10000\n",
      "90/90 [==============================] - 0s 256us/step - loss: 9.3132 - val_loss: 1.5733\n",
      "Epoch 5682/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.4752 - val_loss: 1.5067\n",
      "Epoch 5683/10000\n",
      "90/90 [==============================] - 0s 180us/step - loss: 8.7379 - val_loss: 1.5172\n",
      "Epoch 5684/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 9.1365 - val_loss: 1.5393\n",
      "Epoch 5685/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.9749 - val_loss: 1.5485\n",
      "Epoch 5686/10000\n",
      "90/90 [==============================] - 0s 235us/step - loss: 9.0369 - val_loss: 1.5603\n",
      "Epoch 5687/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 8.7196 - val_loss: 1.4756\n",
      "Epoch 5688/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 8.9829 - val_loss: 1.4747\n",
      "Epoch 5689/10000\n",
      "90/90 [==============================] - 0s 211us/step - loss: 9.5263 - val_loss: 1.6124\n",
      "Epoch 5690/10000\n",
      "90/90 [==============================] - 0s 192us/step - loss: 8.5366 - val_loss: 1.5474\n",
      "Epoch 5691/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 9.4012 - val_loss: 1.6213\n",
      "Epoch 5692/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 8.9352 - val_loss: 1.5996\n",
      "Epoch 5693/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.3232 - val_loss: 1.6532\n",
      "Epoch 5694/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 8.6540 - val_loss: 1.6290\n",
      "Epoch 5695/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 9.1688 - val_loss: 1.5734\n",
      "Epoch 5696/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 8.9130 - val_loss: 1.5967\n",
      "Epoch 5697/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.5054 - val_loss: 1.6330\n",
      "Epoch 5698/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.8976 - val_loss: 1.5635\n",
      "Epoch 5699/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 9.2231 - val_loss: 1.6387\n",
      "Epoch 5700/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.6246 - val_loss: 1.5395\n",
      "Epoch 5701/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.2319 - val_loss: 1.5833\n",
      "Epoch 5702/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.1537 - val_loss: 1.5200\n",
      "Epoch 5703/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.0123 - val_loss: 1.4364\n",
      "Epoch 5704/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.1466 - val_loss: 1.4745\n",
      "Epoch 5705/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0208 - val_loss: 1.6195\n",
      "Epoch 5706/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.3011 - val_loss: 1.7152\n",
      "Epoch 5707/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.1773 - val_loss: 1.6119\n",
      "Epoch 5708/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.3227 - val_loss: 1.5812\n",
      "Epoch 5709/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.8561 - val_loss: 1.4948\n",
      "Epoch 5710/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.9072 - val_loss: 1.5166\n",
      "Epoch 5711/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0810 - val_loss: 1.4758\n",
      "Epoch 5712/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 10.0870 - val_loss: 1.7379\n",
      "Epoch 5713/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.3513 - val_loss: 1.6031\n",
      "Epoch 5714/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.1316 - val_loss: 1.5112\n",
      "Epoch 5715/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.4580 - val_loss: 1.5251\n",
      "Epoch 5716/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8120 - val_loss: 1.4745\n",
      "Epoch 5717/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 9.2736 - val_loss: 1.5805\n",
      "Epoch 5718/10000\n",
      "90/90 [==============================] - 0s 212us/step - loss: 8.9761 - val_loss: 1.6250\n",
      "Epoch 5719/10000\n",
      "90/90 [==============================] - 0s 293us/step - loss: 9.2065 - val_loss: 1.5834\n",
      "Epoch 5720/10000\n",
      "90/90 [==============================] - 0s 241us/step - loss: 9.1651 - val_loss: 1.5516\n",
      "Epoch 5721/10000\n",
      "90/90 [==============================] - 0s 190us/step - loss: 9.4554 - val_loss: 1.5470\n",
      "Epoch 5722/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 9.0439 - val_loss: 1.4951\n",
      "Epoch 5723/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 9.2872 - val_loss: 1.5533\n",
      "Epoch 5724/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.2449 - val_loss: 1.5456\n",
      "Epoch 5725/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.8480 - val_loss: 1.5243\n",
      "Epoch 5726/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.2179 - val_loss: 1.6323\n",
      "Epoch 5727/10000\n",
      "90/90 [==============================] - 0s 184us/step - loss: 9.5558 - val_loss: 1.7043\n",
      "Epoch 5728/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.9681 - val_loss: 1.5594\n",
      "Epoch 5729/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 9.2993 - val_loss: 1.5126\n",
      "Epoch 5730/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.6715 - val_loss: 1.5498\n",
      "Epoch 5731/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.2071 - val_loss: 1.5699\n",
      "Epoch 5732/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.0679 - val_loss: 1.5528\n",
      "Epoch 5733/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1145 - val_loss: 1.5665\n",
      "Epoch 5734/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.3271 - val_loss: 1.5898\n",
      "Epoch 5735/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 9.1856 - val_loss: 1.5860\n",
      "Epoch 5736/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 8.9212 - val_loss: 1.5218\n",
      "Epoch 5737/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.1081 - val_loss: 1.6387\n",
      "Epoch 5738/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.0858 - val_loss: 1.6632\n",
      "Epoch 5739/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.1583 - val_loss: 1.6282\n",
      "Epoch 5740/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 8.7154 - val_loss: 1.5401\n",
      "Epoch 5741/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.5141 - val_loss: 1.5038\n",
      "Epoch 5742/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.9591 - val_loss: 1.5692\n",
      "Epoch 5743/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 8.9574 - val_loss: 1.6127\n",
      "Epoch 5744/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.9468 - val_loss: 1.5630\n",
      "Epoch 5745/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.7197 - val_loss: 1.5795\n",
      "Epoch 5746/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.7954 - val_loss: 1.5597\n",
      "Epoch 5747/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.0735 - val_loss: 1.6983\n",
      "Epoch 5748/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.5179 - val_loss: 1.7175\n",
      "Epoch 5749/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2738 - val_loss: 1.5747\n",
      "Epoch 5750/10000\n",
      "90/90 [==============================] - 0s 223us/step - loss: 8.9930 - val_loss: 1.6192\n",
      "Epoch 5751/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1603 - val_loss: 1.6378\n",
      "Epoch 5752/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 9.2115 - val_loss: 1.6330\n",
      "Epoch 5753/10000\n",
      "90/90 [==============================] - 0s 171us/step - loss: 8.9838 - val_loss: 1.5211\n",
      "Epoch 5754/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 9.2429 - val_loss: 1.5296\n",
      "Epoch 5755/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 8.9819 - val_loss: 1.4872\n",
      "Epoch 5756/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 8.9452 - val_loss: 1.4839\n",
      "Epoch 5757/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.7827 - val_loss: 1.5426\n",
      "Epoch 5758/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.8480 - val_loss: 1.6409\n",
      "Epoch 5759/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.0699 - val_loss: 1.7000\n",
      "Epoch 5760/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.1360 - val_loss: 1.6727\n",
      "Epoch 5761/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 9.3405 - val_loss: 1.5842\n",
      "Epoch 5762/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.0911 - val_loss: 1.6215\n",
      "Epoch 5763/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.3240 - val_loss: 1.5744\n",
      "Epoch 5764/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 9.0318 - val_loss: 1.5437\n",
      "Epoch 5765/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 9.0708 - val_loss: 1.5475\n",
      "Epoch 5766/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 9.6080 - val_loss: 1.6099\n",
      "Epoch 5767/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 8.9212 - val_loss: 1.5607\n",
      "Epoch 5768/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 8.8099 - val_loss: 1.5256\n",
      "Epoch 5769/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 9.2074 - val_loss: 1.5944\n",
      "Epoch 5770/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 9.5015 - val_loss: 1.7387\n",
      "Epoch 5771/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 9.0188 - val_loss: 1.7030\n",
      "Epoch 5772/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 8.9302 - val_loss: 1.5751\n",
      "Epoch 5773/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 9.4039 - val_loss: 1.5747\n",
      "Epoch 5774/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 9.1304 - val_loss: 1.5342\n",
      "Epoch 5775/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 9.4954 - val_loss: 1.5657\n",
      "Epoch 5776/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.6658 - val_loss: 1.4666\n",
      "Epoch 5777/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.8457 - val_loss: 1.4380\n",
      "Epoch 5778/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.4154 - val_loss: 1.6833\n",
      "Epoch 5779/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 9.0736 - val_loss: 1.7698\n",
      "Epoch 5780/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 122us/step - loss: 8.8162 - val_loss: 1.6766\n",
      "Epoch 5781/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.0444 - val_loss: 1.5641\n",
      "Epoch 5782/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.7983 - val_loss: 1.6196\n",
      "Epoch 5783/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.1612 - val_loss: 1.6492\n",
      "Epoch 5784/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.4837 - val_loss: 1.6986\n",
      "Epoch 5785/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.0718 - val_loss: 1.6497\n",
      "Epoch 5786/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 9.2801 - val_loss: 1.6075\n",
      "Epoch 5787/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 9.4030 - val_loss: 1.5347\n",
      "Epoch 5788/10000\n",
      "90/90 [==============================] - 0s 215us/step - loss: 9.2042 - val_loss: 1.5420\n",
      "Epoch 5789/10000\n",
      "90/90 [==============================] - 0s 413us/step - loss: 8.8890 - val_loss: 1.4900\n",
      "Epoch 5790/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 9.5407 - val_loss: 1.6353\n",
      "Epoch 5791/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.8845 - val_loss: 1.6283\n",
      "Epoch 5792/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.0327 - val_loss: 1.6439\n",
      "Epoch 5793/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.3280 - val_loss: 1.6530\n",
      "Epoch 5794/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 8.7583 - val_loss: 1.5623\n",
      "Epoch 5795/10000\n",
      "90/90 [==============================] - 0s 202us/step - loss: 9.1741 - val_loss: 1.5751\n",
      "Epoch 5796/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 9.1404 - val_loss: 1.5048\n",
      "Epoch 5797/10000\n",
      "90/90 [==============================] - 0s 231us/step - loss: 8.8921 - val_loss: 1.5466\n",
      "Epoch 5798/10000\n",
      "90/90 [==============================] - 0s 217us/step - loss: 8.8015 - val_loss: 1.5632\n",
      "Epoch 5799/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.9490 - val_loss: 1.7035\n",
      "Epoch 5800/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 8.8369 - val_loss: 1.6464\n",
      "Epoch 5801/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 9.2347 - val_loss: 1.6716\n",
      "Epoch 5802/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.8947 - val_loss: 1.6040\n",
      "Epoch 5803/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 9.0972 - val_loss: 1.5968\n",
      "Epoch 5804/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 9.0145 - val_loss: 1.5281\n",
      "Epoch 5805/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.2993 - val_loss: 1.6404\n",
      "Epoch 5806/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.0091 - val_loss: 1.5880\n",
      "Epoch 5807/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.4121 - val_loss: 1.6387\n",
      "Epoch 5808/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.0855 - val_loss: 1.6357\n",
      "Epoch 5809/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.1864 - val_loss: 1.7299\n",
      "Epoch 5810/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.9963 - val_loss: 1.6949\n",
      "Epoch 5811/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.1315 - val_loss: 1.6019\n",
      "Epoch 5812/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1729 - val_loss: 1.6075\n",
      "Epoch 5813/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.9661 - val_loss: 1.6511\n",
      "Epoch 5814/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.8898 - val_loss: 1.6650\n",
      "Epoch 5815/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.1087 - val_loss: 1.6567\n",
      "Epoch 5816/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.8995 - val_loss: 1.5690\n",
      "Epoch 5817/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.2923 - val_loss: 1.6590\n",
      "Epoch 5818/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.1567 - val_loss: 1.7207\n",
      "Epoch 5819/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.0180 - val_loss: 1.6336\n",
      "Epoch 5820/10000\n",
      "90/90 [==============================] - 0s 281us/step - loss: 9.2661 - val_loss: 1.6043\n",
      "Epoch 5821/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 9.3734 - val_loss: 1.6035\n",
      "Epoch 5822/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.1470 - val_loss: 1.6902\n",
      "Epoch 5823/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.0233 - val_loss: 1.6242\n",
      "Epoch 5824/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.1204 - val_loss: 1.6218\n",
      "Epoch 5825/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 9.5132 - val_loss: 1.6126\n",
      "Epoch 5826/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 8.6778 - val_loss: 1.6088\n",
      "Epoch 5827/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 9.4496 - val_loss: 1.6711\n",
      "Epoch 5828/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.7612 - val_loss: 1.5970\n",
      "Epoch 5829/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.0778 - val_loss: 1.6660\n",
      "Epoch 5830/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 9.1921 - val_loss: 1.7156\n",
      "Epoch 5831/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 9.2121 - val_loss: 1.6224\n",
      "Epoch 5832/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.9829 - val_loss: 1.6206\n",
      "Epoch 5833/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 8.9740 - val_loss: 1.6354\n",
      "Epoch 5834/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 9.0517 - val_loss: 1.6025\n",
      "Epoch 5835/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 9.3457 - val_loss: 1.5960\n",
      "Epoch 5836/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 9.2895 - val_loss: 1.6117\n",
      "Epoch 5837/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.3108 - val_loss: 1.6092\n",
      "Epoch 5838/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.2931 - val_loss: 1.6327\n",
      "Epoch 5839/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 9.0003 - val_loss: 1.5672\n",
      "Epoch 5840/10000\n",
      "90/90 [==============================] - 0s 184us/step - loss: 9.0577 - val_loss: 1.5767\n",
      "Epoch 5841/10000\n",
      "90/90 [==============================] - 0s 194us/step - loss: 9.0999 - val_loss: 1.6595\n",
      "Epoch 5842/10000\n",
      "90/90 [==============================] - 0s 224us/step - loss: 8.9908 - val_loss: 1.6166\n",
      "Epoch 5843/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2429 - val_loss: 1.6697\n",
      "Epoch 5844/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.8323 - val_loss: 1.6629\n",
      "Epoch 5845/10000\n",
      "90/90 [==============================] - 0s 191us/step - loss: 8.8878 - val_loss: 1.6089\n",
      "Epoch 5846/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1155 - val_loss: 1.7059\n",
      "Epoch 5847/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.9314 - val_loss: 1.7990\n",
      "Epoch 5848/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.0646 - val_loss: 1.7265\n",
      "Epoch 5849/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.2141 - val_loss: 1.5554\n",
      "Epoch 5850/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.9420 - val_loss: 1.5490\n",
      "Epoch 5851/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.4083 - val_loss: 1.5917\n",
      "Epoch 5852/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.0024 - val_loss: 1.5960\n",
      "Epoch 5853/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.7283 - val_loss: 1.7265\n",
      "Epoch 5854/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.9467 - val_loss: 1.6574\n",
      "Epoch 5855/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.1727 - val_loss: 1.7422\n",
      "Epoch 5856/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.7983 - val_loss: 1.6237\n",
      "Epoch 5857/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0124 - val_loss: 1.7062\n",
      "Epoch 5858/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.9142 - val_loss: 1.7318\n",
      "Epoch 5859/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.3631 - val_loss: 1.7619\n",
      "Epoch 5860/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1543 - val_loss: 1.6811\n",
      "Epoch 5861/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.4689 - val_loss: 1.6400\n",
      "Epoch 5862/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8476 - val_loss: 1.5830\n",
      "Epoch 5863/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.4000 - val_loss: 1.4686\n",
      "Epoch 5864/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.1680 - val_loss: 1.6090\n",
      "Epoch 5865/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0169 - val_loss: 1.7097\n",
      "Epoch 5866/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.2252 - val_loss: 1.7254\n",
      "Epoch 5867/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.0451 - val_loss: 1.6849\n",
      "Epoch 5868/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.0928 - val_loss: 1.6672\n",
      "Epoch 5869/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2321 - val_loss: 1.7360\n",
      "Epoch 5870/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.9930 - val_loss: 1.7197\n",
      "Epoch 5871/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.5101 - val_loss: 1.7646\n",
      "Epoch 5872/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.1105 - val_loss: 1.7140\n",
      "Epoch 5873/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.3008 - val_loss: 1.5688\n",
      "Epoch 5874/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.7467 - val_loss: 1.4518\n",
      "Epoch 5875/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.0742 - val_loss: 1.4956\n",
      "Epoch 5876/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 9.1182 - val_loss: 1.6252\n",
      "Epoch 5877/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.1221 - val_loss: 1.7056\n",
      "Epoch 5878/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.8670 - val_loss: 1.6795\n",
      "Epoch 5879/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 9.2941 - val_loss: 1.6813\n",
      "Epoch 5880/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 8.9251 - val_loss: 1.6180\n",
      "Epoch 5881/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.8547 - val_loss: 1.6275\n",
      "Epoch 5882/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.4300 - val_loss: 1.7289\n",
      "Epoch 5883/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.5575 - val_loss: 1.6299\n",
      "Epoch 5884/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 8.6834 - val_loss: 1.5232\n",
      "Epoch 5885/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 9.1520 - val_loss: 1.5229\n",
      "Epoch 5886/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.6014 - val_loss: 1.6327\n",
      "Epoch 5887/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.6501 - val_loss: 1.7712\n",
      "Epoch 5888/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.9088 - val_loss: 1.6938\n",
      "Epoch 5889/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.8682 - val_loss: 1.7355\n",
      "Epoch 5890/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.9844 - val_loss: 1.8254\n",
      "Epoch 5891/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.1869 - val_loss: 1.8067\n",
      "Epoch 5892/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 9.2228 - val_loss: 1.7270\n",
      "Epoch 5893/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2118 - val_loss: 1.6211\n",
      "Epoch 5894/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 8.8767 - val_loss: 1.5879\n",
      "Epoch 5895/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 9.0122 - val_loss: 1.6073\n",
      "Epoch 5896/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.2394 - val_loss: 1.6406\n",
      "Epoch 5897/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.9909 - val_loss: 1.5372\n",
      "Epoch 5898/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.8976 - val_loss: 1.5817\n",
      "Epoch 5899/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.6427 - val_loss: 1.6241\n",
      "Epoch 5900/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2557 - val_loss: 1.7310\n",
      "Epoch 5901/10000\n",
      "90/90 [==============================] - 0s 213us/step - loss: 9.1591 - val_loss: 1.7586\n",
      "Epoch 5902/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 9.1290 - val_loss: 1.7533\n",
      "Epoch 5903/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.7468 - val_loss: 1.7564\n",
      "Epoch 5904/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1468 - val_loss: 1.6624\n",
      "Epoch 5905/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.3208 - val_loss: 1.6248\n",
      "Epoch 5906/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.0302 - val_loss: 1.6398\n",
      "Epoch 5907/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 9.0346 - val_loss: 1.6748\n",
      "Epoch 5908/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 8.9200 - val_loss: 1.6776\n",
      "Epoch 5909/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 8.9092 - val_loss: 1.7289\n",
      "Epoch 5910/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 9.1679 - val_loss: 1.7821\n",
      "Epoch 5911/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 9.0356 - val_loss: 1.7367\n",
      "Epoch 5912/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.0684 - val_loss: 1.7322\n",
      "Epoch 5913/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 8.7869 - val_loss: 1.6515\n",
      "Epoch 5914/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 9.0912 - val_loss: 1.7512\n",
      "Epoch 5915/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.9420 - val_loss: 1.7489\n",
      "Epoch 5916/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 8.6714 - val_loss: 1.7025\n",
      "Epoch 5917/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 9.0427 - val_loss: 1.6959\n",
      "Epoch 5918/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.8586 - val_loss: 1.6262\n",
      "Epoch 5919/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.6887 - val_loss: 1.7775\n",
      "Epoch 5920/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.1106 - val_loss: 1.6729\n",
      "Epoch 5921/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.0191 - val_loss: 1.5761\n",
      "Epoch 5922/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.8100 - val_loss: 1.5426\n",
      "Epoch 5923/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8582 - val_loss: 1.5375\n",
      "Epoch 5924/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 8.5817 - val_loss: 1.6633\n",
      "Epoch 5925/10000\n",
      "90/90 [==============================] - 0s 198us/step - loss: 8.8161 - val_loss: 1.7355\n",
      "Epoch 5926/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 8.9895 - val_loss: 1.8216\n",
      "Epoch 5927/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 9.5585 - val_loss: 1.8222\n",
      "Epoch 5928/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.3691 - val_loss: 1.7475\n",
      "Epoch 5929/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.8333 - val_loss: 1.6362\n",
      "Epoch 5930/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.1839 - val_loss: 1.6813\n",
      "Epoch 5931/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.8691 - val_loss: 1.6635\n",
      "Epoch 5932/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9817 - val_loss: 1.6852\n",
      "Epoch 5933/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.3037 - val_loss: 1.6848\n",
      "Epoch 5934/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 103us/step - loss: 8.8994 - val_loss: 1.6488\n",
      "Epoch 5935/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5901 - val_loss: 1.6196\n",
      "Epoch 5936/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.2380 - val_loss: 1.7351\n",
      "Epoch 5937/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.9190 - val_loss: 1.7145\n",
      "Epoch 5938/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.0347 - val_loss: 1.7812\n",
      "Epoch 5939/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.8431 - val_loss: 1.7086\n",
      "Epoch 5940/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.5362 - val_loss: 1.7463\n",
      "Epoch 5941/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0469 - val_loss: 1.7752\n",
      "Epoch 5942/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.0274 - val_loss: 1.7567\n",
      "Epoch 5943/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.5635 - val_loss: 1.7410\n",
      "Epoch 5944/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.0321 - val_loss: 1.5880\n",
      "Epoch 5945/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.5250 - val_loss: 1.7118\n",
      "Epoch 5946/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.9966 - val_loss: 1.7096\n",
      "Epoch 5947/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.9028 - val_loss: 1.6903\n",
      "Epoch 5948/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.2929 - val_loss: 1.7594\n",
      "Epoch 5949/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 9.3931 - val_loss: 1.7592\n",
      "Epoch 5950/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.4858 - val_loss: 1.6951\n",
      "Epoch 5951/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.8358 - val_loss: 1.6863\n",
      "Epoch 5952/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.9126 - val_loss: 1.7120\n",
      "Epoch 5953/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.1085 - val_loss: 1.7546\n",
      "Epoch 5954/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.9846 - val_loss: 1.7538\n",
      "Epoch 5955/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0393 - val_loss: 1.6574\n",
      "Epoch 5956/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.9896 - val_loss: 1.6309\n",
      "Epoch 5957/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.9646 - val_loss: 1.6754\n",
      "Epoch 5958/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.7813 - val_loss: 1.7366\n",
      "Epoch 5959/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 8.7894 - val_loss: 1.7969\n",
      "Epoch 5960/10000\n",
      "90/90 [==============================] - 0s 168us/step - loss: 9.1701 - val_loss: 1.8258\n",
      "Epoch 5961/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 8.4995 - val_loss: 1.7162\n",
      "Epoch 5962/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 8.7400 - val_loss: 1.7076\n",
      "Epoch 5963/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.2938 - val_loss: 1.7935\n",
      "Epoch 5964/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 8.9308 - val_loss: 1.7315\n",
      "Epoch 5965/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.9224 - val_loss: 1.6929\n",
      "Epoch 5966/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 9.662 - 0s 119us/step - loss: 8.9538 - val_loss: 1.6703\n",
      "Epoch 5967/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.7086 - val_loss: 1.5630\n",
      "Epoch 5968/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.0658 - val_loss: 1.7170\n",
      "Epoch 5969/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.9666 - val_loss: 1.7803\n",
      "Epoch 5970/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.2346 - val_loss: 1.8750\n",
      "Epoch 5971/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 9.2686 - val_loss: 1.8199\n",
      "Epoch 5972/10000\n",
      "90/90 [==============================] - 0s 164us/step - loss: 8.7943 - val_loss: 1.7000\n",
      "Epoch 5973/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 8.8855 - val_loss: 1.6188\n",
      "Epoch 5974/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 8.9501 - val_loss: 1.6620\n",
      "Epoch 5975/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8995 - val_loss: 1.7656\n",
      "Epoch 5976/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 9.2978 - val_loss: 1.8640\n",
      "Epoch 5977/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.1051 - val_loss: 1.8287\n",
      "Epoch 5978/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 8.7919 - val_loss: 1.7175\n",
      "Epoch 5979/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.1228 - val_loss: 1.6554\n",
      "Epoch 5980/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 8.9852 - val_loss: 1.6199\n",
      "Epoch 5981/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.0225 - val_loss: 1.6222\n",
      "Epoch 5982/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.7333 - val_loss: 1.6316\n",
      "Epoch 5983/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.1508 - val_loss: 1.7252\n",
      "Epoch 5984/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.7808 - val_loss: 1.6916\n",
      "Epoch 5985/10000\n",
      "90/90 [==============================] - 0s 320us/step - loss: 8.5069 - val_loss: 1.7798\n",
      "Epoch 5986/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.0243 - val_loss: 1.8499\n",
      "Epoch 5987/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.4817 - val_loss: 1.8387\n",
      "Epoch 5988/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.2437 - val_loss: 1.8421\n",
      "Epoch 5989/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 9.0309 - val_loss: 1.7860\n",
      "Epoch 5990/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.6937 - val_loss: 1.6877\n",
      "Epoch 5991/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.0472 - val_loss: 1.7128\n",
      "Epoch 5992/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 9.0615 - val_loss: 1.6907\n",
      "Epoch 5993/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 9.1953 - val_loss: 1.7398\n",
      "Epoch 5994/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 9.2869 - val_loss: 1.7249\n",
      "Epoch 5995/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 8.4410 - val_loss: 1.6432\n",
      "Epoch 5996/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 8.8359 - val_loss: 1.6034\n",
      "Epoch 5997/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.7131 - val_loss: 1.7720\n",
      "Epoch 5998/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.9602 - val_loss: 1.7421\n",
      "Epoch 5999/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 9.2206 - val_loss: 1.8139\n",
      "Epoch 6000/10000\n",
      "90/90 [==============================] - 0s 196us/step - loss: 8.7010 - val_loss: 1.7621\n",
      "Epoch 6001/10000\n",
      "90/90 [==============================] - 0s 654us/step - loss: 9.1685 - val_loss: 1.7533\n",
      "Epoch 6002/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 8.8265 - val_loss: 1.7426\n",
      "Epoch 6003/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 9.3761 - val_loss: 1.8672\n",
      "Epoch 6004/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.8664 - val_loss: 1.8579\n",
      "Epoch 6005/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.8605 - val_loss: 1.7400\n",
      "Epoch 6006/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 9.5827 - val_loss: 1.7858\n",
      "Epoch 6007/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.0134 - val_loss: 1.6959\n",
      "Epoch 6008/10000\n",
      "90/90 [==============================] - 0s 180us/step - loss: 9.2430 - val_loss: 1.8062\n",
      "Epoch 6009/10000\n",
      "90/90 [==============================] - 0s 375us/step - loss: 9.2803 - val_loss: 1.8922\n",
      "Epoch 6010/10000\n",
      "90/90 [==============================] - 0s 311us/step - loss: 9.2919 - val_loss: 1.8775\n",
      "Epoch 6011/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 9.1007 - val_loss: 1.7293\n",
      "Epoch 6012/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 8.7984 - val_loss: 1.6238\n",
      "Epoch 6013/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 8.7999 - val_loss: 1.6612\n",
      "Epoch 6014/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.9996 - val_loss: 1.7717\n",
      "Epoch 6015/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.8189 - val_loss: 1.8343\n",
      "Epoch 6016/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 8.6410 - val_loss: 1.7748\n",
      "Epoch 6017/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 8.8491 - val_loss: 1.7928\n",
      "Epoch 6018/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 8.7080 - val_loss: 1.7564\n",
      "Epoch 6019/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.7380 - val_loss: 1.7247\n",
      "Epoch 6020/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 9.4996 - val_loss: 1.8717\n",
      "Epoch 6021/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 9.2487 - val_loss: 1.8568\n",
      "Epoch 6022/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.6303 - val_loss: 1.7423\n",
      "Epoch 6023/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 8.4534 - val_loss: 1.6915\n",
      "Epoch 6024/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 8.5787 - val_loss: 1.7052\n",
      "Epoch 6025/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.3212 - val_loss: 1.8739\n",
      "Epoch 6026/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.8972 - val_loss: 1.8577\n",
      "Epoch 6027/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.8364 - val_loss: 1.8513\n",
      "Epoch 6028/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 9.0180 - val_loss: 1.8322\n",
      "Epoch 6029/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.5662 - val_loss: 1.7763\n",
      "Epoch 6030/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 8.9651 - val_loss: 1.7393\n",
      "Epoch 6031/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.4016 - val_loss: 1.6951\n",
      "Epoch 6032/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.1356 - val_loss: 1.7651\n",
      "Epoch 6033/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.7201 - val_loss: 1.7214\n",
      "Epoch 6034/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.7855 - val_loss: 1.6689\n",
      "Epoch 6035/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.8513 - val_loss: 1.8281\n",
      "Epoch 6036/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.1055 - val_loss: 1.9507\n",
      "Epoch 6037/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 8.8179 - val_loss: 1.8912\n",
      "Epoch 6038/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 8.8828 - val_loss: 1.8270\n",
      "Epoch 6039/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 8.6443 - val_loss: 1.7423\n",
      "Epoch 6040/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.0062 - val_loss: 1.7476\n",
      "Epoch 6041/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.8529 - val_loss: 1.7019\n",
      "Epoch 6042/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 8.9014 - val_loss: 1.7218\n",
      "Epoch 6043/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.9583 - val_loss: 1.6795\n",
      "Epoch 6044/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.8751 - val_loss: 1.6946\n",
      "Epoch 6045/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.3997 - val_loss: 1.7956\n",
      "Epoch 6046/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.9425 - val_loss: 1.8599\n",
      "Epoch 6047/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8475 - val_loss: 1.9254\n",
      "Epoch 6048/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 9.1618 - val_loss: 1.9099\n",
      "Epoch 6049/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 8.8437 - val_loss: 1.8024\n",
      "Epoch 6050/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.7830 - val_loss: 1.7175\n",
      "Epoch 6051/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1059 - val_loss: 1.6306\n",
      "Epoch 6052/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 8.7496 - val_loss: 1.6699\n",
      "Epoch 6053/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.9483 - val_loss: 1.8764\n",
      "Epoch 6054/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.7938 - val_loss: 1.9275\n",
      "Epoch 6055/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 8.6191 - val_loss: 1.8255\n",
      "Epoch 6056/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.8187 - val_loss: 1.8118\n",
      "Epoch 6057/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.9139 - val_loss: 1.9254\n",
      "Epoch 6058/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.9554 - val_loss: 1.8674\n",
      "Epoch 6059/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 8.8035 - val_loss: 1.7687\n",
      "Epoch 6060/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8628 - val_loss: 1.6630\n",
      "Epoch 6061/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.9150 - val_loss: 1.6954\n",
      "Epoch 6062/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.0253 - val_loss: 1.7079\n",
      "Epoch 6063/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.2659 - val_loss: 1.8339\n",
      "Epoch 6064/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.5363 - val_loss: 1.7375\n",
      "Epoch 6065/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 9.0728 - val_loss: 1.7480\n",
      "Epoch 6066/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 9.2117 - val_loss: 1.7804\n",
      "Epoch 6067/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.0557 - val_loss: 1.9163\n",
      "Epoch 6068/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.5676 - val_loss: 1.8445\n",
      "Epoch 6069/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.4190 - val_loss: 1.8487\n",
      "Epoch 6070/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0744 - val_loss: 1.8856\n",
      "Epoch 6071/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 9.1620 - val_loss: 1.8943\n",
      "Epoch 6072/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.2701 - val_loss: 1.8061\n",
      "Epoch 6073/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.8033 - val_loss: 1.7171\n",
      "Epoch 6074/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.1736 - val_loss: 1.7273\n",
      "Epoch 6075/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 9.1910 - val_loss: 1.7821\n",
      "Epoch 6076/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.8282 - val_loss: 1.8061\n",
      "Epoch 6077/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.9250 - val_loss: 1.8006\n",
      "Epoch 6078/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.5642 - val_loss: 1.7723\n",
      "Epoch 6079/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.8013 - val_loss: 1.7954\n",
      "Epoch 6080/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.9705 - val_loss: 1.8622\n",
      "Epoch 6081/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 9.4740 - val_loss: 1.9145\n",
      "Epoch 6082/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.9508 - val_loss: 1.8283\n",
      "Epoch 6083/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.8502 - val_loss: 1.8151\n",
      "Epoch 6084/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.5708 - val_loss: 1.7640\n",
      "Epoch 6085/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1306 - val_loss: 1.8879\n",
      "Epoch 6086/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 9.1305 - val_loss: 1.8890\n",
      "Epoch 6087/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.8679 - val_loss: 1.9081\n",
      "Epoch 6088/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 103us/step - loss: 8.5931 - val_loss: 1.7744\n",
      "Epoch 6089/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 8.9016 - val_loss: 1.8116\n",
      "Epoch 6090/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.9070 - val_loss: 1.8434\n",
      "Epoch 6091/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.6305 - val_loss: 1.9945\n",
      "Epoch 6092/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.0942 - val_loss: 1.9310\n",
      "Epoch 6093/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.9355 - val_loss: 1.7515\n",
      "Epoch 6094/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.9691 - val_loss: 1.7058\n",
      "Epoch 6095/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.2234 - val_loss: 1.8424\n",
      "Epoch 6096/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.7588 - val_loss: 1.9663\n",
      "Epoch 6097/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1436 - val_loss: 1.9505\n",
      "Epoch 6098/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.5618 - val_loss: 1.8622\n",
      "Epoch 6099/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.7481 - val_loss: 1.7578\n",
      "Epoch 6100/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.0474 - val_loss: 1.8302\n",
      "Epoch 6101/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 9.1249 - val_loss: 1.8835\n",
      "Epoch 6102/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.7730 - val_loss: 1.7267\n",
      "Epoch 6103/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.2735 - val_loss: 1.6658\n",
      "Epoch 6104/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 9.0549 - val_loss: 1.7390\n",
      "Epoch 6105/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.8932 - val_loss: 1.8114\n",
      "Epoch 6106/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.6366 - val_loss: 1.9428\n",
      "Epoch 6107/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.0516 - val_loss: 1.9644\n",
      "Epoch 6108/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.9777 - val_loss: 1.9344\n",
      "Epoch 6109/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.6835 - val_loss: 1.8528\n",
      "Epoch 6110/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0527 - val_loss: 1.8657\n",
      "Epoch 6111/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 9.1683 - val_loss: 1.8680\n",
      "Epoch 6112/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.8637 - val_loss: 1.7854\n",
      "Epoch 6113/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 9.1152 - val_loss: 1.7514\n",
      "Epoch 6114/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 9.1111 - val_loss: 1.7713\n",
      "Epoch 6115/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5005 - val_loss: 1.7623\n",
      "Epoch 6116/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.8141 - val_loss: 1.8634\n",
      "Epoch 6117/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.8182 - val_loss: 1.8237\n",
      "Epoch 6118/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.7764 - val_loss: 1.9074\n",
      "Epoch 6119/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 9.1866 - val_loss: 1.9672\n",
      "Epoch 6120/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.0036 - val_loss: 1.9529\n",
      "Epoch 6121/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.0794 - val_loss: 1.9068\n",
      "Epoch 6122/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.9276 - val_loss: 1.9784\n",
      "Epoch 6123/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 9.0954 - val_loss: 1.9484\n",
      "Epoch 6124/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.7220 - val_loss: 1.8677\n",
      "Epoch 6125/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.4402 - val_loss: 1.7957\n",
      "Epoch 6126/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.0549 - val_loss: 1.8409\n",
      "Epoch 6127/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.1525 - val_loss: 1.8743\n",
      "Epoch 6128/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9075 - val_loss: 1.8929\n",
      "Epoch 6129/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.9383 - val_loss: 1.8255\n",
      "Epoch 6130/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.8962 - val_loss: 1.7508\n",
      "Epoch 6131/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6159 - val_loss: 1.7578\n",
      "Epoch 6132/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.7479 - val_loss: 1.7747\n",
      "Epoch 6133/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 9.1250 - val_loss: 1.8803\n",
      "Epoch 6134/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.8480 - val_loss: 1.9116\n",
      "Epoch 6135/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.9297 - val_loss: 1.8310\n",
      "Epoch 6136/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1901 - val_loss: 1.9416\n",
      "Epoch 6137/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.8618 - val_loss: 1.9655\n",
      "Epoch 6138/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.0113 - val_loss: 1.8139\n",
      "Epoch 6139/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.6483 - val_loss: 1.7946\n",
      "Epoch 6140/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.8673 - val_loss: 1.8540\n",
      "Epoch 6141/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.9501 - val_loss: 1.9035\n",
      "Epoch 6142/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.7371 - val_loss: 1.9041\n",
      "Epoch 6143/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.5947 - val_loss: 1.8709\n",
      "Epoch 6144/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.7797 - val_loss: 1.8342\n",
      "Epoch 6145/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.7986 - val_loss: 1.8593\n",
      "Epoch 6146/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6534 - val_loss: 1.9821\n",
      "Epoch 6147/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 9.1355 - val_loss: 2.0466\n",
      "Epoch 6148/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 9.1145 - val_loss: 1.9705\n",
      "Epoch 6149/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 9.4133 - val_loss: 1.8788\n",
      "Epoch 6150/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.6062 - val_loss: 1.7398\n",
      "Epoch 6151/10000\n",
      "90/90 [==============================] - 0s 161us/step - loss: 9.1862 - val_loss: 1.9134\n",
      "Epoch 6152/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.6182 - val_loss: 1.9325\n",
      "Epoch 6153/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.4776 - val_loss: 1.9022\n",
      "Epoch 6154/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.5428 - val_loss: 1.8090\n",
      "Epoch 6155/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 9.1898 - val_loss: 1.9170\n",
      "Epoch 6156/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.3983 - val_loss: 1.8810\n",
      "Epoch 6157/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.7198 - val_loss: 1.8953\n",
      "Epoch 6158/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.4680 - val_loss: 1.9056\n",
      "Epoch 6159/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.8404 - val_loss: 1.9185\n",
      "Epoch 6160/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.7018 - val_loss: 1.9794\n",
      "Epoch 6161/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.5160 - val_loss: 1.8958\n",
      "Epoch 6162/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.4791 - val_loss: 1.8550\n",
      "Epoch 6163/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.8225 - val_loss: 1.8733\n",
      "Epoch 6164/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.9982 - val_loss: 1.8949\n",
      "Epoch 6165/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.4619 - val_loss: 1.8038\n",
      "Epoch 6166/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.7602 - val_loss: 1.7990\n",
      "Epoch 6167/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.6681 - val_loss: 1.9108\n",
      "Epoch 6168/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.8644 - val_loss: 1.9679\n",
      "Epoch 6169/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.7321 - val_loss: 1.9705\n",
      "Epoch 6170/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.7140 - val_loss: 1.9251\n",
      "Epoch 6171/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.7659 - val_loss: 1.9421\n",
      "Epoch 6172/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5793 - val_loss: 1.8918\n",
      "Epoch 6173/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.3744 - val_loss: 1.8866\n",
      "Epoch 6174/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.6669 - val_loss: 1.8344\n",
      "Epoch 6175/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.9925 - val_loss: 1.9336\n",
      "Epoch 6176/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.7775 - val_loss: 2.0451\n",
      "Epoch 6177/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.8736 - val_loss: 1.9893\n",
      "Epoch 6178/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.2364 - val_loss: 1.8975\n",
      "Epoch 6179/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.4587 - val_loss: 1.8538\n",
      "Epoch 6180/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 9.3808 - val_loss: 1.9517\n",
      "Epoch 6181/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.5986 - val_loss: 1.8933\n",
      "Epoch 6182/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.5868 - val_loss: 1.9352\n",
      "Epoch 6183/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.7758 - val_loss: 1.9418\n",
      "Epoch 6184/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.7307 - val_loss: 1.9221\n",
      "Epoch 6185/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.4812 - val_loss: 1.8207\n",
      "Epoch 6186/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.9839 - val_loss: 2.0105\n",
      "Epoch 6187/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.4890 - val_loss: 2.0522\n",
      "Epoch 6188/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.8156 - val_loss: 2.1174\n",
      "Epoch 6189/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.5820 - val_loss: 2.0662\n",
      "Epoch 6190/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.1643 - val_loss: 1.9412\n",
      "Epoch 6191/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.5845 - val_loss: 1.8434\n",
      "Epoch 6192/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.2370 - val_loss: 1.8119\n",
      "Epoch 6193/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6731 - val_loss: 1.9679\n",
      "Epoch 6194/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.7086 - val_loss: 1.9819\n",
      "Epoch 6195/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.6298 - val_loss: 1.8957\n",
      "Epoch 6196/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.5309 - val_loss: 1.8222\n",
      "Epoch 6197/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.2875 - val_loss: 1.8435\n",
      "Epoch 6198/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.7252 - val_loss: 1.9425\n",
      "Epoch 6199/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.8664 - val_loss: 1.9804\n",
      "Epoch 6200/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.6288 - val_loss: 1.9700\n",
      "Epoch 6201/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.8081 - val_loss: 1.9003\n",
      "Epoch 6202/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.0583 - val_loss: 1.8536\n",
      "Epoch 6203/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 9.1401 - val_loss: 1.9012\n",
      "Epoch 6204/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 9.0846 - val_loss: 1.8798\n",
      "Epoch 6205/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5329 - val_loss: 1.7982\n",
      "Epoch 6206/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 9.0230 - val_loss: 1.8820\n",
      "Epoch 6207/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.9429 - val_loss: 2.1409\n",
      "Epoch 6208/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.8472 - val_loss: 2.1879\n",
      "Epoch 6209/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.0030 - val_loss: 2.1125\n",
      "Epoch 6210/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.0182 - val_loss: 2.0005\n",
      "Epoch 6211/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.8088 - val_loss: 1.9274\n",
      "Epoch 6212/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.7956 - val_loss: 1.8687\n",
      "Epoch 6213/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.9571 - val_loss: 1.8843\n",
      "Epoch 6214/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.6840 - val_loss: 1.9176\n",
      "Epoch 6215/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.6447 - val_loss: 1.9738\n",
      "Epoch 6216/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 9.1595 - val_loss: 1.9519\n",
      "Epoch 6217/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 9.0686 - val_loss: 1.9450\n",
      "Epoch 6218/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.5402 - val_loss: 1.9312\n",
      "Epoch 6219/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6545 - val_loss: 1.9426\n",
      "Epoch 6220/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.4246 - val_loss: 1.8954\n",
      "Epoch 6221/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 9.2539 - val_loss: 2.0182\n",
      "Epoch 6222/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.5337 - val_loss: 1.9703\n",
      "Epoch 6223/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.9592 - val_loss: 1.9322\n",
      "Epoch 6224/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.4262 - val_loss: 1.8487\n",
      "Epoch 6225/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9726 - val_loss: 1.9564\n",
      "Epoch 6226/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.7089 - val_loss: 1.9868\n",
      "Epoch 6227/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.6863 - val_loss: 2.1281\n",
      "Epoch 6228/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.5357 - val_loss: 2.0977\n",
      "Epoch 6229/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 8.6351 - val_loss: 1.9841\n",
      "Epoch 6230/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 8.5089 - val_loss: 1.9428\n",
      "Epoch 6231/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 8.8644 - val_loss: 1.9366\n",
      "Epoch 6232/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 8.4239 - val_loss: 1.8850\n",
      "Epoch 6233/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.8909 - val_loss: 1.9683\n",
      "Epoch 6234/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 8.7920 - val_loss: 1.9902\n",
      "Epoch 6235/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 8.4078 - val_loss: 1.9081\n",
      "Epoch 6236/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.7870 - val_loss: 1.9807\n",
      "Epoch 6237/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 9.1130 - val_loss: 1.9994\n",
      "Epoch 6238/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.5772 - val_loss: 1.9181\n",
      "Epoch 6239/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.4697 - val_loss: 1.9119\n",
      "Epoch 6240/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.3140 - val_loss: 1.9347\n",
      "Epoch 6241/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.6885 - val_loss: 1.9827\n",
      "Epoch 6242/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 90us/step - loss: 8.2785 - val_loss: 1.9745\n",
      "Epoch 6243/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6940 - val_loss: 2.0209\n",
      "Epoch 6244/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.7392 - val_loss: 2.1012\n",
      "Epoch 6245/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.7778 - val_loss: 2.0485\n",
      "Epoch 6246/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.8947 - val_loss: 2.0483\n",
      "Epoch 6247/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.9048 - val_loss: 1.9778\n",
      "Epoch 6248/10000\n",
      "90/90 [==============================] - 0s 216us/step - loss: 8.5519 - val_loss: 1.8496\n",
      "Epoch 6249/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.4426 - val_loss: 1.8413\n",
      "Epoch 6250/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.4244 - val_loss: 1.9104\n",
      "Epoch 6251/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.7923 - val_loss: 2.0775\n",
      "Epoch 6252/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.6188 - val_loss: 2.1008\n",
      "Epoch 6253/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.4014 - val_loss: 2.0143\n",
      "Epoch 6254/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.3951 - val_loss: 2.0443\n",
      "Epoch 6255/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.9556 - val_loss: 2.0852\n",
      "Epoch 6256/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.3646 - val_loss: 2.0547\n",
      "Epoch 6257/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.3551 - val_loss: 2.0252\n",
      "Epoch 6258/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.9227 - val_loss: 2.0383\n",
      "Epoch 6259/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 8.5235 - val_loss: 2.0263\n",
      "Epoch 6260/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.4790 - val_loss: 1.9706\n",
      "Epoch 6261/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.7464 - val_loss: 1.9531\n",
      "Epoch 6262/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5455 - val_loss: 1.9402\n",
      "Epoch 6263/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.3541 - val_loss: 1.9292\n",
      "Epoch 6264/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.3729 - val_loss: 2.0227\n",
      "Epoch 6265/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 8.6660 - val_loss: 2.0079\n",
      "Epoch 6266/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.9255 - val_loss: 1.9677\n",
      "Epoch 6267/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.7672 - val_loss: 1.9741\n",
      "Epoch 6268/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.7945 - val_loss: 1.9685\n",
      "Epoch 6269/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 9.1753 - val_loss: 2.0833\n",
      "Epoch 6270/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.4204 - val_loss: 2.0431\n",
      "Epoch 6271/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.7588 - val_loss: 2.0928\n",
      "Epoch 6272/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.6446 - val_loss: 2.0445\n",
      "Epoch 6273/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.8720 - val_loss: 1.9541\n",
      "Epoch 6274/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.6566 - val_loss: 1.9818\n",
      "Epoch 6275/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.6264 - val_loss: 2.0171\n",
      "Epoch 6276/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.5422 - val_loss: 1.9860\n",
      "Epoch 6277/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.6223 - val_loss: 2.0318\n",
      "Epoch 6278/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.7835 - val_loss: 2.0523\n",
      "Epoch 6279/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.4134 - val_loss: 1.9752\n",
      "Epoch 6280/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.6933 - val_loss: 1.9896\n",
      "Epoch 6281/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.7451 - val_loss: 2.1097\n",
      "Epoch 6282/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6779 - val_loss: 2.0844\n",
      "Epoch 6283/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.6890 - val_loss: 2.0532\n",
      "Epoch 6284/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.6706 - val_loss: 2.0385\n",
      "Epoch 6285/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.5045 - val_loss: 1.9609\n",
      "Epoch 6286/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.8344 - val_loss: 1.9616\n",
      "Epoch 6287/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.0529 - val_loss: 2.0605\n",
      "Epoch 6288/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.7830 - val_loss: 1.9759\n",
      "Epoch 6289/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.5593 - val_loss: 1.8888\n",
      "Epoch 6290/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.8100 - val_loss: 1.9708\n",
      "Epoch 6291/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.6490 - val_loss: 2.1118\n",
      "Epoch 6292/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.8820 - val_loss: 2.1303\n",
      "Epoch 6293/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.6664 - val_loss: 2.0417\n",
      "Epoch 6294/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.6718 - val_loss: 2.1275\n",
      "Epoch 6295/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 8.6755 - val_loss: 2.0367\n",
      "Epoch 6296/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.5557 - val_loss: 1.9531\n",
      "Epoch 6297/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.6863 - val_loss: 1.9874\n",
      "Epoch 6298/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5218 - val_loss: 2.0103\n",
      "Epoch 6299/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.6212 - val_loss: 1.9709\n",
      "Epoch 6300/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.4575 - val_loss: 1.9940\n",
      "Epoch 6301/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.3360 - val_loss: 2.0698\n",
      "Epoch 6302/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 9.2574 - val_loss: 2.1373\n",
      "Epoch 6303/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.5731 - val_loss: 2.0928\n",
      "Epoch 6304/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.4778 - val_loss: 2.0165\n",
      "Epoch 6305/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.5535 - val_loss: 2.1371\n",
      "Epoch 6306/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.6816 - val_loss: 2.1762\n",
      "Epoch 6307/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.6174 - val_loss: 2.1978\n",
      "Epoch 6308/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.7544 - val_loss: 2.1534\n",
      "Epoch 6309/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.1681 - val_loss: 2.0833\n",
      "Epoch 6310/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.8094 - val_loss: 2.0041\n",
      "Epoch 6311/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.3932 - val_loss: 1.9094\n",
      "Epoch 6312/10000\n",
      "90/90 [==============================] - 0s 81us/step - loss: 8.5385 - val_loss: 1.9815\n",
      "Epoch 6313/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.2672 - val_loss: 2.0183\n",
      "Epoch 6314/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.5078 - val_loss: 2.0643\n",
      "Epoch 6315/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.4556 - val_loss: 2.0466\n",
      "Epoch 6316/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.9547 - val_loss: 2.1252\n",
      "Epoch 6317/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.7366 - val_loss: 2.1212\n",
      "Epoch 6318/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.7421 - val_loss: 2.1315\n",
      "Epoch 6319/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.3437 - val_loss: 2.0777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6320/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.6779 - val_loss: 2.0982\n",
      "Epoch 6321/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.6324 - val_loss: 2.1219\n",
      "Epoch 6322/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 9.0722 - val_loss: 2.2058\n",
      "Epoch 6323/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.6871 - val_loss: 2.1608\n",
      "Epoch 6324/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.3991 - val_loss: 2.1059\n",
      "Epoch 6325/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 8.3797 - val_loss: 2.0576\n",
      "Epoch 6326/10000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 8.6250 - val_loss: 2.0292\n",
      "Epoch 6327/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.4024 - val_loss: 2.1083\n",
      "Epoch 6328/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.8021 - val_loss: 2.1621\n",
      "Epoch 6329/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.0343 - val_loss: 2.0100\n",
      "Epoch 6330/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.4551 - val_loss: 2.1429\n",
      "Epoch 6331/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.9020 - val_loss: 2.1544\n",
      "Epoch 6332/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.7358 - val_loss: 2.1196\n",
      "Epoch 6333/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.9416 - val_loss: 2.0851\n",
      "Epoch 6334/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.8767 - val_loss: 2.1316\n",
      "Epoch 6335/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5997 - val_loss: 2.0328\n",
      "Epoch 6336/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.6549 - val_loss: 2.1324\n",
      "Epoch 6337/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.8333 - val_loss: 2.1276\n",
      "Epoch 6338/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.5095 - val_loss: 2.0472\n",
      "Epoch 6339/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.8105 - val_loss: 2.0204\n",
      "Epoch 6340/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.6276 - val_loss: 2.0018\n",
      "Epoch 6341/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.5530 - val_loss: 2.0988\n",
      "Epoch 6342/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.5422 - val_loss: 2.1765\n",
      "Epoch 6343/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.3567 - val_loss: 2.1267\n",
      "Epoch 6344/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 9.1593 - val_loss: 2.1554\n",
      "Epoch 6345/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.6460 - val_loss: 2.0823\n",
      "Epoch 6346/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.8189 - val_loss: 2.0789\n",
      "Epoch 6347/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.7046 - val_loss: 2.1253\n",
      "Epoch 6348/10000\n",
      "90/90 [==============================] - 0s 223us/step - loss: 8.6298 - val_loss: 2.1793\n",
      "Epoch 6349/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 8.5906 - val_loss: 2.0943\n",
      "Epoch 6350/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.9472 - val_loss: 2.0729\n",
      "Epoch 6351/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.8975 - val_loss: 2.1237\n",
      "Epoch 6352/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.9947 - val_loss: 2.1026\n",
      "Epoch 6353/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.6937 - val_loss: 2.0738\n",
      "Epoch 6354/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.5785 - val_loss: 2.1180\n",
      "Epoch 6355/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.2415 - val_loss: 2.1168\n",
      "Epoch 6356/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.4036 - val_loss: 2.1913\n",
      "Epoch 6357/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.6984 - val_loss: 2.2144\n",
      "Epoch 6358/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.8614 - val_loss: 2.1174\n",
      "Epoch 6359/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.6031 - val_loss: 2.1093\n",
      "Epoch 6360/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5593 - val_loss: 2.1141\n",
      "Epoch 6361/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.6566 - val_loss: 2.1791\n",
      "Epoch 6362/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6494 - val_loss: 2.2024\n",
      "Epoch 6363/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 8.4803 - val_loss: 2.1095\n",
      "Epoch 6364/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.4397 - val_loss: 2.1378\n",
      "Epoch 6365/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.4353 - val_loss: 2.1296\n",
      "Epoch 6366/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.7013 - val_loss: 2.1757\n",
      "Epoch 6367/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.5294 - val_loss: 2.1494\n",
      "Epoch 6368/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.6399 - val_loss: 2.1653\n",
      "Epoch 6369/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.5693 - val_loss: 2.2165\n",
      "Epoch 6370/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.5397 - val_loss: 2.1970\n",
      "Epoch 6371/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.3128 - val_loss: 2.1888\n",
      "Epoch 6372/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.5521 - val_loss: 2.1341\n",
      "Epoch 6373/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.3685 - val_loss: 2.1107\n",
      "Epoch 6374/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.6377 - val_loss: 2.1260\n",
      "Epoch 6375/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.5062 - val_loss: 2.1242\n",
      "Epoch 6376/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.8672 - val_loss: 2.0490\n",
      "Epoch 6377/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 9.1023 - val_loss: 2.1154\n",
      "Epoch 6378/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.4762 - val_loss: 2.1051\n",
      "Epoch 6379/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.4432 - val_loss: 2.0571\n",
      "Epoch 6380/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 8.7702 - val_loss: 2.1912\n",
      "Epoch 6381/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 8.4848 - val_loss: 2.1490\n",
      "Epoch 6382/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 8.1689 - val_loss: 2.1527\n",
      "Epoch 6383/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.5447 - val_loss: 2.2195\n",
      "Epoch 6384/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.3295 - val_loss: 2.2541\n",
      "Epoch 6385/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.3685 - val_loss: 2.2102\n",
      "Epoch 6386/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.9922 - val_loss: 2.2400\n",
      "Epoch 6387/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.4530 - val_loss: 2.1093\n",
      "Epoch 6388/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1625 - val_loss: 2.0913\n",
      "Epoch 6389/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.2672 - val_loss: 2.0799\n",
      "Epoch 6390/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.4716 - val_loss: 2.1787\n",
      "Epoch 6391/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.5066 - val_loss: 2.1777\n",
      "Epoch 6392/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.1661 - val_loss: 2.1215\n",
      "Epoch 6393/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.5858 - val_loss: 2.2387\n",
      "Epoch 6394/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.7084 - val_loss: 2.2732\n",
      "Epoch 6395/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.1569 - val_loss: 2.2350\n",
      "Epoch 6396/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.7123 - val_loss: 2.2037\n",
      "Epoch 6397/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.2743 - val_loss: 2.1343\n",
      "Epoch 6398/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.5808 - val_loss: 2.1981\n",
      "Epoch 6399/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.5166 - val_loss: 2.2340\n",
      "Epoch 6400/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.8782 - val_loss: 2.2201\n",
      "Epoch 6401/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 8.4436 - val_loss: 2.1048\n",
      "Epoch 6402/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.9491 - val_loss: 2.1719\n",
      "Epoch 6403/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.5919 - val_loss: 2.2169\n",
      "Epoch 6404/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 8.6075 - val_loss: 2.2369\n",
      "Epoch 6405/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.6327 - val_loss: 2.2246\n",
      "Epoch 6406/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6844 - val_loss: 2.2880\n",
      "Epoch 6407/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 8.4250 - val_loss: 2.1695\n",
      "Epoch 6408/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.8334 - val_loss: 2.1345\n",
      "Epoch 6409/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.7336 - val_loss: 2.2065\n",
      "Epoch 6410/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.2590 - val_loss: 2.1585\n",
      "Epoch 6411/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.8868 - val_loss: 2.1904\n",
      "Epoch 6412/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.7700 - val_loss: 2.2391\n",
      "Epoch 6413/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.7377 - val_loss: 2.1770\n",
      "Epoch 6414/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.4090 - val_loss: 2.2429\n",
      "Epoch 6415/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.7823 - val_loss: 2.3056\n",
      "Epoch 6416/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.3803 - val_loss: 2.2782\n",
      "Epoch 6417/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.5022 - val_loss: 2.2164\n",
      "Epoch 6418/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.3363 - val_loss: 2.2104\n",
      "Epoch 6419/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.1068 - val_loss: 2.2658\n",
      "Epoch 6420/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.1651 - val_loss: 2.2050\n",
      "Epoch 6421/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.5923 - val_loss: 2.2521\n",
      "Epoch 6422/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.3541 - val_loss: 2.2449\n",
      "Epoch 6423/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.2068 - val_loss: 2.2303\n",
      "Epoch 6424/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.4609 - val_loss: 2.2741\n",
      "Epoch 6425/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.7937 - val_loss: 2.2111\n",
      "Epoch 6426/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 8.8511 - val_loss: 2.1720\n",
      "Epoch 6427/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.5670 - val_loss: 2.1071\n",
      "Epoch 6428/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.5548 - val_loss: 2.0611\n",
      "Epoch 6429/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 7.9783 - val_loss: 2.1042\n",
      "Epoch 6430/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.4507 - val_loss: 2.1993\n",
      "Epoch 6431/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.4000 - val_loss: 2.2706\n",
      "Epoch 6432/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.6613 - val_loss: 2.2834\n",
      "Epoch 6433/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.7114 - val_loss: 2.2730\n",
      "Epoch 6434/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.5943 - val_loss: 2.2457\n",
      "Epoch 6435/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.4488 - val_loss: 2.2581\n",
      "Epoch 6436/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.5109 - val_loss: 2.3338\n",
      "Epoch 6437/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.0569 - val_loss: 2.2516\n",
      "Epoch 6438/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.9841 - val_loss: 2.3097\n",
      "Epoch 6439/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.5432 - val_loss: 2.2405\n",
      "Epoch 6440/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.5789 - val_loss: 2.1904\n",
      "Epoch 6441/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.3324 - val_loss: 2.1981\n",
      "Epoch 6442/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.4175 - val_loss: 2.2418\n",
      "Epoch 6443/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.5761 - val_loss: 2.3481\n",
      "Epoch 6444/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5714 - val_loss: 2.2849\n",
      "Epoch 6445/10000\n",
      "90/90 [==============================] - 0s 199us/step - loss: 8.3756 - val_loss: 2.1363\n",
      "Epoch 6446/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 8.4069 - val_loss: 2.1571\n",
      "Epoch 6447/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 9.0212 - val_loss: 2.3311\n",
      "Epoch 6448/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.0974 - val_loss: 2.2975\n",
      "Epoch 6449/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.2101 - val_loss: 2.3100\n",
      "Epoch 6450/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 8.5353 - val_loss: 2.3342\n",
      "Epoch 6451/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.3619 - val_loss: 2.2583\n",
      "Epoch 6452/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 9.0144 - val_loss: 2.3117\n",
      "Epoch 6453/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.3832 - val_loss: 2.2664\n",
      "Epoch 6454/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.3604 - val_loss: 2.2321\n",
      "Epoch 6455/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.4538 - val_loss: 2.1624\n",
      "Epoch 6456/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.7398 - val_loss: 2.2132\n",
      "Epoch 6457/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.3898 - val_loss: 2.2553\n",
      "Epoch 6458/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.5376 - val_loss: 2.2606\n",
      "Epoch 6459/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.5297 - val_loss: 2.2318\n",
      "Epoch 6460/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.2469 - val_loss: 2.2626\n",
      "Epoch 6461/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.6955 - val_loss: 2.3221\n",
      "Epoch 6462/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.2484 - val_loss: 2.3752\n",
      "Epoch 6463/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.3571 - val_loss: 2.3263\n",
      "Epoch 6464/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.4563 - val_loss: 2.2970\n",
      "Epoch 6465/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.4930 - val_loss: 2.2108\n",
      "Epoch 6466/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.3544 - val_loss: 2.3134\n",
      "Epoch 6467/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6003 - val_loss: 2.3082\n",
      "Epoch 6468/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.4013 - val_loss: 2.3254\n",
      "Epoch 6469/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.3484 - val_loss: 2.2230\n",
      "Epoch 6470/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.9214 - val_loss: 2.3460\n",
      "Epoch 6471/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.7909 - val_loss: 2.3966\n",
      "Epoch 6472/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.2662 - val_loss: 2.3335\n",
      "Epoch 6473/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.1486 - val_loss: 2.2818\n",
      "Epoch 6474/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.4526 - val_loss: 2.2609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6475/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.4220 - val_loss: 2.2839\n",
      "Epoch 6476/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1077 - val_loss: 2.1519\n",
      "Epoch 6477/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5980 - val_loss: 2.2271\n",
      "Epoch 6478/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.2593 - val_loss: 2.3906\n",
      "Epoch 6479/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 8.3982 - val_loss: 2.3870\n",
      "Epoch 6480/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.2560 - val_loss: 2.3798\n",
      "Epoch 6481/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.3898 - val_loss: 2.3892\n",
      "Epoch 6482/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.4693 - val_loss: 2.3364\n",
      "Epoch 6483/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.5904 - val_loss: 2.2477\n",
      "Epoch 6484/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.3131 - val_loss: 2.2482\n",
      "Epoch 6485/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.3864 - val_loss: 2.2431\n",
      "Epoch 6486/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.4728 - val_loss: 2.2976\n",
      "Epoch 6487/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.9392 - val_loss: 2.3685\n",
      "Epoch 6488/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.5980 - val_loss: 2.4434\n",
      "Epoch 6489/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.3198 - val_loss: 2.3335\n",
      "Epoch 6490/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.6407 - val_loss: 2.2907\n",
      "Epoch 6491/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.4545 - val_loss: 2.2991\n",
      "Epoch 6492/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.6710 - val_loss: 2.3809\n",
      "Epoch 6493/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.7704 - val_loss: 2.3060\n",
      "Epoch 6494/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.5479 - val_loss: 2.1759\n",
      "Epoch 6495/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.5680 - val_loss: 2.2371\n",
      "Epoch 6496/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.2448 - val_loss: 2.2934\n",
      "Epoch 6497/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.5251 - val_loss: 2.4050\n",
      "Epoch 6498/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.5892 - val_loss: 2.4515\n",
      "Epoch 6499/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.3037 - val_loss: 2.4364\n",
      "Epoch 6500/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.0597 - val_loss: 2.3653\n",
      "Epoch 6501/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.3142 - val_loss: 2.3537\n",
      "Epoch 6502/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.1500 - val_loss: 2.2929\n",
      "Epoch 6503/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5726 - val_loss: 2.2740\n",
      "Epoch 6504/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.0769 - val_loss: 2.2530\n",
      "Epoch 6505/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.0530 - val_loss: 2.2684\n",
      "Epoch 6506/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.3361 - val_loss: 2.3991\n",
      "Epoch 6507/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.8391 - val_loss: 2.4723\n",
      "Epoch 6508/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.3215 - val_loss: 2.4427\n",
      "Epoch 6509/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.4253 - val_loss: 2.3497\n",
      "Epoch 6510/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.3826 - val_loss: 2.4027\n",
      "Epoch 6511/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 8.0680 - val_loss: 2.3900\n",
      "Epoch 6512/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.8630 - val_loss: 2.4360\n",
      "Epoch 6513/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.0472 - val_loss: 2.3147\n",
      "Epoch 6514/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.6138 - val_loss: 2.3179\n",
      "Epoch 6515/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.2661 - val_loss: 2.3759\n",
      "Epoch 6516/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.8389 - val_loss: 2.3082\n",
      "Epoch 6517/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.8636 - val_loss: 2.3317\n",
      "Epoch 6518/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.3248 - val_loss: 2.3491\n",
      "Epoch 6519/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.7968 - val_loss: 2.3536\n",
      "Epoch 6520/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.8540 - val_loss: 2.2418\n",
      "Epoch 6521/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.9492 - val_loss: 2.3334\n",
      "Epoch 6522/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.3152 - val_loss: 2.2762\n",
      "Epoch 6523/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.1363 - val_loss: 2.4029\n",
      "Epoch 6524/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.0604 - val_loss: 2.4189\n",
      "Epoch 6525/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.1628 - val_loss: 2.3821\n",
      "Epoch 6526/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.1905 - val_loss: 2.3303\n",
      "Epoch 6527/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.3557 - val_loss: 2.4132\n",
      "Epoch 6528/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 8.1728 - val_loss: 2.3302\n",
      "Epoch 6529/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.5495 - val_loss: 2.3370\n",
      "Epoch 6530/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.3331 - val_loss: 2.3571\n",
      "Epoch 6531/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.4385 - val_loss: 2.3179\n",
      "Epoch 6532/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.9581 - val_loss: 2.3592\n",
      "Epoch 6533/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.1304 - val_loss: 2.3858\n",
      "Epoch 6534/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.5041 - val_loss: 2.4368\n",
      "Epoch 6535/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.0690 - val_loss: 2.4606\n",
      "Epoch 6536/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.3320 - val_loss: 2.5169\n",
      "Epoch 6537/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.9805 - val_loss: 2.4670\n",
      "Epoch 6538/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.2374 - val_loss: 2.3957\n",
      "Epoch 6539/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 8.0440 - val_loss: 2.3166\n",
      "Epoch 6540/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.4244 - val_loss: 2.3012\n",
      "Epoch 6541/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.6489 - val_loss: 2.3401\n",
      "Epoch 6542/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.4332 - val_loss: 2.3865\n",
      "Epoch 6543/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.6800 - val_loss: 2.5100\n",
      "Epoch 6544/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.0907 - val_loss: 2.4565\n",
      "Epoch 6545/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 8.4093 - val_loss: 2.4262\n",
      "Epoch 6546/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.2531 - val_loss: 2.5150\n",
      "Epoch 6547/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5406 - val_loss: 2.4281\n",
      "Epoch 6548/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.2628 - val_loss: 2.3118\n",
      "Epoch 6549/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 7.9100 - val_loss: 2.3969\n",
      "Epoch 6550/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.1995 - val_loss: 2.3640\n",
      "Epoch 6551/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.7358 - val_loss: 2.3545\n",
      "Epoch 6552/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.5455 - val_loss: 2.4180\n",
      "Epoch 6553/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.1871 - val_loss: 2.3950\n",
      "Epoch 6554/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.2834 - val_loss: 2.4182\n",
      "Epoch 6555/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.3970 - val_loss: 2.4352\n",
      "Epoch 6556/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.7812 - val_loss: 2.5503\n",
      "Epoch 6557/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.6526 - val_loss: 2.4849\n",
      "Epoch 6558/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.9901 - val_loss: 2.4462\n",
      "Epoch 6559/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1411 - val_loss: 2.4568\n",
      "Epoch 6560/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.5052 - val_loss: 2.4450\n",
      "Epoch 6561/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.6397 - val_loss: 2.4601\n",
      "Epoch 6562/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.3238 - val_loss: 2.4977\n",
      "Epoch 6563/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.5510 - val_loss: 2.4900\n",
      "Epoch 6564/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.5484 - val_loss: 2.5178\n",
      "Epoch 6565/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.2529 - val_loss: 2.4942\n",
      "Epoch 6566/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.0797 - val_loss: 2.4588\n",
      "Epoch 6567/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.7627 - val_loss: 2.4309\n",
      "Epoch 6568/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.1120 - val_loss: 2.3715\n",
      "Epoch 6569/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.5964 - val_loss: 2.4712\n",
      "Epoch 6570/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.2912 - val_loss: 2.4829\n",
      "Epoch 6571/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.6439 - val_loss: 2.4880\n",
      "Epoch 6572/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.1909 - val_loss: 2.3914\n",
      "Epoch 6573/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.1058 - val_loss: 2.4282\n",
      "Epoch 6574/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.9910 - val_loss: 2.4577\n",
      "Epoch 6575/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.1979 - val_loss: 2.3662\n",
      "Epoch 6576/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.1463 - val_loss: 2.4161\n",
      "Epoch 6577/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.2266 - val_loss: 2.4844\n",
      "Epoch 6578/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.9866 - val_loss: 2.4231\n",
      "Epoch 6579/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.3982 - val_loss: 2.5073\n",
      "Epoch 6580/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.6244 - val_loss: 2.5054\n",
      "Epoch 6581/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.3470 - val_loss: 2.5200\n",
      "Epoch 6582/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.0308 - val_loss: 2.4874\n",
      "Epoch 6583/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.4482 - val_loss: 2.4442\n",
      "Epoch 6584/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.3971 - val_loss: 2.4823\n",
      "Epoch 6585/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.6799 - val_loss: 2.4794\n",
      "Epoch 6586/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.4843 - val_loss: 2.5183\n",
      "Epoch 6587/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.3377 - val_loss: 2.4947\n",
      "Epoch 6588/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5071 - val_loss: 2.5974\n",
      "Epoch 6589/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.1537 - val_loss: 2.5268\n",
      "Epoch 6590/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.9880 - val_loss: 2.5372\n",
      "Epoch 6591/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.2976 - val_loss: 2.5416\n",
      "Epoch 6592/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.2107 - val_loss: 2.5856\n",
      "Epoch 6593/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.0434 - val_loss: 2.5483\n",
      "Epoch 6594/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.6420 - val_loss: 2.5123\n",
      "Epoch 6595/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.2810 - val_loss: 2.5159\n",
      "Epoch 6596/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.6201 - val_loss: 2.4119\n",
      "Epoch 6597/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.0018 - val_loss: 2.5025\n",
      "Epoch 6598/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.2727 - val_loss: 2.4647\n",
      "Epoch 6599/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.4648 - val_loss: 2.4463\n",
      "Epoch 6600/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.4407 - val_loss: 2.4511\n",
      "Epoch 6601/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.5047 - val_loss: 2.5128\n",
      "Epoch 6602/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.4597 - val_loss: 2.5416\n",
      "Epoch 6603/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.4907 - val_loss: 2.5974\n",
      "Epoch 6604/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.4097 - val_loss: 2.6058\n",
      "Epoch 6605/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.0624 - val_loss: 2.5573\n",
      "Epoch 6606/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.5885 - val_loss: 2.5085\n",
      "Epoch 6607/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.5519 - val_loss: 2.5553\n",
      "Epoch 6608/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9880 - val_loss: 2.5436\n",
      "Epoch 6609/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.4100 - val_loss: 2.4693\n",
      "Epoch 6610/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.4803 - val_loss: 2.5168\n",
      "Epoch 6611/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.3810 - val_loss: 2.5089\n",
      "Epoch 6612/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.2728 - val_loss: 2.5853\n",
      "Epoch 6613/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.4815 - val_loss: 2.5446\n",
      "Epoch 6614/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.7778 - val_loss: 2.5659\n",
      "Epoch 6615/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.7726 - val_loss: 2.5515\n",
      "Epoch 6616/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.7358 - val_loss: 2.4904\n",
      "Epoch 6617/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.1152 - val_loss: 2.4185\n",
      "Epoch 6618/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.0079 - val_loss: 2.4640\n",
      "Epoch 6619/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.6385 - val_loss: 2.6377\n",
      "Epoch 6620/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.0874 - val_loss: 2.7036\n",
      "Epoch 6621/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.4928 - val_loss: 2.6253\n",
      "Epoch 6622/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.3793 - val_loss: 2.5282\n",
      "Epoch 6623/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.3642 - val_loss: 2.4921\n",
      "Epoch 6624/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1636 - val_loss: 2.4612\n",
      "Epoch 6625/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.3817 - val_loss: 2.4796\n",
      "Epoch 6626/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.3484 - val_loss: 2.6649\n",
      "Epoch 6627/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.2539 - val_loss: 2.6168\n",
      "Epoch 6628/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.2164 - val_loss: 2.6488\n",
      "Epoch 6629/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.3251 - val_loss: 2.6360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6630/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.1171 - val_loss: 2.6405\n",
      "Epoch 6631/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.0571 - val_loss: 2.5411\n",
      "Epoch 6632/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.3806 - val_loss: 2.4349\n",
      "Epoch 6633/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 7.7343 - val_loss: 2.4445\n",
      "Epoch 6634/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.0222 - val_loss: 2.5089\n",
      "Epoch 6635/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.6407 - val_loss: 2.6391\n",
      "Epoch 6636/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.5460 - val_loss: 2.5946\n",
      "Epoch 6637/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1707 - val_loss: 2.6218\n",
      "Epoch 6638/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.4260 - val_loss: 2.6048\n",
      "Epoch 6639/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 8.2978 - val_loss: 2.5401\n",
      "Epoch 6640/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.2051 - val_loss: 2.5815\n",
      "Epoch 6641/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.2267 - val_loss: 2.6224\n",
      "Epoch 6642/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 8.0477 - val_loss: 2.6951\n",
      "Epoch 6643/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.3186 - val_loss: 2.6553\n",
      "Epoch 6644/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.1518 - val_loss: 2.6399\n",
      "Epoch 6645/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.4062 - val_loss: 2.6535\n",
      "Epoch 6646/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.3052 - val_loss: 2.5489\n",
      "Epoch 6647/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1727 - val_loss: 2.5917\n",
      "Epoch 6648/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 8.1707 - val_loss: 2.6385\n",
      "Epoch 6649/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 8.3126 - val_loss: 2.7101\n",
      "Epoch 6650/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.8310 - val_loss: 2.7139\n",
      "Epoch 6651/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.1277 - val_loss: 2.7100\n",
      "Epoch 6652/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 8.0300 - val_loss: 2.6354\n",
      "Epoch 6653/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 7.8201 - val_loss: 2.5332\n",
      "Epoch 6654/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 8.2432 - val_loss: 2.5373\n",
      "Epoch 6655/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.0712 - val_loss: 2.5648\n",
      "Epoch 6656/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.0141 - val_loss: 2.4874\n",
      "Epoch 6657/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.8218 - val_loss: 2.5331\n",
      "Epoch 6658/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.1579 - val_loss: 2.5597\n",
      "Epoch 6659/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.0882 - val_loss: 2.6079\n",
      "Epoch 6660/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.1926 - val_loss: 2.7708\n",
      "Epoch 6661/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.4640 - val_loss: 2.7846\n",
      "Epoch 6662/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.1613 - val_loss: 2.6831\n",
      "Epoch 6663/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.2659 - val_loss: 2.6591\n",
      "Epoch 6664/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.1429 - val_loss: 2.5456\n",
      "Epoch 6665/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.2616 - val_loss: 2.6020\n",
      "Epoch 6666/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5029 - val_loss: 2.6722\n",
      "Epoch 6667/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.4059 - val_loss: 2.7409\n",
      "Epoch 6668/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 8.0131 - val_loss: 2.6598\n",
      "Epoch 6669/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.0697 - val_loss: 2.6574\n",
      "Epoch 6670/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.6586 - val_loss: 2.5812\n",
      "Epoch 6671/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.5081 - val_loss: 2.6580\n",
      "Epoch 6672/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.2000 - val_loss: 2.7153\n",
      "Epoch 6673/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.2246 - val_loss: 2.6172\n",
      "Epoch 6674/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.9825 - val_loss: 2.5746\n",
      "Epoch 6675/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.7910 - val_loss: 2.5681\n",
      "Epoch 6676/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.4010 - val_loss: 2.6592\n",
      "Epoch 6677/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.3927 - val_loss: 2.6624\n",
      "Epoch 6678/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9069 - val_loss: 2.7604\n",
      "Epoch 6679/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.2514 - val_loss: 2.8155\n",
      "Epoch 6680/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.4823 - val_loss: 2.7374\n",
      "Epoch 6681/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.6273 - val_loss: 2.7510\n",
      "Epoch 6682/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.3930 - val_loss: 2.6435\n",
      "Epoch 6683/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.3235 - val_loss: 2.6384\n",
      "Epoch 6684/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.1125 - val_loss: 2.5810\n",
      "Epoch 6685/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.8114 - val_loss: 2.4762\n",
      "Epoch 6686/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.2114 - val_loss: 2.5599\n",
      "Epoch 6687/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.1376 - val_loss: 2.6605\n",
      "Epoch 6688/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 8.1914 - val_loss: 2.7419\n",
      "Epoch 6689/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.1685 - val_loss: 2.7504\n",
      "Epoch 6690/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.9417 - val_loss: 2.7781\n",
      "Epoch 6691/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.8888 - val_loss: 2.7755\n",
      "Epoch 6692/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.0887 - val_loss: 2.8698\n",
      "Epoch 6693/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.1038 - val_loss: 2.8190\n",
      "Epoch 6694/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.1536 - val_loss: 2.7747\n",
      "Epoch 6695/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.0857 - val_loss: 2.7247\n",
      "Epoch 6696/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.7508 - val_loss: 2.6431\n",
      "Epoch 6697/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.3232 - val_loss: 2.6937\n",
      "Epoch 6698/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.3039 - val_loss: 2.6189\n",
      "Epoch 6699/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.0524 - val_loss: 2.6012\n",
      "Epoch 6700/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.0077 - val_loss: 2.6446\n",
      "Epoch 6701/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.9936 - val_loss: 2.6675\n",
      "Epoch 6702/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.8268 - val_loss: 2.7248\n",
      "Epoch 6703/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.2006 - val_loss: 2.7916\n",
      "Epoch 6704/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.9939 - val_loss: 2.7072\n",
      "Epoch 6705/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.0964 - val_loss: 2.6993\n",
      "Epoch 6706/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.3688 - val_loss: 2.7378\n",
      "Epoch 6707/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 8.2922 - val_loss: 2.7741\n",
      "Epoch 6708/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.8520 - val_loss: 2.7253\n",
      "Epoch 6709/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 8.0169 - val_loss: 2.7809\n",
      "Epoch 6710/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.9797 - val_loss: 2.6847\n",
      "Epoch 6711/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 8.2167 - val_loss: 2.6739\n",
      "Epoch 6712/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 8.1987 - val_loss: 2.7412\n",
      "Epoch 6713/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.0676 - val_loss: 2.7591\n",
      "Epoch 6714/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.0572 - val_loss: 2.7601\n",
      "Epoch 6715/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.2483 - val_loss: 2.6980\n",
      "Epoch 6716/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.2241 - val_loss: 2.7323\n",
      "Epoch 6717/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1219 - val_loss: 2.7913\n",
      "Epoch 6718/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.3309 - val_loss: 2.8298\n",
      "Epoch 6719/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 8.3174 - val_loss: 2.7181\n",
      "Epoch 6720/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 8.2928 - val_loss: 2.6033\n",
      "Epoch 6721/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.9691 - val_loss: 2.5993\n",
      "Epoch 6722/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 7.8985 - val_loss: 2.6820\n",
      "Epoch 6723/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.8021 - val_loss: 2.7995\n",
      "Epoch 6724/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.1105 - val_loss: 2.8546\n",
      "Epoch 6725/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.5629 - val_loss: 2.8993\n",
      "Epoch 6726/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.0639 - val_loss: 2.9663\n",
      "Epoch 6727/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.1379 - val_loss: 2.8095\n",
      "Epoch 6728/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.6821 - val_loss: 2.7657\n",
      "Epoch 6729/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 8.5050 - val_loss: 2.7307\n",
      "Epoch 6730/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.9956 - val_loss: 2.6629\n",
      "Epoch 6731/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.3252 - val_loss: 2.7369\n",
      "Epoch 6732/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.9215 - val_loss: 2.7694\n",
      "Epoch 6733/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 8.6648 - val_loss: 2.8604\n",
      "Epoch 6734/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.1959 - val_loss: 2.7489\n",
      "Epoch 6735/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.7133 - val_loss: 2.8322\n",
      "Epoch 6736/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.7165 - val_loss: 2.7153\n",
      "Epoch 6737/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 8.2147 - val_loss: 2.7261\n",
      "Epoch 6738/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.2489 - val_loss: 2.7629\n",
      "Epoch 6739/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.1406 - val_loss: 2.8123\n",
      "Epoch 6740/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.8110 - val_loss: 2.8990\n",
      "Epoch 6741/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.4877 - val_loss: 2.9378\n",
      "Epoch 6742/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 8.1006 - val_loss: 2.9913\n",
      "Epoch 6743/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.1998 - val_loss: 2.9321\n",
      "Epoch 6744/10000\n",
      "90/90 [==============================] - 0s 223us/step - loss: 7.8890 - val_loss: 2.7014\n",
      "Epoch 6745/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.5791 - val_loss: 2.6849\n",
      "Epoch 6746/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9633 - val_loss: 2.6986\n",
      "Epoch 6747/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.0900 - val_loss: 2.7528\n",
      "Epoch 6748/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.0360 - val_loss: 2.8264\n",
      "Epoch 6749/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.8702 - val_loss: 2.9326\n",
      "Epoch 6750/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.5475 - val_loss: 2.9515\n",
      "Epoch 6751/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.1740 - val_loss: 2.9539\n",
      "Epoch 6752/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 8.1849 - val_loss: 2.8502\n",
      "Epoch 6753/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.1787 - val_loss: 2.7709\n",
      "Epoch 6754/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.3596 - val_loss: 2.8011\n",
      "Epoch 6755/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.4790 - val_loss: 2.7875\n",
      "Epoch 6756/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.9365 - val_loss: 2.7811\n",
      "Epoch 6757/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.6535 - val_loss: 2.8168\n",
      "Epoch 6758/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.2310 - val_loss: 2.7851\n",
      "Epoch 6759/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.9457 - val_loss: 2.7854\n",
      "Epoch 6760/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.2270 - val_loss: 2.8824\n",
      "Epoch 6761/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.8173 - val_loss: 2.8909\n",
      "Epoch 6762/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.1478 - val_loss: 2.9787\n",
      "Epoch 6763/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.1349 - val_loss: 2.9219\n",
      "Epoch 6764/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.0343 - val_loss: 2.8255\n",
      "Epoch 6765/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.2266 - val_loss: 2.8317\n",
      "Epoch 6766/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.3665 - val_loss: 2.8694\n",
      "Epoch 6767/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.0497 - val_loss: 2.8298\n",
      "Epoch 6768/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.4074 - val_loss: 2.8961\n",
      "Epoch 6769/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.5892 - val_loss: 2.8257\n",
      "Epoch 6770/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 8.4698 - val_loss: 2.8566\n",
      "Epoch 6771/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.9321 - val_loss: 2.8871\n",
      "Epoch 6772/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.2070 - val_loss: 2.9265\n",
      "Epoch 6773/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 8.2363 - val_loss: 2.8248\n",
      "Epoch 6774/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.0073 - val_loss: 2.8056\n",
      "Epoch 6775/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 7.6487 - val_loss: 2.7036\n",
      "Epoch 6776/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.2750 - val_loss: 2.8330\n",
      "Epoch 6777/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.2291 - val_loss: 2.9909\n",
      "Epoch 6778/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 8.6235 - val_loss: 3.0440\n",
      "Epoch 6779/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.3353 - val_loss: 3.0092\n",
      "Epoch 6780/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.9877 - val_loss: 2.9218\n",
      "Epoch 6781/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.9477 - val_loss: 2.8458\n",
      "Epoch 6782/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 7.7317 - val_loss: 2.7940\n",
      "Epoch 6783/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 7.5134 - val_loss: 2.8161\n",
      "Epoch 6784/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 139us/step - loss: 8.2495 - val_loss: 2.8512\n",
      "Epoch 6785/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.9647 - val_loss: 2.8709\n",
      "Epoch 6786/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 8.0927 - val_loss: 2.9875\n",
      "Epoch 6787/10000\n",
      "90/90 [==============================] - 0s 171us/step - loss: 8.2067 - val_loss: 3.1051\n",
      "Epoch 6788/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.3873 - val_loss: 3.1035\n",
      "Epoch 6789/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.2253 - val_loss: 3.0755\n",
      "Epoch 6790/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.9498 - val_loss: 2.8846\n",
      "Epoch 6791/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.4238 - val_loss: 2.8508\n",
      "Epoch 6792/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 8.3689 - val_loss: 2.9013\n",
      "Epoch 6793/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 8.0549 - val_loss: 3.0330\n",
      "Epoch 6794/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.8419 - val_loss: 3.0062\n",
      "Epoch 6795/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.0272 - val_loss: 2.8969\n",
      "Epoch 6796/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 7.9665 - val_loss: 2.8651\n",
      "Epoch 6797/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 8.1037 - val_loss: 2.9277\n",
      "Epoch 6798/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.0448 - val_loss: 2.8682\n",
      "Epoch 6799/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.1320 - val_loss: 2.8426\n",
      "Epoch 6800/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.1868 - val_loss: 2.8606\n",
      "Epoch 6801/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.5205 - val_loss: 2.9968\n",
      "Epoch 6802/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.0322 - val_loss: 3.0598\n",
      "Epoch 6803/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.9879 - val_loss: 3.1338\n",
      "Epoch 6804/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 8.0227 - val_loss: 2.9856\n",
      "Epoch 6805/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.0344 - val_loss: 2.9165\n",
      "Epoch 6806/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 8.1290 - val_loss: 2.8119\n",
      "Epoch 6807/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.8022 - val_loss: 2.9242\n",
      "Epoch 6808/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.1534 - val_loss: 3.0724\n",
      "Epoch 6809/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.0203 - val_loss: 3.0805\n",
      "Epoch 6810/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.9002 - val_loss: 2.9838\n",
      "Epoch 6811/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9833 - val_loss: 2.8786\n",
      "Epoch 6812/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.8856 - val_loss: 2.9231\n",
      "Epoch 6813/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.1190 - val_loss: 2.9047\n",
      "Epoch 6814/10000\n",
      "90/90 [==============================] - 0s 180us/step - loss: 7.7904 - val_loss: 2.9694\n",
      "Epoch 6815/10000\n",
      "90/90 [==============================] - 0s 429us/step - loss: 7.6705 - val_loss: 2.9798\n",
      "Epoch 6816/10000\n",
      "90/90 [==============================] - 0s 193us/step - loss: 7.8807 - val_loss: 2.9622\n",
      "Epoch 6817/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.3873 - val_loss: 2.9423\n",
      "Epoch 6818/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.7601 - val_loss: 2.9597\n",
      "Epoch 6819/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.8535 - val_loss: 2.9770\n",
      "Epoch 6820/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.9329 - val_loss: 2.9913\n",
      "Epoch 6821/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.1229 - val_loss: 2.9310\n",
      "Epoch 6822/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.5462 - val_loss: 2.9662\n",
      "Epoch 6823/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.1634 - val_loss: 2.9399\n",
      "Epoch 6824/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.6383 - val_loss: 3.0229\n",
      "Epoch 6825/10000\n",
      "90/90 [==============================] - 0s 220us/step - loss: 8.0497 - val_loss: 3.0595\n",
      "Epoch 6826/10000\n",
      "90/90 [==============================] - 0s 170us/step - loss: 7.7485 - val_loss: 3.0611\n",
      "Epoch 6827/10000\n",
      "90/90 [==============================] - 0s 172us/step - loss: 7.3722 - val_loss: 3.0342\n",
      "Epoch 6828/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.9301 - val_loss: 3.0053\n",
      "Epoch 6829/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.8864 - val_loss: 3.0102\n",
      "Epoch 6830/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 8.0162 - val_loss: 3.0077\n",
      "Epoch 6831/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.6035 - val_loss: 2.9850\n",
      "Epoch 6832/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.9149 - val_loss: 3.0063\n",
      "Epoch 6833/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.8399 - val_loss: 3.0377\n",
      "Epoch 6834/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 8.1061 - val_loss: 3.1319\n",
      "Epoch 6835/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.0875 - val_loss: 3.1757\n",
      "Epoch 6836/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.2862 - val_loss: 3.0864\n",
      "Epoch 6837/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.8319 - val_loss: 3.0524\n",
      "Epoch 6838/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.8592 - val_loss: 2.9868\n",
      "Epoch 6839/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 8.0234 - val_loss: 3.0297\n",
      "Epoch 6840/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.7524 - val_loss: 3.1196\n",
      "Epoch 6841/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.8644 - val_loss: 3.2187\n",
      "Epoch 6842/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 8.1054 - val_loss: 3.2189\n",
      "Epoch 6843/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.9662 - val_loss: 3.0954\n",
      "Epoch 6844/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.6253 - val_loss: 3.0516\n",
      "Epoch 6845/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.9195 - val_loss: 3.0658\n",
      "Epoch 6846/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 8.1321 - val_loss: 2.9987\n",
      "Epoch 6847/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.6079 - val_loss: 2.9898\n",
      "Epoch 6848/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.1981 - val_loss: 3.0210\n",
      "Epoch 6849/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.7819 - val_loss: 3.0909\n",
      "Epoch 6850/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.9862 - val_loss: 3.1432\n",
      "Epoch 6851/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.7940 - val_loss: 3.1129\n",
      "Epoch 6852/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.9000 - val_loss: 3.0515\n",
      "Epoch 6853/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.0523 - val_loss: 3.0523\n",
      "Epoch 6854/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 8.0209 - val_loss: 3.1060\n",
      "Epoch 6855/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.3281 - val_loss: 3.1359\n",
      "Epoch 6856/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.8828 - val_loss: 3.1483\n",
      "Epoch 6857/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.2704 - val_loss: 3.0507\n",
      "Epoch 6858/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9800 - val_loss: 2.9837\n",
      "Epoch 6859/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.8388 - val_loss: 3.0609\n",
      "Epoch 6860/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.0394 - val_loss: 3.1277\n",
      "Epoch 6861/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 8.2212 - val_loss: 3.1011\n",
      "Epoch 6862/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.8689 - val_loss: 3.2383\n",
      "Epoch 6863/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.0184 - val_loss: 3.3743\n",
      "Epoch 6864/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.8262 - val_loss: 3.1911\n",
      "Epoch 6865/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.4966 - val_loss: 3.1364\n",
      "Epoch 6866/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.3935 - val_loss: 3.0426\n",
      "Epoch 6867/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.7319 - val_loss: 3.0260\n",
      "Epoch 6868/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 7.8908 - val_loss: 2.9247\n",
      "Epoch 6869/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.7486 - val_loss: 3.0567\n",
      "Epoch 6870/10000\n",
      "90/90 [==============================] - 0s 164us/step - loss: 7.9417 - val_loss: 3.1398\n",
      "Epoch 6871/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 7.7272 - val_loss: 3.2590\n",
      "Epoch 6872/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 8.1090 - val_loss: 3.1496\n",
      "Epoch 6873/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 7.9972 - val_loss: 3.1612\n",
      "Epoch 6874/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 8.0524 - val_loss: 3.0811\n",
      "Epoch 6875/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.6545 - val_loss: 3.1174\n",
      "Epoch 6876/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 8.1233 - val_loss: 3.0721\n",
      "Epoch 6877/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 8.1842 - val_loss: 3.0455\n",
      "Epoch 6878/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.2863 - val_loss: 3.2114\n",
      "Epoch 6879/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.2536 - val_loss: 3.2391\n",
      "Epoch 6880/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.1697 - val_loss: 3.2562\n",
      "Epoch 6881/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.8827 - val_loss: 3.2929\n",
      "Epoch 6882/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.5275 - val_loss: 3.1884\n",
      "Epoch 6883/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.6144 - val_loss: 3.0989\n",
      "Epoch 6884/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.5196 - val_loss: 3.1349\n",
      "Epoch 6885/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.8540 - val_loss: 3.0291\n",
      "Epoch 6886/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 8.1311 - val_loss: 3.1124\n",
      "Epoch 6887/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.2078 - val_loss: 3.2072\n",
      "Epoch 6888/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.7087 - val_loss: 3.1367\n",
      "Epoch 6889/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.5763 - val_loss: 3.1832\n",
      "Epoch 6890/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.8078 - val_loss: 3.1965\n",
      "Epoch 6891/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 8.0379 - val_loss: 3.2016\n",
      "Epoch 6892/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.0273 - val_loss: 3.3020\n",
      "Epoch 6893/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.4837 - val_loss: 3.3518\n",
      "Epoch 6894/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.2615 - val_loss: 3.1722\n",
      "Epoch 6895/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.2042 - val_loss: 3.1566\n",
      "Epoch 6896/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.5191 - val_loss: 3.1168\n",
      "Epoch 6897/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.7159 - val_loss: 3.1627\n",
      "Epoch 6898/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.1534 - val_loss: 3.1874\n",
      "Epoch 6899/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 7.9244 - val_loss: 3.2555\n",
      "Epoch 6900/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9649 - val_loss: 3.3066\n",
      "Epoch 6901/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.6446 - val_loss: 3.3696\n",
      "Epoch 6902/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.9582 - val_loss: 3.3003\n",
      "Epoch 6903/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.9174 - val_loss: 3.2493\n",
      "Epoch 6904/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.4154 - val_loss: 3.2471\n",
      "Epoch 6905/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.2919 - val_loss: 3.2265\n",
      "Epoch 6906/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.4023 - val_loss: 3.3273\n",
      "Epoch 6907/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.6891 - val_loss: 3.3532\n",
      "Epoch 6908/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.9857 - val_loss: 3.1383\n",
      "Epoch 6909/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.4544 - val_loss: 2.9973\n",
      "Epoch 6910/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 7.9962 - val_loss: 3.1064\n",
      "Epoch 6911/10000\n",
      "90/90 [==============================] - 0s 211us/step - loss: 7.7143 - val_loss: 3.1744\n",
      "Epoch 6912/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 8.0103 - val_loss: 3.2053\n",
      "Epoch 6913/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.8461 - val_loss: 3.3848\n",
      "Epoch 6914/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.9890 - val_loss: 3.4823\n",
      "Epoch 6915/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.4913 - val_loss: 3.4589\n",
      "Epoch 6916/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 8.1107 - val_loss: 3.3339\n",
      "Epoch 6917/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.3151 - val_loss: 3.1870\n",
      "Epoch 6918/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.8668 - val_loss: 3.1138\n",
      "Epoch 6919/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.7694 - val_loss: 3.1645\n",
      "Epoch 6920/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.7009 - val_loss: 3.2176\n",
      "Epoch 6921/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.9771 - val_loss: 3.1433\n",
      "Epoch 6922/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.6922 - val_loss: 3.2162\n",
      "Epoch 6923/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.5362 - val_loss: 3.2186\n",
      "Epoch 6924/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 7.4774 - val_loss: 3.1492\n",
      "Epoch 6925/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.1372 - val_loss: 3.2753\n",
      "Epoch 6926/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.7859 - val_loss: 3.3254\n",
      "Epoch 6927/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.9723 - val_loss: 3.3551\n",
      "Epoch 6928/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 7.4778 - val_loss: 3.3980\n",
      "Epoch 6929/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.0074 - val_loss: 3.3790\n",
      "Epoch 6930/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.6965 - val_loss: 3.3533\n",
      "Epoch 6931/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 8.1509 - val_loss: 3.2290\n",
      "Epoch 6932/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.7167 - val_loss: 3.2789\n",
      "Epoch 6933/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.9672 - val_loss: 3.3148\n",
      "Epoch 6934/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 8.4691 - val_loss: 3.3779\n",
      "Epoch 6935/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.0945 - val_loss: 3.4812\n",
      "Epoch 6936/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9191 - val_loss: 3.3344\n",
      "Epoch 6937/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 8.1668 - val_loss: 3.2246\n",
      "Epoch 6938/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 112us/step - loss: 7.7565 - val_loss: 3.1666\n",
      "Epoch 6939/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.5882 - val_loss: 3.2366\n",
      "Epoch 6940/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.8264 - val_loss: 3.3290\n",
      "Epoch 6941/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.6150 - val_loss: 3.3802\n",
      "Epoch 6942/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.0990 - val_loss: 3.4174\n",
      "Epoch 6943/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.6827 - val_loss: 3.4378\n",
      "Epoch 6944/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.0290 - val_loss: 3.4082\n",
      "Epoch 6945/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.0742 - val_loss: 3.3919\n",
      "Epoch 6946/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.8723 - val_loss: 3.3079\n",
      "Epoch 6947/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 8.0227 - val_loss: 3.3975\n",
      "Epoch 6948/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.7560 - val_loss: 3.4520\n",
      "Epoch 6949/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.2102 - val_loss: 3.4424\n",
      "Epoch 6950/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 8.1380 - val_loss: 3.4388\n",
      "Epoch 6951/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.6589 - val_loss: 3.3330\n",
      "Epoch 6952/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.6062 - val_loss: 3.3353\n",
      "Epoch 6953/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.3269 - val_loss: 3.4032\n",
      "Epoch 6954/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 8.0059 - val_loss: 3.4304\n",
      "Epoch 6955/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.6166 - val_loss: 3.4850\n",
      "Epoch 6956/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.9525 - val_loss: 3.5179\n",
      "Epoch 6957/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 8.3856 - val_loss: 3.4010\n",
      "Epoch 6958/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.9304 - val_loss: 3.4676\n",
      "Epoch 6959/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.8617 - val_loss: 3.5830\n",
      "Epoch 6960/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.7219 - val_loss: 3.4705\n",
      "Epoch 6961/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.7203 - val_loss: 3.3339\n",
      "Epoch 6962/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.3175 - val_loss: 3.1935\n",
      "Epoch 6963/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.9083 - val_loss: 3.3101\n",
      "Epoch 6964/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.9700 - val_loss: 3.3520\n",
      "Epoch 6965/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 8.0412 - val_loss: 3.3244\n",
      "Epoch 6966/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 7.9378 - val_loss: 3.4745\n",
      "Epoch 6967/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 7.6892 - val_loss: 3.5374\n",
      "Epoch 6968/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.4704 - val_loss: 3.5135\n",
      "Epoch 6969/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.5486 - val_loss: 3.4136\n",
      "Epoch 6970/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.3984 - val_loss: 3.2734\n",
      "Epoch 6971/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.5802 - val_loss: 3.4052\n",
      "Epoch 6972/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.7452 - val_loss: 3.3714\n",
      "Epoch 6973/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.5596 - val_loss: 3.3399\n",
      "Epoch 6974/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.2668 - val_loss: 3.3201\n",
      "Epoch 6975/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 8.1354 - val_loss: 3.4137\n",
      "Epoch 6976/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.6924 - val_loss: 3.4290\n",
      "Epoch 6977/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 7.8200 - val_loss: 3.6025\n",
      "Epoch 6978/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.9984 - val_loss: 3.6184\n",
      "Epoch 6979/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.5864 - val_loss: 3.5506\n",
      "Epoch 6980/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.5857 - val_loss: 3.4963\n",
      "Epoch 6981/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.6359 - val_loss: 3.4364\n",
      "Epoch 6982/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.3198 - val_loss: 3.4397\n",
      "Epoch 6983/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 8.1933 - val_loss: 3.5592\n",
      "Epoch 6984/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.8174 - val_loss: 3.4008\n",
      "Epoch 6985/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 7.6275 - val_loss: 3.4142\n",
      "Epoch 6986/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.6929 - val_loss: 3.3466\n",
      "Epoch 6987/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.8262 - val_loss: 3.3415\n",
      "Epoch 6988/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.6354 - val_loss: 3.3846\n",
      "Epoch 6989/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.5930 - val_loss: 3.5188\n",
      "Epoch 6990/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.0040 - val_loss: 3.5125\n",
      "Epoch 6991/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.7057 - val_loss: 3.4843\n",
      "Epoch 6992/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 7.9372 - val_loss: 3.4748\n",
      "Epoch 6993/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.6502 - val_loss: 3.5194\n",
      "Epoch 6994/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.5610 - val_loss: 3.5287\n",
      "Epoch 6995/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.8129 - val_loss: 3.5758\n",
      "Epoch 6996/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.4282 - val_loss: 3.6244\n",
      "Epoch 6997/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.6482 - val_loss: 3.6097\n",
      "Epoch 6998/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.8282 - val_loss: 3.3161\n",
      "Epoch 6999/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.7909 - val_loss: 3.3272\n",
      "Epoch 7000/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.6962 - val_loss: 3.3332\n",
      "Epoch 7001/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.5458 - val_loss: 3.4343\n",
      "Epoch 7002/10000\n",
      "90/90 [==============================] - 0s 190us/step - loss: 7.6567 - val_loss: 3.5497\n",
      "Epoch 7003/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.7526 - val_loss: 3.5075\n",
      "Epoch 7004/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.6836 - val_loss: 3.4665\n",
      "Epoch 7005/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.9190 - val_loss: 3.5682\n",
      "Epoch 7006/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.7516 - val_loss: 3.4903\n",
      "Epoch 7007/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.9158 - val_loss: 3.4987\n",
      "Epoch 7008/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.7050 - val_loss: 3.4686\n",
      "Epoch 7009/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.5496 - val_loss: 3.3756\n",
      "Epoch 7010/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.4567 - val_loss: 3.4684\n",
      "Epoch 7011/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.6488 - val_loss: 3.5519\n",
      "Epoch 7012/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.7797 - val_loss: 3.5275\n",
      "Epoch 7013/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.8354 - val_loss: 3.6368\n",
      "Epoch 7014/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.4780 - val_loss: 3.5977\n",
      "Epoch 7015/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.8576 - val_loss: 3.5150\n",
      "Epoch 7016/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.5039 - val_loss: 3.4821\n",
      "Epoch 7017/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.8278 - val_loss: 3.5410\n",
      "Epoch 7018/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.7302 - val_loss: 3.6806\n",
      "Epoch 7019/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 7.9000 - val_loss: 3.7268\n",
      "Epoch 7020/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.8224 - val_loss: 3.8218\n",
      "Epoch 7021/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.7934 - val_loss: 3.7463\n",
      "Epoch 7022/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.5455 - val_loss: 3.3444\n",
      "Epoch 7023/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.2929 - val_loss: 3.3142\n",
      "Epoch 7024/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 8.3909 - val_loss: 3.3717\n",
      "Epoch 7025/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 7.9331 - val_loss: 3.4376\n",
      "Epoch 7026/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 7.4355 - val_loss: 3.5789\n",
      "Epoch 7027/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.7641 - val_loss: 3.6528\n",
      "Epoch 7028/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.8324 - val_loss: 3.6184\n",
      "Epoch 7029/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.5061 - val_loss: 3.5712\n",
      "Epoch 7030/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.7296 - val_loss: 3.6060\n",
      "Epoch 7031/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.7918 - val_loss: 3.6143\n",
      "Epoch 7032/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 8.0229 - val_loss: 3.6797\n",
      "Epoch 7033/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.7085 - val_loss: 3.5423\n",
      "Epoch 7034/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.8895 - val_loss: 3.5158\n",
      "Epoch 7035/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.6545 - val_loss: 3.5809\n",
      "Epoch 7036/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.2222 - val_loss: 3.6772\n",
      "Epoch 7037/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 7.4799 - val_loss: 3.6945\n",
      "Epoch 7038/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.8418 - val_loss: 3.5604\n",
      "Epoch 7039/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.3901 - val_loss: 3.5911\n",
      "Epoch 7040/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.5205 - val_loss: 3.5593\n",
      "Epoch 7041/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 7.6703 - val_loss: 3.5471\n",
      "Epoch 7042/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.9018 - val_loss: 3.4201\n",
      "Epoch 7043/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 7.5518 - val_loss: 3.5816\n",
      "Epoch 7044/10000\n",
      "90/90 [==============================] - 0s 203us/step - loss: 7.6190 - val_loss: 3.6033\n",
      "Epoch 7045/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 7.2904 - val_loss: 3.6332\n",
      "Epoch 7046/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 7.5814 - val_loss: 3.6585\n",
      "Epoch 7047/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 7.7074 - val_loss: 3.6092\n",
      "Epoch 7048/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 8.0851 - val_loss: 3.4960\n",
      "Epoch 7049/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.9038 - val_loss: 3.5117\n",
      "Epoch 7050/10000\n",
      "90/90 [==============================] - 0s 179us/step - loss: 7.4042 - val_loss: 3.6877\n",
      "Epoch 7051/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 7.6696 - val_loss: 3.6674\n",
      "Epoch 7052/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 8.3895 - val_loss: 3.6531\n",
      "Epoch 7053/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.5392 - val_loss: 3.7709\n",
      "Epoch 7054/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.8819 - val_loss: 3.7347\n",
      "Epoch 7055/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.4735 - val_loss: 3.7231\n",
      "Epoch 7056/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.2709 - val_loss: 3.7256\n",
      "Epoch 7057/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 8.0749 - val_loss: 3.5357\n",
      "Epoch 7058/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 8.0926 - val_loss: 3.5110\n",
      "Epoch 7059/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.9349 - val_loss: 3.5991\n",
      "Epoch 7060/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 7.5544 - val_loss: 3.5593\n",
      "Epoch 7061/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 7.4439 - val_loss: 3.6944\n",
      "Epoch 7062/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.2231 - val_loss: 3.6687\n",
      "Epoch 7063/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 7.7194 - val_loss: 3.6248\n",
      "Epoch 7064/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.8031 - val_loss: 3.5843\n",
      "Epoch 7065/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.6571 - val_loss: 3.6446\n",
      "Epoch 7066/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.5774 - val_loss: 3.7472\n",
      "Epoch 7067/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.8872 - val_loss: 3.7678\n",
      "Epoch 7068/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.6481 - val_loss: 3.6878\n",
      "Epoch 7069/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.9821 - val_loss: 3.6273\n",
      "Epoch 7070/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 7.4693 - val_loss: 3.6412\n",
      "Epoch 7071/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.1303 - val_loss: 3.7285\n",
      "Epoch 7072/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.7056 - val_loss: 3.5772\n",
      "Epoch 7073/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 8.1137 - val_loss: 3.6824\n",
      "Epoch 7074/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.7003 - val_loss: 3.7181\n",
      "Epoch 7075/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.5559 - val_loss: 3.8491\n",
      "Epoch 7076/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.5340 - val_loss: 3.8672\n",
      "Epoch 7077/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.2422 - val_loss: 3.9303\n",
      "Epoch 7078/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.4453 - val_loss: 3.8400\n",
      "Epoch 7079/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 8.1153 - val_loss: 3.6040\n",
      "Epoch 7080/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.6702 - val_loss: 3.5213\n",
      "Epoch 7081/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.5006 - val_loss: 3.5777\n",
      "Epoch 7082/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.8584 - val_loss: 3.5764\n",
      "Epoch 7083/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.1307 - val_loss: 3.5999\n",
      "Epoch 7084/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.6866 - val_loss: 3.5843\n",
      "Epoch 7085/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.6732 - val_loss: 3.7491\n",
      "Epoch 7086/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 7.5502 - val_loss: 3.8951\n",
      "Epoch 7087/10000\n",
      "90/90 [==============================] - 0s 228us/step - loss: 7.9337 - val_loss: 3.7207\n",
      "Epoch 7088/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.6349 - val_loss: 3.7120\n",
      "Epoch 7089/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.5472 - val_loss: 3.8267\n",
      "Epoch 7090/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.5031 - val_loss: 3.8023\n",
      "Epoch 7091/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 7.2439 - val_loss: 3.6408\n",
      "Epoch 7092/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 111us/step - loss: 7.8291 - val_loss: 3.6522\n",
      "Epoch 7093/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.4063 - val_loss: 3.6493\n",
      "Epoch 7094/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.5702 - val_loss: 3.6805\n",
      "Epoch 7095/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.6724 - val_loss: 3.8408\n",
      "Epoch 7096/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.5870 - val_loss: 3.8891\n",
      "Epoch 7097/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.7698 - val_loss: 3.8679\n",
      "Epoch 7098/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 7.6362 - val_loss: 3.7885\n",
      "Epoch 7099/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.8672 - val_loss: 3.7882\n",
      "Epoch 7100/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.6862 - val_loss: 3.6261\n",
      "Epoch 7101/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.9836 - val_loss: 3.5836\n",
      "Epoch 7102/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.4520 - val_loss: 3.7417\n",
      "Epoch 7103/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.6684 - val_loss: 3.9810\n",
      "Epoch 7104/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.5269 - val_loss: 3.9545\n",
      "Epoch 7105/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.5195 - val_loss: 3.8638\n",
      "Epoch 7106/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.7236 - val_loss: 3.7756\n",
      "Epoch 7107/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.3650 - val_loss: 3.8545\n",
      "Epoch 7108/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.2360 - val_loss: 3.8403\n",
      "Epoch 7109/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 8.0508 - val_loss: 3.7030\n",
      "Epoch 7110/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 7.6937 - val_loss: 3.6515\n",
      "Epoch 7111/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.8940 - val_loss: 3.7174\n",
      "Epoch 7112/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.8877 - val_loss: 3.7135\n",
      "Epoch 7113/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.5260 - val_loss: 3.8623\n",
      "Epoch 7114/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 7.1431 - val_loss: 3.9308\n",
      "Epoch 7115/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 8.0903 - val_loss: 3.7673\n",
      "Epoch 7116/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.2892 - val_loss: 3.7639\n",
      "Epoch 7117/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.5276 - val_loss: 3.7821\n",
      "Epoch 7118/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 7.3904 - val_loss: 3.9112\n",
      "Epoch 7119/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 7.2448 - val_loss: 3.9666\n",
      "Epoch 7120/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.4342 - val_loss: 3.9860\n",
      "Epoch 7121/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 7.7509 - val_loss: 3.8374\n",
      "Epoch 7122/10000\n",
      "90/90 [==============================] - 0s 187us/step - loss: 7.9266 - val_loss: 3.8004\n",
      "Epoch 7123/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.6920 - val_loss: 3.6469\n",
      "Epoch 7124/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 7.7601 - val_loss: 3.6401\n",
      "Epoch 7125/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 7.5397 - val_loss: 3.7398\n",
      "Epoch 7126/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.9719 - val_loss: 3.8191\n",
      "Epoch 7127/10000\n",
      "90/90 [==============================] - 0s 171us/step - loss: 7.8074 - val_loss: 3.8021\n",
      "Epoch 7128/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.4041 - val_loss: 3.8257\n",
      "Epoch 7129/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.1754 - val_loss: 4.0532\n",
      "Epoch 7130/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.8194 - val_loss: 3.9978\n",
      "Epoch 7131/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.1271 - val_loss: 3.8884\n",
      "Epoch 7132/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.4105 - val_loss: 3.8413\n",
      "Epoch 7133/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.7041 - val_loss: 3.8046\n",
      "Epoch 7134/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.3190 - val_loss: 3.8942\n",
      "Epoch 7135/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.6547 - val_loss: 3.7137\n",
      "Epoch 7136/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.6266 - val_loss: 3.6585\n",
      "Epoch 7137/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.6377 - val_loss: 3.7183\n",
      "Epoch 7138/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.4476 - val_loss: 4.0028\n",
      "Epoch 7139/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.5642 - val_loss: 3.9584\n",
      "Epoch 7140/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.1978 - val_loss: 4.0178\n",
      "Epoch 7141/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.6050 - val_loss: 3.9469\n",
      "Epoch 7142/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.9884 - val_loss: 3.8657\n",
      "Epoch 7143/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.5981 - val_loss: 3.9579\n",
      "Epoch 7144/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.4601 - val_loss: 4.0284\n",
      "Epoch 7145/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.6468 - val_loss: 3.8948\n",
      "Epoch 7146/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.3544 - val_loss: 3.8149\n",
      "Epoch 7147/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.5833 - val_loss: 3.7425\n",
      "Epoch 7148/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.4796 - val_loss: 3.8280\n",
      "Epoch 7149/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.4900 - val_loss: 3.8442\n",
      "Epoch 7150/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.6908 - val_loss: 3.9646\n",
      "Epoch 7151/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.5534 - val_loss: 3.9188\n",
      "Epoch 7152/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.3174 - val_loss: 3.8354\n",
      "Epoch 7153/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.5111 - val_loss: 3.7794\n",
      "Epoch 7154/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.5607 - val_loss: 3.7294\n",
      "Epoch 7155/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.2521 - val_loss: 3.8576\n",
      "Epoch 7156/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.4677 - val_loss: 4.0105\n",
      "Epoch 7157/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.4240 - val_loss: 4.0206\n",
      "Epoch 7158/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.7961 - val_loss: 4.1829\n",
      "Epoch 7159/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.3280 - val_loss: 4.0958\n",
      "Epoch 7160/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.3267 - val_loss: 4.0476\n",
      "Epoch 7161/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.4078 - val_loss: 3.9488\n",
      "Epoch 7162/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.5012 - val_loss: 3.8211\n",
      "Epoch 7163/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.3816 - val_loss: 3.8225\n",
      "Epoch 7164/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.5773 - val_loss: 3.7857\n",
      "Epoch 7165/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.3147 - val_loss: 3.9491\n",
      "Epoch 7166/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.4606 - val_loss: 3.9116\n",
      "Epoch 7167/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.3288 - val_loss: 3.8330\n",
      "Epoch 7168/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 7.3195 - val_loss: 3.8771\n",
      "Epoch 7169/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.4995 - val_loss: 3.9450\n",
      "Epoch 7170/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.3105 - val_loss: 3.9586\n",
      "Epoch 7171/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 7.7266 - val_loss: 4.0114\n",
      "Epoch 7172/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 7.5452 - val_loss: 4.0075\n",
      "Epoch 7173/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.8552 - val_loss: 4.0267\n",
      "Epoch 7174/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 7.6874 - val_loss: 4.0938\n",
      "Epoch 7175/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.2574 - val_loss: 4.0674\n",
      "Epoch 7176/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.5394 - val_loss: 3.8861\n",
      "Epoch 7177/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.9331 - val_loss: 3.8222\n",
      "Epoch 7178/10000\n",
      "90/90 [==============================] - 0s 321us/step - loss: 7.7527 - val_loss: 3.8578\n",
      "Epoch 7179/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 8.0191 - val_loss: 3.9644\n",
      "Epoch 7180/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.1678 - val_loss: 4.1063\n",
      "Epoch 7181/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.9277 - val_loss: 4.0763\n",
      "Epoch 7182/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.3320 - val_loss: 3.9305\n",
      "Epoch 7183/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.2838 - val_loss: 3.8736\n",
      "Epoch 7184/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.6566 - val_loss: 3.8223\n",
      "Epoch 7185/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.3795 - val_loss: 4.0098\n",
      "Epoch 7186/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.4207 - val_loss: 4.0326\n",
      "Epoch 7187/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 7.3609 - val_loss: 4.0358\n",
      "Epoch 7188/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.2438 - val_loss: 4.1225\n",
      "Epoch 7189/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.9246 - val_loss: 4.1264\n",
      "Epoch 7190/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.8104 - val_loss: 3.9284\n",
      "Epoch 7191/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.3089 - val_loss: 3.8696\n",
      "Epoch 7192/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.2333 - val_loss: 3.9068\n",
      "Epoch 7193/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.2707 - val_loss: 3.9747\n",
      "Epoch 7194/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.1521 - val_loss: 3.9996\n",
      "Epoch 7195/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.5764 - val_loss: 4.0538\n",
      "Epoch 7196/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.4234 - val_loss: 4.1689\n",
      "Epoch 7197/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.6223 - val_loss: 4.0707\n",
      "Epoch 7198/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.3061 - val_loss: 4.1223\n",
      "Epoch 7199/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.5082 - val_loss: 3.9775\n",
      "Epoch 7200/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.6836 - val_loss: 4.1047\n",
      "Epoch 7201/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.3336 - val_loss: 4.0252\n",
      "Epoch 7202/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.0982 - val_loss: 4.0450\n",
      "Epoch 7203/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.5794 - val_loss: 3.9309\n",
      "Epoch 7204/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.8240 - val_loss: 3.9344\n",
      "Epoch 7205/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.3435 - val_loss: 4.0460\n",
      "Epoch 7206/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.3415 - val_loss: 4.1431\n",
      "Epoch 7207/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.4212 - val_loss: 4.1016\n",
      "Epoch 7208/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.5850 - val_loss: 3.9671\n",
      "Epoch 7209/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.1241 - val_loss: 4.0027\n",
      "Epoch 7210/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.5197 - val_loss: 4.0531\n",
      "Epoch 7211/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.5232 - val_loss: 4.0145\n",
      "Epoch 7212/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.3812 - val_loss: 3.9760\n",
      "Epoch 7213/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.2280 - val_loss: 4.0073\n",
      "Epoch 7214/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 7.0496 - val_loss: 4.0700\n",
      "Epoch 7215/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.9132 - val_loss: 4.0597\n",
      "Epoch 7216/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.2896 - val_loss: 3.9596\n",
      "Epoch 7217/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.5951 - val_loss: 4.0116\n",
      "Epoch 7218/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.3776 - val_loss: 4.1884\n",
      "Epoch 7219/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.2976 - val_loss: 4.3307\n",
      "Epoch 7220/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 7.3490 - val_loss: 4.4278\n",
      "Epoch 7221/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 7.5262 - val_loss: 4.1710\n",
      "Epoch 7222/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 7.7932 - val_loss: 3.9460\n",
      "Epoch 7223/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.1732 - val_loss: 4.0969\n",
      "Epoch 7224/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.2346 - val_loss: 4.1130\n",
      "Epoch 7225/10000\n",
      "90/90 [==============================] - 0s 173us/step - loss: 7.1468 - val_loss: 4.0061\n",
      "Epoch 7226/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.2344 - val_loss: 3.8715\n",
      "Epoch 7227/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.5816 - val_loss: 3.9592\n",
      "Epoch 7228/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.7156 - val_loss: 4.0549\n",
      "Epoch 7229/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.3285 - val_loss: 4.2052\n",
      "Epoch 7230/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 7.3009 - val_loss: 4.3616\n",
      "Epoch 7231/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.2292 - val_loss: 4.2639\n",
      "Epoch 7232/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.1365 - val_loss: 4.1538\n",
      "Epoch 7233/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 7.8209 - val_loss: 4.1102\n",
      "Epoch 7234/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.4237 - val_loss: 4.0372\n",
      "Epoch 7235/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.0745 - val_loss: 3.9631\n",
      "Epoch 7236/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.3126 - val_loss: 3.9862\n",
      "Epoch 7237/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.8066 - val_loss: 4.0747\n",
      "Epoch 7238/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 7.4344 - val_loss: 4.2108\n",
      "Epoch 7239/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.4113 - val_loss: 4.2635\n",
      "Epoch 7240/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.2804 - val_loss: 4.3340\n",
      "Epoch 7241/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.7985 - val_loss: 4.3809\n",
      "Epoch 7242/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.2699 - val_loss: 4.1567\n",
      "Epoch 7243/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.9615 - val_loss: 4.0939\n",
      "Epoch 7244/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 7.2619 - val_loss: 4.1663\n",
      "Epoch 7245/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.2624 - val_loss: 4.0164\n",
      "Epoch 7246/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 101us/step - loss: 7.5349 - val_loss: 4.0131\n",
      "Epoch 7247/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.3383 - val_loss: 4.0910\n",
      "Epoch 7248/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 7.1988 - val_loss: 4.1374\n",
      "Epoch 7249/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.1174 - val_loss: 4.2533\n",
      "Epoch 7250/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.1986 - val_loss: 4.2275\n",
      "Epoch 7251/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.9911 - val_loss: 4.1845\n",
      "Epoch 7252/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.9386 - val_loss: 4.1083\n",
      "Epoch 7253/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.1464 - val_loss: 4.0308\n",
      "Epoch 7254/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.2244 - val_loss: 4.1003\n",
      "Epoch 7255/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.5071 - val_loss: 4.2074\n",
      "Epoch 7256/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.3958 - val_loss: 4.2932\n",
      "Epoch 7257/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.2972 - val_loss: 4.4495\n",
      "Epoch 7258/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.1173 - val_loss: 4.3083\n",
      "Epoch 7259/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 7.1167 - val_loss: 4.1221\n",
      "Epoch 7260/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.0176 - val_loss: 3.9227\n",
      "Epoch 7261/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.3092 - val_loss: 3.9414\n",
      "Epoch 7262/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.3745 - val_loss: 4.1019\n",
      "Epoch 7263/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.1081 - val_loss: 4.2353\n",
      "Epoch 7264/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.0307 - val_loss: 4.3069\n",
      "Epoch 7265/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.8172 - val_loss: 4.0841\n",
      "Epoch 7266/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 7.1594 - val_loss: 4.0961\n",
      "Epoch 7267/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.2258 - val_loss: 4.1481\n",
      "Epoch 7268/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 7.3390 - val_loss: 4.2778\n",
      "Epoch 7269/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.3334 - val_loss: 4.1158\n",
      "Epoch 7270/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.5112 - val_loss: 4.4065\n",
      "Epoch 7271/10000\n",
      "90/90 [==============================] - 0s 310us/step - loss: 7.3738 - val_loss: 4.2723\n",
      "Epoch 7272/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.0990 - val_loss: 4.4166\n",
      "Epoch 7273/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.3357 - val_loss: 4.2170\n",
      "Epoch 7274/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.4129 - val_loss: 4.0881\n",
      "Epoch 7275/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.1878 - val_loss: 4.0990\n",
      "Epoch 7276/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.8577 - val_loss: 4.2648\n",
      "Epoch 7277/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.0707 - val_loss: 4.4683\n",
      "Epoch 7278/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.1570 - val_loss: 4.2698\n",
      "Epoch 7279/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.2138 - val_loss: 4.3447\n",
      "Epoch 7280/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.5084 - val_loss: 4.5088\n",
      "Epoch 7281/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.2546 - val_loss: 4.2694\n",
      "Epoch 7282/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.1914 - val_loss: 4.2287\n",
      "Epoch 7283/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.2328 - val_loss: 4.1176\n",
      "Epoch 7284/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.6282 - val_loss: 4.1328\n",
      "Epoch 7285/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.2316 - val_loss: 4.1542\n",
      "Epoch 7286/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.6766 - val_loss: 4.3675\n",
      "Epoch 7287/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.4622 - val_loss: 4.3343\n",
      "Epoch 7288/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.3872 - val_loss: 4.2880\n",
      "Epoch 7289/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.2393 - val_loss: 4.1331\n",
      "Epoch 7290/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.3120 - val_loss: 4.1612\n",
      "Epoch 7291/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.0176 - val_loss: 4.2191\n",
      "Epoch 7292/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.8920 - val_loss: 4.2997\n",
      "Epoch 7293/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.1823 - val_loss: 4.3624\n",
      "Epoch 7294/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.5840 - val_loss: 4.2331\n",
      "Epoch 7295/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.1391 - val_loss: 4.3433\n",
      "Epoch 7296/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.2750 - val_loss: 4.4352\n",
      "Epoch 7297/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.2265 - val_loss: 4.4142\n",
      "Epoch 7298/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.1019 - val_loss: 4.3891\n",
      "Epoch 7299/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 7.0736 - val_loss: 4.2009\n",
      "Epoch 7300/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.5153 - val_loss: 4.2387\n",
      "Epoch 7301/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.8505 - val_loss: 4.4514\n",
      "Epoch 7302/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 7.1398 - val_loss: 4.3569\n",
      "Epoch 7303/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.5428 - val_loss: 4.1788\n",
      "Epoch 7304/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.1874 - val_loss: 4.1837\n",
      "Epoch 7305/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.9990 - val_loss: 4.1344\n",
      "Epoch 7306/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 7.5996 - val_loss: 4.2041\n",
      "Epoch 7307/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.3836 - val_loss: 4.2217\n",
      "Epoch 7308/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.9172 - val_loss: 4.3042\n",
      "Epoch 7309/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.9415 - val_loss: 4.4486\n",
      "Epoch 7310/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 6.8036 - val_loss: 4.3070\n",
      "Epoch 7311/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.2089 - val_loss: 4.1196\n",
      "Epoch 7312/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.5952 - val_loss: 4.1488\n",
      "Epoch 7313/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.8566 - val_loss: 4.3494\n",
      "Epoch 7314/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.2745 - val_loss: 4.4080\n",
      "Epoch 7315/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.0092 - val_loss: 4.3281\n",
      "Epoch 7316/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.5608 - val_loss: 4.2171\n",
      "Epoch 7317/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.4030 - val_loss: 4.2910\n",
      "Epoch 7318/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.1480 - val_loss: 4.4507\n",
      "Epoch 7319/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.1110 - val_loss: 4.3620\n",
      "Epoch 7320/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.6177 - val_loss: 4.2216\n",
      "Epoch 7321/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.2373 - val_loss: 4.1541\n",
      "Epoch 7322/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.9803 - val_loss: 4.3963\n",
      "Epoch 7323/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.3332 - val_loss: 4.3391\n",
      "Epoch 7324/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 7.3417 - val_loss: 4.3792\n",
      "Epoch 7325/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.3421 - val_loss: 4.2946\n",
      "Epoch 7326/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.1503 - val_loss: 4.3697\n",
      "Epoch 7327/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.9970 - val_loss: 4.4438\n",
      "Epoch 7328/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.1442 - val_loss: 4.5038\n",
      "Epoch 7329/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.9358 - val_loss: 4.3891\n",
      "Epoch 7330/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.0321 - val_loss: 4.2943\n",
      "Epoch 7331/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.0907 - val_loss: 4.2902\n",
      "Epoch 7332/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.9955 - val_loss: 4.3483\n",
      "Epoch 7333/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.5562 - val_loss: 4.5086\n",
      "Epoch 7334/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.2867 - val_loss: 4.3429\n",
      "Epoch 7335/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.1045 - val_loss: 4.3460\n",
      "Epoch 7336/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.2803 - val_loss: 4.2530\n",
      "Epoch 7337/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.1004 - val_loss: 4.3440\n",
      "Epoch 7338/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.2455 - val_loss: 4.4019\n",
      "Epoch 7339/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.1235 - val_loss: 4.3798\n",
      "Epoch 7340/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.9847 - val_loss: 4.6591\n",
      "Epoch 7341/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.0289 - val_loss: 4.7569\n",
      "Epoch 7342/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.2253 - val_loss: 4.5053\n",
      "Epoch 7343/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.9740 - val_loss: 4.2686\n",
      "Epoch 7344/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.1213 - val_loss: 4.0374\n",
      "Epoch 7345/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.3201 - val_loss: 4.2624\n",
      "Epoch 7346/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.4901 - val_loss: 4.3442\n",
      "Epoch 7347/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.2538 - val_loss: 4.5701\n",
      "Epoch 7348/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.3049 - val_loss: 4.5912\n",
      "Epoch 7349/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.0897 - val_loss: 4.5734\n",
      "Epoch 7350/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.7837 - val_loss: 4.3188\n",
      "Epoch 7351/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.1945 - val_loss: 4.0953\n",
      "Epoch 7352/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.8506 - val_loss: 4.2435\n",
      "Epoch 7353/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.2972 - val_loss: 4.4951\n",
      "Epoch 7354/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.8919 - val_loss: 4.5465\n",
      "Epoch 7355/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.2236 - val_loss: 4.5208\n",
      "Epoch 7356/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.0481 - val_loss: 4.5389\n",
      "Epoch 7357/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.1946 - val_loss: 4.5988\n",
      "Epoch 7358/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.3140 - val_loss: 4.5240\n",
      "Epoch 7359/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.2211 - val_loss: 4.3827\n",
      "Epoch 7360/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.9538 - val_loss: 4.4347\n",
      "Epoch 7361/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.6135 - val_loss: 4.4246\n",
      "Epoch 7362/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.2000 - val_loss: 4.6367\n",
      "Epoch 7363/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.2594 - val_loss: 4.5751\n",
      "Epoch 7364/10000\n",
      "90/90 [==============================] - 0s 293us/step - loss: 7.0871 - val_loss: 4.4923\n",
      "Epoch 7365/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.3472 - val_loss: 4.3636\n",
      "Epoch 7366/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.4082 - val_loss: 4.1983\n",
      "Epoch 7367/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.4811 - val_loss: 4.2527\n",
      "Epoch 7368/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.6363 - val_loss: 4.6155\n",
      "Epoch 7369/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.2728 - val_loss: 4.5017\n",
      "Epoch 7370/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.9108 - val_loss: 4.4939\n",
      "Epoch 7371/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 6.9733 - val_loss: 4.4897\n",
      "Epoch 7372/10000\n",
      "90/90 [==============================] - 0s 189us/step - loss: 7.2106 - val_loss: 4.5517\n",
      "Epoch 7373/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.9060 - val_loss: 4.7528\n",
      "Epoch 7374/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.0734 - val_loss: 4.5726\n",
      "Epoch 7375/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.2577 - val_loss: 4.4835\n",
      "Epoch 7376/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 7.1581 - val_loss: 4.2526\n",
      "Epoch 7377/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.1365 - val_loss: 4.2152\n",
      "Epoch 7378/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.1438 - val_loss: 4.2087\n",
      "Epoch 7379/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.1079 - val_loss: 4.3888\n",
      "Epoch 7380/10000\n",
      "90/90 [==============================] - 0s 81us/step - loss: 6.8632 - val_loss: 4.4698\n",
      "Epoch 7381/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.7384 - val_loss: 4.5931\n",
      "Epoch 7382/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.6906 - val_loss: 4.7950\n",
      "Epoch 7383/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 7.2441 - val_loss: 4.8828\n",
      "Epoch 7384/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.9260 - val_loss: 4.7169\n",
      "Epoch 7385/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.2288 - val_loss: 4.5053\n",
      "Epoch 7386/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 7.1034 - val_loss: 4.3754\n",
      "Epoch 7387/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 7.1005 - val_loss: 4.3588\n",
      "Epoch 7388/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 6.9809 - val_loss: 4.4146\n",
      "Epoch 7389/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.9476 - val_loss: 4.3248\n",
      "Epoch 7390/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 7.1632 - val_loss: 4.4589\n",
      "Epoch 7391/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 7.0545 - val_loss: 4.5496\n",
      "Epoch 7392/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 6.7321 - val_loss: 4.6238\n",
      "Epoch 7393/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.3965 - val_loss: 4.4357\n",
      "Epoch 7394/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.0672 - val_loss: 4.4884\n",
      "Epoch 7395/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.9358 - val_loss: 4.5457\n",
      "Epoch 7396/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 7.4521 - val_loss: 4.7349\n",
      "Epoch 7397/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.4240 - val_loss: 4.7293\n",
      "Epoch 7398/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.8922 - val_loss: 4.6321\n",
      "Epoch 7399/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 7.2558 - val_loss: 4.4310\n",
      "Epoch 7400/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.9627 - val_loss: 4.5912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7401/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.9357 - val_loss: 4.5749\n",
      "Epoch 7402/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.8680 - val_loss: 4.3843\n",
      "Epoch 7403/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 7.1947 - val_loss: 4.4663\n",
      "Epoch 7404/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.2511 - val_loss: 4.6521\n",
      "Epoch 7405/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.7642 - val_loss: 4.5358\n",
      "Epoch 7406/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 7.1581 - val_loss: 4.4303\n",
      "Epoch 7407/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.9202 - val_loss: 4.5048\n",
      "Epoch 7408/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.6990 - val_loss: 4.5466\n",
      "Epoch 7409/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.2018 - val_loss: 4.6109\n",
      "Epoch 7410/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.1239 - val_loss: 4.6591\n",
      "Epoch 7411/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 7.3089 - val_loss: 4.5508\n",
      "Epoch 7412/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 7.1833 - val_loss: 4.3558\n",
      "Epoch 7413/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.1318 - val_loss: 4.3426\n",
      "Epoch 7414/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.0005 - val_loss: 4.6041\n",
      "Epoch 7415/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.6540 - val_loss: 4.7212\n",
      "Epoch 7416/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 7.4312 - val_loss: 4.5587\n",
      "Epoch 7417/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.9764 - val_loss: 4.3921\n",
      "Epoch 7418/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.5844 - val_loss: 4.5774\n",
      "Epoch 7419/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 7.4121 - val_loss: 4.5951\n",
      "Epoch 7420/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.8992 - val_loss: 4.7691\n",
      "Epoch 7421/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 7.1192 - val_loss: 4.8327\n",
      "Epoch 7422/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 7.2362 - val_loss: 4.6856\n",
      "Epoch 7423/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 7.1936 - val_loss: 4.7806\n",
      "Epoch 7424/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.8312 - val_loss: 4.6659\n",
      "Epoch 7425/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.3703 - val_loss: 4.5376\n",
      "Epoch 7426/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.8416 - val_loss: 4.4871\n",
      "Epoch 7427/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.1500 - val_loss: 4.3789\n",
      "Epoch 7428/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.0850 - val_loss: 4.2934\n",
      "Epoch 7429/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.1464 - val_loss: 4.5177\n",
      "Epoch 7430/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 7.1311 - val_loss: 4.6472\n",
      "Epoch 7431/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.2074 - val_loss: 4.6987\n",
      "Epoch 7432/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.8753 - val_loss: 4.8975\n",
      "Epoch 7433/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 7.3281 - val_loss: 4.6966\n",
      "Epoch 7434/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.8884 - val_loss: 4.7860\n",
      "Epoch 7435/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.7410 - val_loss: 4.7408\n",
      "Epoch 7436/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.8023 - val_loss: 4.8489\n",
      "Epoch 7437/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.1223 - val_loss: 4.8799\n",
      "Epoch 7438/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 7.3604 - val_loss: 4.5787\n",
      "Epoch 7439/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.9528 - val_loss: 4.5578\n",
      "Epoch 7440/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.1825 - val_loss: 4.6169\n",
      "Epoch 7441/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.2014 - val_loss: 4.3947\n",
      "Epoch 7442/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.9424 - val_loss: 4.3990\n",
      "Epoch 7443/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.2022 - val_loss: 4.4725\n",
      "Epoch 7444/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.5803 - val_loss: 4.7261\n",
      "Epoch 7445/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.7536 - val_loss: 4.8983\n",
      "Epoch 7446/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 6.9009 - val_loss: 4.8327\n",
      "Epoch 7447/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.9272 - val_loss: 4.7289\n",
      "Epoch 7448/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.9300 - val_loss: 4.7928\n",
      "Epoch 7449/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.4009 - val_loss: 4.6519\n",
      "Epoch 7450/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 7.2153 - val_loss: 4.5880\n",
      "Epoch 7451/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.7042 - val_loss: 4.7347\n",
      "Epoch 7452/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.4248 - val_loss: 4.6618\n",
      "Epoch 7453/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 7.1097 - val_loss: 4.4658\n",
      "Epoch 7454/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.8938 - val_loss: 4.4609\n",
      "Epoch 7455/10000\n",
      "90/90 [==============================] - 0s 303us/step - loss: 6.3936 - val_loss: 4.6742\n",
      "Epoch 7456/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.6922 - val_loss: 5.0435\n",
      "Epoch 7457/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 7.2510 - val_loss: 4.8832\n",
      "Epoch 7458/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.1811 - val_loss: 4.7153\n",
      "Epoch 7459/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.8435 - val_loss: 4.8034\n",
      "Epoch 7460/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.4643 - val_loss: 4.7721\n",
      "Epoch 7461/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.5420 - val_loss: 4.7136\n",
      "Epoch 7462/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.9343 - val_loss: 4.6450\n",
      "Epoch 7463/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.7728 - val_loss: 4.4888\n",
      "Epoch 7464/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.9633 - val_loss: 4.6417\n",
      "Epoch 7465/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.8948 - val_loss: 4.7501\n",
      "Epoch 7466/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.3881 - val_loss: 4.8174\n",
      "Epoch 7467/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.8879 - val_loss: 4.9418\n",
      "Epoch 7468/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.2862 - val_loss: 4.9125\n",
      "Epoch 7469/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.9719 - val_loss: 4.9805\n",
      "Epoch 7470/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.0677 - val_loss: 4.6591\n",
      "Epoch 7471/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.8587 - val_loss: 4.4851\n",
      "Epoch 7472/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.8104 - val_loss: 4.5672\n",
      "Epoch 7473/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 7.5684 - val_loss: 4.6306\n",
      "Epoch 7474/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.9263 - val_loss: 4.8612\n",
      "Epoch 7475/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.9745 - val_loss: 4.8177\n",
      "Epoch 7476/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.7463 - val_loss: 4.6566\n",
      "Epoch 7477/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.9068 - val_loss: 4.6211\n",
      "Epoch 7478/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.0279 - val_loss: 4.7152\n",
      "Epoch 7479/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 7.1054 - val_loss: 4.6914\n",
      "Epoch 7480/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.3574 - val_loss: 4.7135\n",
      "Epoch 7481/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.2742 - val_loss: 4.7353\n",
      "Epoch 7482/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.7680 - val_loss: 4.7109\n",
      "Epoch 7483/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.5011 - val_loss: 4.7196\n",
      "Epoch 7484/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.0428 - val_loss: 4.9395\n",
      "Epoch 7485/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.9743 - val_loss: 5.0243\n",
      "Epoch 7486/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.7714 - val_loss: 5.0420\n",
      "Epoch 7487/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.9281 - val_loss: 4.7653\n",
      "Epoch 7488/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.6161 - val_loss: 4.8485\n",
      "Epoch 7489/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 7.0501 - val_loss: 4.8186\n",
      "Epoch 7490/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 7.2441 - val_loss: 4.6879\n",
      "Epoch 7491/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.8512 - val_loss: 4.5207\n",
      "Epoch 7492/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.9737 - val_loss: 4.5556\n",
      "Epoch 7493/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.7311 - val_loss: 4.5375\n",
      "Epoch 7494/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.9611 - val_loss: 4.7551\n",
      "Epoch 7495/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.2539 - val_loss: 4.9533\n",
      "Epoch 7496/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 7.2188 - val_loss: 5.3046\n",
      "Epoch 7497/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.6808 - val_loss: 5.1320\n",
      "Epoch 7498/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.1984 - val_loss: 4.6292\n",
      "Epoch 7499/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.6927 - val_loss: 4.5662\n",
      "Epoch 7500/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.9558 - val_loss: 4.4788\n",
      "Epoch 7501/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.4996 - val_loss: 4.7301\n",
      "Epoch 7502/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.7954 - val_loss: 5.0021\n",
      "Epoch 7503/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 7.0480 - val_loss: 4.9314\n",
      "Epoch 7504/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.5798 - val_loss: 4.9918\n",
      "Epoch 7505/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 7.0411 - val_loss: 4.6962\n",
      "Epoch 7506/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.7933 - val_loss: 4.5857\n",
      "Epoch 7507/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.9825 - val_loss: 4.6062\n",
      "Epoch 7508/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 7.1753 - val_loss: 4.7223\n",
      "Epoch 7509/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.7351 - val_loss: 4.7921\n",
      "Epoch 7510/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.9327 - val_loss: 4.7833\n",
      "Epoch 7511/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.9736 - val_loss: 4.9358\n",
      "Epoch 7512/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.5415 - val_loss: 5.1269\n",
      "Epoch 7513/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 7.0170 - val_loss: 5.1975\n",
      "Epoch 7514/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.1693 - val_loss: 4.9232\n",
      "Epoch 7515/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 7.0773 - val_loss: 4.6714\n",
      "Epoch 7516/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 7.2652 - val_loss: 4.5855\n",
      "Epoch 7517/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.8999 - val_loss: 4.6678\n",
      "Epoch 7518/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.3838 - val_loss: 4.9685\n",
      "Epoch 7519/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.8450 - val_loss: 4.9517\n",
      "Epoch 7520/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.8823 - val_loss: 4.9325\n",
      "Epoch 7521/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.7907 - val_loss: 4.8622\n",
      "Epoch 7522/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 7.1949 - val_loss: 4.8685\n",
      "Epoch 7523/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.8554 - val_loss: 4.9439\n",
      "Epoch 7524/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.7624 - val_loss: 4.8924\n",
      "Epoch 7525/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.2633 - val_loss: 4.8535\n",
      "Epoch 7526/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.8991 - val_loss: 4.8128\n",
      "Epoch 7527/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.8945 - val_loss: 4.9592\n",
      "Epoch 7528/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.2446 - val_loss: 4.9034\n",
      "Epoch 7529/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.9967 - val_loss: 5.1671\n",
      "Epoch 7530/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.5771 - val_loss: 5.0333\n",
      "Epoch 7531/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.8500 - val_loss: 4.6018\n",
      "Epoch 7532/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.7381 - val_loss: 4.5749\n",
      "Epoch 7533/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.7693 - val_loss: 4.8286\n",
      "Epoch 7534/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.1901 - val_loss: 4.8626\n",
      "Epoch 7535/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.8171 - val_loss: 5.1037\n",
      "Epoch 7536/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.6464 - val_loss: 5.1128\n",
      "Epoch 7537/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.3447 - val_loss: 4.8629\n",
      "Epoch 7538/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.9822 - val_loss: 4.8061\n",
      "Epoch 7539/10000\n",
      "90/90 [==============================] - 0s 301us/step - loss: 6.8431 - val_loss: 4.7882\n",
      "Epoch 7540/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.1944 - val_loss: 4.8566\n",
      "Epoch 7541/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.8429 - val_loss: 4.9479\n",
      "Epoch 7542/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 7.3387 - val_loss: 5.0218\n",
      "Epoch 7543/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.8969 - val_loss: 5.0076\n",
      "Epoch 7544/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.2291 - val_loss: 4.9889\n",
      "Epoch 7545/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 7.0991 - val_loss: 5.1631\n",
      "Epoch 7546/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.9484 - val_loss: 5.0314\n",
      "Epoch 7547/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.8345 - val_loss: 5.0482\n",
      "Epoch 7548/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.9932 - val_loss: 5.1346\n",
      "Epoch 7549/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.5002 - val_loss: 5.0481\n",
      "Epoch 7550/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.8170 - val_loss: 4.9455\n",
      "Epoch 7551/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.5486 - val_loss: 4.9270\n",
      "Epoch 7552/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.7336 - val_loss: 4.9009\n",
      "Epoch 7553/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.6790 - val_loss: 4.8480\n",
      "Epoch 7554/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.7237 - val_loss: 4.8416\n",
      "Epoch 7555/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 102us/step - loss: 6.8323 - val_loss: 4.9390\n",
      "Epoch 7556/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 7.0051 - val_loss: 4.7194\n",
      "Epoch 7557/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.6827 - val_loss: 4.9691\n",
      "Epoch 7558/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 7.2011 - val_loss: 4.6430\n",
      "Epoch 7559/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.1573 - val_loss: 4.7225\n",
      "Epoch 7560/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.9138 - val_loss: 5.1660\n",
      "Epoch 7561/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.8597 - val_loss: 4.9830\n",
      "Epoch 7562/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.6444 - val_loss: 4.9955\n",
      "Epoch 7563/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.7532 - val_loss: 4.8980\n",
      "Epoch 7564/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.8534 - val_loss: 5.0017\n",
      "Epoch 7565/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 7.0072 - val_loss: 5.4864\n",
      "Epoch 7566/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.5974 - val_loss: 5.5151\n",
      "Epoch 7567/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.0167 - val_loss: 4.9079\n",
      "Epoch 7568/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5981 - val_loss: 4.7563\n",
      "Epoch 7569/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 7.0652 - val_loss: 4.4900\n",
      "Epoch 7570/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.8052 - val_loss: 4.6592\n",
      "Epoch 7571/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.7709 - val_loss: 4.7472\n",
      "Epoch 7572/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.8677 - val_loss: 4.6809\n",
      "Epoch 7573/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.8836 - val_loss: 4.6504\n",
      "Epoch 7574/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.6516 - val_loss: 4.8631\n",
      "Epoch 7575/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.6897 - val_loss: 5.0940\n",
      "Epoch 7576/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.1715 - val_loss: 5.4288\n",
      "Epoch 7577/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.7045 - val_loss: 5.4132\n",
      "Epoch 7578/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.6679 - val_loss: 5.2541\n",
      "Epoch 7579/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.7911 - val_loss: 4.8677\n",
      "Epoch 7580/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.9980 - val_loss: 4.8327\n",
      "Epoch 7581/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 7.4778 - val_loss: 4.8219\n",
      "Epoch 7582/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.9441 - val_loss: 4.9393\n",
      "Epoch 7583/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.8742 - val_loss: 5.2369\n",
      "Epoch 7584/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.9060 - val_loss: 5.2342\n",
      "Epoch 7585/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 7.0039 - val_loss: 4.9326\n",
      "Epoch 7586/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.8075 - val_loss: 4.9184\n",
      "Epoch 7587/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.6581 - val_loss: 5.1476\n",
      "Epoch 7588/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 7.0290 - val_loss: 5.2857\n",
      "Epoch 7589/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.8957 - val_loss: 5.3870\n",
      "Epoch 7590/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 6.7814 - val_loss: 5.0164\n",
      "Epoch 7591/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.8675 - val_loss: 4.8022\n",
      "Epoch 7592/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.5965 - val_loss: 4.8257\n",
      "Epoch 7593/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.7177 - val_loss: 4.8739\n",
      "Epoch 7594/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.7318 - val_loss: 4.8901\n",
      "Epoch 7595/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.4392 - val_loss: 4.8493\n",
      "Epoch 7596/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.9234 - val_loss: 5.1398\n",
      "Epoch 7597/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.8670 - val_loss: 5.1279\n",
      "Epoch 7598/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.0220 - val_loss: 5.2033\n",
      "Epoch 7599/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 7.1832 - val_loss: 5.1562\n",
      "Epoch 7600/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.9454 - val_loss: 5.3324\n",
      "Epoch 7601/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.8097 - val_loss: 5.0273\n",
      "Epoch 7602/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.7214 - val_loss: 4.9042\n",
      "Epoch 7603/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 7.1977 - val_loss: 4.8944\n",
      "Epoch 7604/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.8333 - val_loss: 4.9588\n",
      "Epoch 7605/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.9892 - val_loss: 5.2853\n",
      "Epoch 7606/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.9654 - val_loss: 5.3013\n",
      "Epoch 7607/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.9888 - val_loss: 5.0672\n",
      "Epoch 7608/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.8551 - val_loss: 5.1992\n",
      "Epoch 7609/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.5234 - val_loss: 5.0268\n",
      "Epoch 7610/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.9586 - val_loss: 4.8212\n",
      "Epoch 7611/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.8723 - val_loss: 4.8971\n",
      "Epoch 7612/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.7241 - val_loss: 5.1539\n",
      "Epoch 7613/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.8944 - val_loss: 5.4417\n",
      "Epoch 7614/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.8604 - val_loss: 5.3286\n",
      "Epoch 7615/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.7219 - val_loss: 5.4217\n",
      "Epoch 7616/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.6249 - val_loss: 5.1571\n",
      "Epoch 7617/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.3852 - val_loss: 5.0221\n",
      "Epoch 7618/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 7.1097 - val_loss: 4.7594\n",
      "Epoch 7619/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.2109 - val_loss: 4.8972\n",
      "Epoch 7620/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.7312 - val_loss: 5.2298\n",
      "Epoch 7621/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.6787 - val_loss: 5.4016\n",
      "Epoch 7622/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.9675 - val_loss: 5.0834\n",
      "Epoch 7623/10000\n",
      "90/90 [==============================] - 0s 168us/step - loss: 7.0366 - val_loss: 4.9559\n",
      "Epoch 7624/10000\n",
      "90/90 [==============================] - 0s 318us/step - loss: 6.6248 - val_loss: 4.9419\n",
      "Epoch 7625/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.7090 - val_loss: 5.0245\n",
      "Epoch 7626/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.6653 - val_loss: 4.9650\n",
      "Epoch 7627/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.6015 - val_loss: 5.0445\n",
      "Epoch 7628/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 7.0329 - val_loss: 5.0678\n",
      "Epoch 7629/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.6233 - val_loss: 5.2379\n",
      "Epoch 7630/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.4768 - val_loss: 5.2019\n",
      "Epoch 7631/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.8850 - val_loss: 5.3355\n",
      "Epoch 7632/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.7218 - val_loss: 5.2282\n",
      "Epoch 7633/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.1704 - val_loss: 4.9291\n",
      "Epoch 7634/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.9808 - val_loss: 4.8903\n",
      "Epoch 7635/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3863 - val_loss: 5.1073\n",
      "Epoch 7636/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.1384 - val_loss: 5.1320\n",
      "Epoch 7637/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.7458 - val_loss: 4.9792\n",
      "Epoch 7638/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.9077 - val_loss: 5.2916\n",
      "Epoch 7639/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.5345 - val_loss: 5.4194\n",
      "Epoch 7640/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.7798 - val_loss: 5.3457\n",
      "Epoch 7641/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.9080 - val_loss: 5.1621\n",
      "Epoch 7642/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.2244 - val_loss: 5.0456\n",
      "Epoch 7643/10000\n",
      "90/90 [==============================] - 0s 171us/step - loss: 6.6010 - val_loss: 4.9091\n",
      "Epoch 7644/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.5291 - val_loss: 5.0450\n",
      "Epoch 7645/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.4673 - val_loss: 5.0365\n",
      "Epoch 7646/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 6.9475 - val_loss: 5.1863\n",
      "Epoch 7647/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.7081 - val_loss: 5.0841\n",
      "Epoch 7648/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.4212 - val_loss: 5.0717\n",
      "Epoch 7649/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 6.6252 - val_loss: 5.0517\n",
      "Epoch 7650/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.7326 - val_loss: 5.0887\n",
      "Epoch 7651/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 6.5119 - val_loss: 5.3187\n",
      "Epoch 7652/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 6.7787 - val_loss: 5.3644\n",
      "Epoch 7653/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 6.6402 - val_loss: 5.4577\n",
      "Epoch 7654/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 6.2793 - val_loss: 5.3358\n",
      "Epoch 7655/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.8910 - val_loss: 5.1818\n",
      "Epoch 7656/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 6.8834 - val_loss: 5.0746\n",
      "Epoch 7657/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.0964 - val_loss: 5.2449\n",
      "Epoch 7658/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 7.1896 - val_loss: 5.0398\n",
      "Epoch 7659/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 7.1523 - val_loss: 4.8200\n",
      "Epoch 7660/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.8263 - val_loss: 5.0193\n",
      "Epoch 7661/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.9757 - val_loss: 5.1909\n",
      "Epoch 7662/10000\n",
      "90/90 [==============================] - 0s 200us/step - loss: 6.7369 - val_loss: 5.2654\n",
      "Epoch 7663/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.8475 - val_loss: 5.3188\n",
      "Epoch 7664/10000\n",
      "90/90 [==============================] - 0s 192us/step - loss: 6.8345 - val_loss: 5.2173\n",
      "Epoch 7665/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.6424 - val_loss: 5.4466\n",
      "Epoch 7666/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.5405 - val_loss: 5.6966\n",
      "Epoch 7667/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.5355 - val_loss: 5.2342\n",
      "Epoch 7668/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 7.3445 - val_loss: 4.8472\n",
      "Epoch 7669/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 6.8174 - val_loss: 4.8612\n",
      "Epoch 7670/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.5323 - val_loss: 5.2964\n",
      "Epoch 7671/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.7270 - val_loss: 5.5159\n",
      "Epoch 7672/10000\n",
      "90/90 [==============================] - 0s 264us/step - loss: 7.0757 - val_loss: 5.1196\n",
      "Epoch 7673/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.4764 - val_loss: 5.0197\n",
      "Epoch 7674/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 7.2659 - val_loss: 5.0081\n",
      "Epoch 7675/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 6.7848 - val_loss: 5.3075\n",
      "Epoch 7676/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.7586 - val_loss: 5.5466\n",
      "Epoch 7677/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.5924 - val_loss: 5.6477\n",
      "Epoch 7678/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.9826 - val_loss: 5.0831\n",
      "Epoch 7679/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.9182 - val_loss: 4.8463\n",
      "Epoch 7680/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.9790 - val_loss: 5.0749\n",
      "Epoch 7681/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.6825 - val_loss: 5.7234\n",
      "Epoch 7682/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.8406 - val_loss: 5.6596\n",
      "Epoch 7683/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.5010 - val_loss: 5.5131\n",
      "Epoch 7684/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.4300 - val_loss: 5.1762\n",
      "Epoch 7685/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.6775 - val_loss: 5.1293\n",
      "Epoch 7686/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 6.9321 - val_loss: 5.2565\n",
      "Epoch 7687/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.8545 - val_loss: 5.4166\n",
      "Epoch 7688/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 6.6498 - val_loss: 5.5476\n",
      "Epoch 7689/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 6.8396 - val_loss: 5.1427\n",
      "Epoch 7690/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.9205 - val_loss: 5.1270\n",
      "Epoch 7691/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.7197 - val_loss: 5.1487\n",
      "Epoch 7692/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.5360 - val_loss: 5.1931\n",
      "Epoch 7693/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 6.6056 - val_loss: 5.3562\n",
      "Epoch 7694/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.6330 - val_loss: 5.1686\n",
      "Epoch 7695/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3787 - val_loss: 5.4614\n",
      "Epoch 7696/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.9333 - val_loss: 5.3881\n",
      "Epoch 7697/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.7732 - val_loss: 5.3835\n",
      "Epoch 7698/10000\n",
      "90/90 [==============================] - 0s 272us/step - loss: 6.2604 - val_loss: 5.3660\n",
      "Epoch 7699/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 6.4338 - val_loss: 5.2293\n",
      "Epoch 7700/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3599 - val_loss: 5.4469\n",
      "Epoch 7701/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.5259 - val_loss: 5.0934\n",
      "Epoch 7702/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.6801 - val_loss: 4.9610\n",
      "Epoch 7703/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.6588 - val_loss: 4.9103\n",
      "Epoch 7704/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.5647 - val_loss: 5.2473\n",
      "Epoch 7705/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 6.3618 - val_loss: 5.5484\n",
      "Epoch 7706/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.9195 - val_loss: 5.3205\n",
      "Epoch 7707/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 6.9034 - val_loss: 5.3787\n",
      "Epoch 7708/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.5958 - val_loss: 5.4875\n",
      "Epoch 7709/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 131us/step - loss: 7.0266 - val_loss: 5.6219\n",
      "Epoch 7710/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.2386 - val_loss: 5.6015\n",
      "Epoch 7711/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.5193 - val_loss: 5.1830\n",
      "Epoch 7712/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.6614 - val_loss: 5.0500\n",
      "Epoch 7713/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 7.1538 - val_loss: 5.0958\n",
      "Epoch 7714/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4904 - val_loss: 5.1362\n",
      "Epoch 7715/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.4835 - val_loss: 4.9900\n",
      "Epoch 7716/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.4311 - val_loss: 5.3496\n",
      "Epoch 7717/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3540 - val_loss: 5.3713\n",
      "Epoch 7718/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.7037 - val_loss: 5.1339\n",
      "Epoch 7719/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.5484 - val_loss: 5.2929\n",
      "Epoch 7720/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.1075 - val_loss: 5.3520\n",
      "Epoch 7721/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.4277 - val_loss: 5.6164\n",
      "Epoch 7722/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.7665 - val_loss: 5.5629\n",
      "Epoch 7723/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.4739 - val_loss: 5.0664\n",
      "Epoch 7724/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 7.0104 - val_loss: 5.0623\n",
      "Epoch 7725/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.5662 - val_loss: 5.1839\n",
      "Epoch 7726/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.4916 - val_loss: 5.4635\n",
      "Epoch 7727/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 6.8156 - val_loss: 5.5023\n",
      "Epoch 7728/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.0131 - val_loss: 5.5446\n",
      "Epoch 7729/10000\n",
      "90/90 [==============================] - 0s 235us/step - loss: 6.5698 - val_loss: 5.3773\n",
      "Epoch 7730/10000\n",
      "90/90 [==============================] - 0s 201us/step - loss: 7.1708 - val_loss: 5.2445\n",
      "Epoch 7731/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2113 - val_loss: 5.4807\n",
      "Epoch 7732/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.8027 - val_loss: 5.6206\n",
      "Epoch 7733/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.7165 - val_loss: 5.3725\n",
      "Epoch 7734/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.8648 - val_loss: 5.2689\n",
      "Epoch 7735/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.9586 - val_loss: 5.1045\n",
      "Epoch 7736/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.7400 - val_loss: 5.0584\n",
      "Epoch 7737/10000\n",
      "90/90 [==============================] - 0s 174us/step - loss: 6.5318 - val_loss: 5.2635\n",
      "Epoch 7738/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.7586 - val_loss: 5.4708\n",
      "Epoch 7739/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.9902 - val_loss: 5.4663\n",
      "Epoch 7740/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.8018 - val_loss: 5.2224\n",
      "Epoch 7741/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4680 - val_loss: 5.2861\n",
      "Epoch 7742/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.6912 - val_loss: 5.5046\n",
      "Epoch 7743/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.3719 - val_loss: 5.5835\n",
      "Epoch 7744/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.6354 - val_loss: 5.4116\n",
      "Epoch 7745/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.1008 - val_loss: 5.1400\n",
      "Epoch 7746/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.6637 - val_loss: 4.9737\n",
      "Epoch 7747/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 6.4334 - val_loss: 4.9556\n",
      "Epoch 7748/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.8764 - val_loss: 5.2441\n",
      "Epoch 7749/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 8.078 - 0s 127us/step - loss: 6.5455 - val_loss: 5.5076\n",
      "Epoch 7750/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.4348 - val_loss: 6.0825\n",
      "Epoch 7751/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.6883 - val_loss: 5.7446\n",
      "Epoch 7752/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 6.9197 - val_loss: 5.2621\n",
      "Epoch 7753/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 7.0333 - val_loss: 5.1332\n",
      "Epoch 7754/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.6996 - val_loss: 5.3224\n",
      "Epoch 7755/10000\n",
      "90/90 [==============================] - 0s 170us/step - loss: 6.6706 - val_loss: 5.5977\n",
      "Epoch 7756/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.5763 - val_loss: 5.3708\n",
      "Epoch 7757/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.6627 - val_loss: 5.3080\n",
      "Epoch 7758/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.4362 - val_loss: 5.4027\n",
      "Epoch 7759/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.0737 - val_loss: 5.3975\n",
      "Epoch 7760/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.4925 - val_loss: 5.9747\n",
      "Epoch 7761/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 6.6917 - val_loss: 6.0209\n",
      "Epoch 7762/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 6.7333 - val_loss: 5.6838\n",
      "Epoch 7763/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.4118 - val_loss: 5.1725\n",
      "Epoch 7764/10000\n",
      "90/90 [==============================] - 0s 173us/step - loss: 6.3119 - val_loss: 4.9469\n",
      "Epoch 7765/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.9063 - val_loss: 4.9552\n",
      "Epoch 7766/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.4609 - val_loss: 5.1556\n",
      "Epoch 7767/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.7767 - val_loss: 5.4563\n",
      "Epoch 7768/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.4866 - val_loss: 5.6741\n",
      "Epoch 7769/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.5946 - val_loss: 5.6752\n",
      "Epoch 7770/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.6836 - val_loss: 5.4426\n",
      "Epoch 7771/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.7596 - val_loss: 5.2283\n",
      "Epoch 7772/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.5990 - val_loss: 5.6209\n",
      "Epoch 7773/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3858 - val_loss: 5.4338\n",
      "Epoch 7774/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4322 - val_loss: 5.5643\n",
      "Epoch 7775/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.6670 - val_loss: 5.1385\n",
      "Epoch 7776/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 7.2289 - val_loss: 5.1989\n",
      "Epoch 7777/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.7375 - val_loss: 5.5240\n",
      "Epoch 7778/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5588 - val_loss: 5.8851\n",
      "Epoch 7779/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.6516 - val_loss: 5.6213\n",
      "Epoch 7780/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 7.3631 - val_loss: 5.0105\n",
      "Epoch 7781/10000\n",
      "90/90 [==============================] - 0s 228us/step - loss: 6.6217 - val_loss: 5.0651\n",
      "Epoch 7782/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 6.6627 - val_loss: 5.5738\n",
      "Epoch 7783/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.7627 - val_loss: 5.8305\n",
      "Epoch 7784/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.7455 - val_loss: 5.8151\n",
      "Epoch 7785/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 7.3751 - val_loss: 5.5343\n",
      "Epoch 7786/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.5253 - val_loss: 5.6459\n",
      "Epoch 7787/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.7039 - val_loss: 5.4035\n",
      "Epoch 7788/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3834 - val_loss: 5.4938\n",
      "Epoch 7789/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.7202 - val_loss: 5.3183\n",
      "Epoch 7790/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.6569 - val_loss: 5.3347\n",
      "Epoch 7791/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 6.8689 - val_loss: 5.3456\n",
      "Epoch 7792/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.7787 - val_loss: 5.7916\n",
      "Epoch 7793/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1163 - val_loss: 5.7790\n",
      "Epoch 7794/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3446 - val_loss: 5.6594\n",
      "Epoch 7795/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.8360 - val_loss: 5.2001\n",
      "Epoch 7796/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.6186 - val_loss: 5.1190\n",
      "Epoch 7797/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.6646 - val_loss: 5.4249\n",
      "Epoch 7798/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.7089 - val_loss: 5.6790\n",
      "Epoch 7799/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 7.3223 - val_loss: 5.4383\n",
      "Epoch 7800/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3790 - val_loss: 5.6419\n",
      "Epoch 7801/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.3613 - val_loss: 5.6912\n",
      "Epoch 7802/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.7636 - val_loss: 5.5785\n",
      "Epoch 7803/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.8878 - val_loss: 5.3652\n",
      "Epoch 7804/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3427 - val_loss: 5.4550\n",
      "Epoch 7805/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.3226 - val_loss: 5.6686\n",
      "Epoch 7806/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.6585 - val_loss: 5.3748\n",
      "Epoch 7807/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.5238 - val_loss: 5.2409\n",
      "Epoch 7808/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.4689 - val_loss: 5.5456\n",
      "Epoch 7809/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.5552 - val_loss: 5.5368\n",
      "Epoch 7810/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.5964 - val_loss: 5.5842\n",
      "Epoch 7811/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.6107 - val_loss: 5.3789\n",
      "Epoch 7812/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.3429 - val_loss: 5.6231\n",
      "Epoch 7813/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.4782 - val_loss: 5.7420\n",
      "Epoch 7814/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.6719 - val_loss: 5.7636\n",
      "Epoch 7815/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.7094 - val_loss: 5.8373\n",
      "Epoch 7816/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.5572 - val_loss: 5.6727\n",
      "Epoch 7817/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.2213 - val_loss: 5.8454\n",
      "Epoch 7818/10000\n",
      "90/90 [==============================] - 0s 243us/step - loss: 6.2658 - val_loss: 5.4234\n",
      "Epoch 7819/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.6857 - val_loss: 5.1529\n",
      "Epoch 7820/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.4605 - val_loss: 5.3226\n",
      "Epoch 7821/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3472 - val_loss: 5.4631\n",
      "Epoch 7822/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 7.1205 - val_loss: 5.1445\n",
      "Epoch 7823/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.8524 - val_loss: 5.1918\n",
      "Epoch 7824/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.2153 - val_loss: 5.5124\n",
      "Epoch 7825/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.4490 - val_loss: 5.7818\n",
      "Epoch 7826/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.5354 - val_loss: 5.5364\n",
      "Epoch 7827/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.7826 - val_loss: 5.4504\n",
      "Epoch 7828/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.5606 - val_loss: 5.6380\n",
      "Epoch 7829/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.5356 - val_loss: 5.7597\n",
      "Epoch 7830/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.1469 - val_loss: 5.6837\n",
      "Epoch 7831/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.5983 - val_loss: 5.7675\n",
      "Epoch 7832/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.5133 - val_loss: 5.4798\n",
      "Epoch 7833/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4457 - val_loss: 5.3465\n",
      "Epoch 7834/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.2568 - val_loss: 5.7256\n",
      "Epoch 7835/10000\n",
      "90/90 [==============================] - 0s 193us/step - loss: 6.8019 - val_loss: 5.9521\n",
      "Epoch 7836/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.6682 - val_loss: 5.7844\n",
      "Epoch 7837/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.6943 - val_loss: 5.3358\n",
      "Epoch 7838/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.4759 - val_loss: 5.3882\n",
      "Epoch 7839/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.8454 - val_loss: 5.2009\n",
      "Epoch 7840/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 7.0524 - val_loss: 5.3230\n",
      "Epoch 7841/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2098 - val_loss: 5.5725\n",
      "Epoch 7842/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.3531 - val_loss: 5.8709\n",
      "Epoch 7843/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.9961 - val_loss: 5.7365\n",
      "Epoch 7844/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.7017 - val_loss: 5.6704\n",
      "Epoch 7845/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.6854 - val_loss: 5.6618\n",
      "Epoch 7846/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2299 - val_loss: 5.6856\n",
      "Epoch 7847/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3208 - val_loss: 5.4864\n",
      "Epoch 7848/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3331 - val_loss: 5.2572\n",
      "Epoch 7849/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.6319 - val_loss: 5.4453\n",
      "Epoch 7850/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.7192 - val_loss: 5.7595\n",
      "Epoch 7851/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.5539 - val_loss: 6.3627\n",
      "Epoch 7852/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.9837 - val_loss: 5.5987\n",
      "Epoch 7853/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.6609 - val_loss: 5.3528\n",
      "Epoch 7854/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5973 - val_loss: 5.4769\n",
      "Epoch 7855/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.9143 - val_loss: 5.6714\n",
      "Epoch 7856/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.7900 - val_loss: 5.8409\n",
      "Epoch 7857/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.2868 - val_loss: 6.0009\n",
      "Epoch 7858/10000\n",
      "90/90 [==============================] - 0s 170us/step - loss: 6.5597 - val_loss: 5.7428\n",
      "Epoch 7859/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.3512 - val_loss: 5.6101\n",
      "Epoch 7860/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1772 - val_loss: 5.5596\n",
      "Epoch 7861/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 6.4011 - val_loss: 5.4329\n",
      "Epoch 7862/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.8219 - val_loss: 5.3643\n",
      "Epoch 7863/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 98us/step - loss: 6.6318 - val_loss: 5.5407\n",
      "Epoch 7864/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.3249 - val_loss: 5.7114\n",
      "Epoch 7865/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 7.0270 - val_loss: 5.6996\n",
      "Epoch 7866/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.5848 - val_loss: 5.6533\n",
      "Epoch 7867/10000\n",
      "90/90 [==============================] - 0s 369us/step - loss: 6.8146 - val_loss: 5.7429\n",
      "Epoch 7868/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3307 - val_loss: 5.8316\n",
      "Epoch 7869/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2576 - val_loss: 5.9761\n",
      "Epoch 7870/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 7.2170 - val_loss: 5.3425\n",
      "Epoch 7871/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.9655 - val_loss: 5.2477\n",
      "Epoch 7872/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.6271 - val_loss: 5.5864\n",
      "Epoch 7873/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.6689 - val_loss: 5.9815\n",
      "Epoch 7874/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1161 - val_loss: 5.8620\n",
      "Epoch 7875/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.5665 - val_loss: 5.4024\n",
      "Epoch 7876/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.7135 - val_loss: 5.2927\n",
      "Epoch 7877/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3553 - val_loss: 5.6708\n",
      "Epoch 7878/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 6.6305 - val_loss: 5.7411\n",
      "Epoch 7879/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.2908 - val_loss: 6.1047\n",
      "Epoch 7880/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.7357 - val_loss: 5.9934\n",
      "Epoch 7881/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.9434 - val_loss: 5.8353\n",
      "Epoch 7882/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3496 - val_loss: 5.7489\n",
      "Epoch 7883/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.5585 - val_loss: 5.5941\n",
      "Epoch 7884/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.5934 - val_loss: 5.5653\n",
      "Epoch 7885/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.8494 - val_loss: 5.5696\n",
      "Epoch 7886/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.4281 - val_loss: 5.5099\n",
      "Epoch 7887/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.2726 - val_loss: 5.5187\n",
      "Epoch 7888/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 7.0207 - val_loss: 5.6596\n",
      "Epoch 7889/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.2800 - val_loss: 5.9601\n",
      "Epoch 7890/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.4075 - val_loss: 5.7589\n",
      "Epoch 7891/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.7374 - val_loss: 5.3250\n",
      "Epoch 7892/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.7779 - val_loss: 5.4870\n",
      "Epoch 7893/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4154 - val_loss: 5.8215\n",
      "Epoch 7894/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.5324 - val_loss: 5.9160\n",
      "Epoch 7895/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.4185 - val_loss: 5.9341\n",
      "Epoch 7896/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 6.7416 - val_loss: 5.7915\n",
      "Epoch 7897/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.4071 - val_loss: 5.7677\n",
      "Epoch 7898/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.6955 - val_loss: 5.7081\n",
      "Epoch 7899/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.3307 - val_loss: 5.4783\n",
      "Epoch 7900/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.3105 - val_loss: 5.4857\n",
      "Epoch 7901/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.7236 - val_loss: 5.6558\n",
      "Epoch 7902/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.8089 - val_loss: 5.5185\n",
      "Epoch 7903/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.5462 - val_loss: 5.4888\n",
      "Epoch 7904/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.7182 - val_loss: 5.5645\n",
      "Epoch 7905/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 6.6109 - val_loss: 5.5796\n",
      "Epoch 7906/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1013 - val_loss: 5.7239\n",
      "Epoch 7907/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.2621 - val_loss: 5.7067\n",
      "Epoch 7908/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.6406 - val_loss: 5.7877\n",
      "Epoch 7909/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.5203 - val_loss: 5.9703\n",
      "Epoch 7910/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.5130 - val_loss: 6.1118\n",
      "Epoch 7911/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.6994 - val_loss: 5.7648\n",
      "Epoch 7912/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.2124 - val_loss: 5.5399\n",
      "Epoch 7913/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2208 - val_loss: 5.7352\n",
      "Epoch 7914/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 5.8415 - val_loss: 5.7915\n",
      "Epoch 7915/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3426 - val_loss: 5.9638\n",
      "Epoch 7916/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.8114 - val_loss: 5.7265\n",
      "Epoch 7917/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.3158 - val_loss: 5.6737\n",
      "Epoch 7918/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.5066 - val_loss: 5.9871\n",
      "Epoch 7919/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2961 - val_loss: 6.1330\n",
      "Epoch 7920/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2948 - val_loss: 5.8525\n",
      "Epoch 7921/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.4600 - val_loss: 5.4284\n",
      "Epoch 7922/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.3994 - val_loss: 5.2831\n",
      "Epoch 7923/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.5674 - val_loss: 5.5640\n",
      "Epoch 7924/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.7031 - val_loss: 5.7906\n",
      "Epoch 7925/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.5848 - val_loss: 5.6497\n",
      "Epoch 7926/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.6706 - val_loss: 5.5389\n",
      "Epoch 7927/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.6614 - val_loss: 5.7243\n",
      "Epoch 7928/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3026 - val_loss: 6.0941\n",
      "Epoch 7929/10000\n",
      "90/90 [==============================] - 0s 84us/step - loss: 6.2191 - val_loss: 5.7648\n",
      "Epoch 7930/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.5211 - val_loss: 5.7308\n",
      "Epoch 7931/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.7280 - val_loss: 5.7312\n",
      "Epoch 7932/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.4646 - val_loss: 5.7604\n",
      "Epoch 7933/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.6729 - val_loss: 5.7444\n",
      "Epoch 7934/10000\n",
      "90/90 [==============================] - 0s 83us/step - loss: 6.4284 - val_loss: 5.6642\n",
      "Epoch 7935/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4877 - val_loss: 5.5514\n",
      "Epoch 7936/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.9096 - val_loss: 5.8437\n",
      "Epoch 7937/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.6112 - val_loss: 5.9228\n",
      "Epoch 7938/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.6386 - val_loss: 5.8072\n",
      "Epoch 7939/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.3110 - val_loss: 5.6255\n",
      "Epoch 7940/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.2568 - val_loss: 5.5563\n",
      "Epoch 7941/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.5621 - val_loss: 5.5818\n",
      "Epoch 7942/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.7537 - val_loss: 5.7101\n",
      "Epoch 7943/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1310 - val_loss: 6.0506\n",
      "Epoch 7944/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.6012 - val_loss: 5.9505\n",
      "Epoch 7945/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.4817 - val_loss: 5.7810\n",
      "Epoch 7946/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.7760 - val_loss: 5.5785\n",
      "Epoch 7947/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.9915 - val_loss: 5.6731\n",
      "Epoch 7948/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.8192 - val_loss: 5.7922\n",
      "Epoch 7949/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.9151 - val_loss: 5.7529\n",
      "Epoch 7950/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.3914 - val_loss: 5.8846\n",
      "Epoch 7951/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.5389 - val_loss: 6.0691\n",
      "Epoch 7952/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.7240 - val_loss: 5.7915\n",
      "Epoch 7953/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9141 - val_loss: 5.9185\n",
      "Epoch 7954/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2902 - val_loss: 5.9503\n",
      "Epoch 7955/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.4105 - val_loss: 5.9125\n",
      "Epoch 7956/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.4426 - val_loss: 6.0660\n",
      "Epoch 7957/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.4546 - val_loss: 5.6165\n",
      "Epoch 7958/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.4134 - val_loss: 5.6944\n",
      "Epoch 7959/10000\n",
      "90/90 [==============================] - 0s 282us/step - loss: 6.3864 - val_loss: 5.5988\n",
      "Epoch 7960/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.7533 - val_loss: 5.5519\n",
      "Epoch 7961/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.5235 - val_loss: 6.0024\n",
      "Epoch 7962/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3334 - val_loss: 5.8341\n",
      "Epoch 7963/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.7783 - val_loss: 5.6684\n",
      "Epoch 7964/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.4838 - val_loss: 5.6902\n",
      "Epoch 7965/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.5637 - val_loss: 6.0026\n",
      "Epoch 7966/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.3822 - val_loss: 6.2371\n",
      "Epoch 7967/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2457 - val_loss: 6.2888\n",
      "Epoch 7968/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 6.3654 - val_loss: 6.1069\n",
      "Epoch 7969/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4691 - val_loss: 5.9606\n",
      "Epoch 7970/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.9494 - val_loss: 5.5329\n",
      "Epoch 7971/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.5458 - val_loss: 5.5344\n",
      "Epoch 7972/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.4707 - val_loss: 5.5317\n",
      "Epoch 7973/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.8315 - val_loss: 5.8436\n",
      "Epoch 7974/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3338 - val_loss: 5.9821\n",
      "Epoch 7975/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.6211 - val_loss: 5.9971\n",
      "Epoch 7976/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.3130 - val_loss: 6.0911\n",
      "Epoch 7977/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3395 - val_loss: 6.1010\n",
      "Epoch 7978/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2209 - val_loss: 5.9431\n",
      "Epoch 7979/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 6.6829 - val_loss: 5.6366\n",
      "Epoch 7980/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.6309 - val_loss: 5.6283\n",
      "Epoch 7981/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.6423 - val_loss: 5.9608\n",
      "Epoch 7982/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3080 - val_loss: 5.7894\n",
      "Epoch 7983/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.2029 - val_loss: 5.6399\n",
      "Epoch 7984/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 7.0126 - val_loss: 5.4714\n",
      "Epoch 7985/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.6862 - val_loss: 5.7035\n",
      "Epoch 7986/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.4584 - val_loss: 6.1031\n",
      "Epoch 7987/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.6511 - val_loss: 5.9951\n",
      "Epoch 7988/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3651 - val_loss: 6.0527\n",
      "Epoch 7989/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.5989 - val_loss: 5.7969\n",
      "Epoch 7990/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 6.3817 - val_loss: 5.9718\n",
      "Epoch 7991/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.1862 - val_loss: 6.1045\n",
      "Epoch 7992/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.8022 - val_loss: 5.8295\n",
      "Epoch 7993/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5861 - val_loss: 5.6558\n",
      "Epoch 7994/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3841 - val_loss: 5.7312\n",
      "Epoch 7995/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.7496 - val_loss: 5.8000\n",
      "Epoch 7996/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.5324 - val_loss: 6.0803\n",
      "Epoch 7997/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.2037 - val_loss: 6.0910\n",
      "Epoch 7998/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.9580 - val_loss: 5.7009\n",
      "Epoch 7999/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1187 - val_loss: 5.9899\n",
      "Epoch 8000/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.8786 - val_loss: 5.8519\n",
      "Epoch 8001/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.5279 - val_loss: 6.1440\n",
      "Epoch 8002/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.4278 - val_loss: 5.9556\n",
      "Epoch 8003/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.6373 - val_loss: 5.9512\n",
      "Epoch 8004/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.4474 - val_loss: 5.8078\n",
      "Epoch 8005/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2681 - val_loss: 6.3053\n",
      "Epoch 8006/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.4444 - val_loss: 6.1835\n",
      "Epoch 8007/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.2428 - val_loss: 6.1337\n",
      "Epoch 8008/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.3315 - val_loss: 5.9570\n",
      "Epoch 8009/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.4996 - val_loss: 6.0536\n",
      "Epoch 8010/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.4650 - val_loss: 6.0391\n",
      "Epoch 8011/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3532 - val_loss: 5.9977\n",
      "Epoch 8012/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 5.9631 - val_loss: 5.6141\n",
      "Epoch 8013/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.3600 - val_loss: 5.5506\n",
      "Epoch 8014/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1309 - val_loss: 5.6446\n",
      "Epoch 8015/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.4034 - val_loss: 6.1725\n",
      "Epoch 8016/10000\n",
      "90/90 [==============================] - 0s 188us/step - loss: 6.0263 - val_loss: 6.4254\n",
      "Epoch 8017/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 176us/step - loss: 6.1368 - val_loss: 6.1850\n",
      "Epoch 8018/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.3442 - val_loss: 5.3944\n",
      "Epoch 8019/10000\n",
      "90/90 [==============================] - 0s 161us/step - loss: 6.7826 - val_loss: 5.2357\n",
      "Epoch 8020/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 6.1010 - val_loss: 5.5178\n",
      "Epoch 8021/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.2009 - val_loss: 5.8501\n",
      "Epoch 8022/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.5764 - val_loss: 5.7891\n",
      "Epoch 8023/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.3079 - val_loss: 6.2627\n",
      "Epoch 8024/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.7755 - val_loss: 6.0540\n",
      "Epoch 8025/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.6426 - val_loss: 5.9489\n",
      "Epoch 8026/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.4970 - val_loss: 6.0644\n",
      "Epoch 8027/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.3483 - val_loss: 6.2424\n",
      "Epoch 8028/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 6.5455 - val_loss: 6.0897\n",
      "Epoch 8029/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1390 - val_loss: 5.9738\n",
      "Epoch 8030/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.4996 - val_loss: 5.9473\n",
      "Epoch 8031/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.2658 - val_loss: 6.0125\n",
      "Epoch 8032/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3415 - val_loss: 6.1753\n",
      "Epoch 8033/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5036 - val_loss: 5.6604\n",
      "Epoch 8034/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.6198 - val_loss: 5.5231\n",
      "Epoch 8035/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2924 - val_loss: 5.6472\n",
      "Epoch 8036/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.4381 - val_loss: 5.8380\n",
      "Epoch 8037/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2720 - val_loss: 6.4328\n",
      "Epoch 8038/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.2841 - val_loss: 6.5313\n",
      "Epoch 8039/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5984 - val_loss: 5.9230\n",
      "Epoch 8040/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.3088 - val_loss: 5.6436\n",
      "Epoch 8041/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5439 - val_loss: 5.6493\n",
      "Epoch 8042/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.4678 - val_loss: 5.6504\n",
      "Epoch 8043/10000\n",
      "90/90 [==============================] - 0s 339us/step - loss: 6.2660 - val_loss: 5.8387\n",
      "Epoch 8044/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.7579 - val_loss: 6.1324\n",
      "Epoch 8045/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.6134 - val_loss: 6.0752\n",
      "Epoch 8046/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5155 - val_loss: 5.9993\n",
      "Epoch 8047/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2246 - val_loss: 5.7141\n",
      "Epoch 8048/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.3971 - val_loss: 5.6823\n",
      "Epoch 8049/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4957 - val_loss: 5.6700\n",
      "Epoch 8050/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.2112 - val_loss: 5.7329\n",
      "Epoch 8051/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3505 - val_loss: 5.9225\n",
      "Epoch 8052/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.4681 - val_loss: 6.0190\n",
      "Epoch 8053/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.2794 - val_loss: 6.3021\n",
      "Epoch 8054/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.4241 - val_loss: 6.0392\n",
      "Epoch 8055/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3884 - val_loss: 6.1489\n",
      "Epoch 8056/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.2628 - val_loss: 5.9400\n",
      "Epoch 8057/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.9282 - val_loss: 6.0306\n",
      "Epoch 8058/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1216 - val_loss: 5.8677\n",
      "Epoch 8059/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3607 - val_loss: 5.8922\n",
      "Epoch 8060/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.3193 - val_loss: 6.1097\n",
      "Epoch 8061/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.7992 - val_loss: 6.0645\n",
      "Epoch 8062/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2862 - val_loss: 6.1004\n",
      "Epoch 8063/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.4872 - val_loss: 5.9561\n",
      "Epoch 8064/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3266 - val_loss: 5.7411\n",
      "Epoch 8065/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.4528 - val_loss: 5.7663\n",
      "Epoch 8066/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2734 - val_loss: 5.8321\n",
      "Epoch 8067/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.5939 - val_loss: 5.8855\n",
      "Epoch 8068/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.6391 - val_loss: 6.1135\n",
      "Epoch 8069/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.2184 - val_loss: 6.6623\n",
      "Epoch 8070/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2811 - val_loss: 6.4525\n",
      "Epoch 8071/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.2605 - val_loss: 6.2518\n",
      "Epoch 8072/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3740 - val_loss: 5.5772\n",
      "Epoch 8073/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.4928 - val_loss: 5.5886\n",
      "Epoch 8074/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.5633 - val_loss: 5.7990\n",
      "Epoch 8075/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.6872 - val_loss: 5.9198\n",
      "Epoch 8076/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.4082 - val_loss: 6.1045\n",
      "Epoch 8077/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2736 - val_loss: 5.9906\n",
      "Epoch 8078/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1360 - val_loss: 5.9023\n",
      "Epoch 8079/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 6.7700 - val_loss: 5.8666\n",
      "Epoch 8080/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3890 - val_loss: 5.9854\n",
      "Epoch 8081/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1497 - val_loss: 6.4929\n",
      "Epoch 8082/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.6729 - val_loss: 6.4343\n",
      "Epoch 8083/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.5257 - val_loss: 6.0357\n",
      "Epoch 8084/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.7720 - val_loss: 5.9590\n",
      "Epoch 8085/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.2494 - val_loss: 6.0624\n",
      "Epoch 8086/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2716 - val_loss: 6.2145\n",
      "Epoch 8087/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1551 - val_loss: 6.2882\n",
      "Epoch 8088/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.4163 - val_loss: 5.8967\n",
      "Epoch 8089/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.5146 - val_loss: 5.6184\n",
      "Epoch 8090/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3533 - val_loss: 5.7435\n",
      "Epoch 8091/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3966 - val_loss: 5.9816\n",
      "Epoch 8092/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.5920 - val_loss: 6.0704\n",
      "Epoch 8093/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3277 - val_loss: 5.8531\n",
      "Epoch 8094/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.6636 - val_loss: 5.7306\n",
      "Epoch 8095/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.3181 - val_loss: 5.9788\n",
      "Epoch 8096/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.5918 - val_loss: 5.9648\n",
      "Epoch 8097/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.6325 - val_loss: 6.2232\n",
      "Epoch 8098/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1558 - val_loss: 6.3674\n",
      "Epoch 8099/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.4574 - val_loss: 5.9400\n",
      "Epoch 8100/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4180 - val_loss: 5.7788\n",
      "Epoch 8101/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.1919 - val_loss: 6.0263\n",
      "Epoch 8102/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.3064 - val_loss: 6.1255\n",
      "Epoch 8103/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.6491 - val_loss: 6.0593\n",
      "Epoch 8104/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.3297 - val_loss: 6.3768\n",
      "Epoch 8105/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2068 - val_loss: 6.2605\n",
      "Epoch 8106/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 6.4863 - val_loss: 5.8929\n",
      "Epoch 8107/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.5408 - val_loss: 5.8328\n",
      "Epoch 8108/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.2463 - val_loss: 6.2772\n",
      "Epoch 8109/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.7906 - val_loss: 6.0318\n",
      "Epoch 8110/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.3645 - val_loss: 6.0691\n",
      "Epoch 8111/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3366 - val_loss: 5.8283\n",
      "Epoch 8112/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.5229 - val_loss: 5.7671\n",
      "Epoch 8113/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.2525 - val_loss: 5.9013\n",
      "Epoch 8114/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.8249 - val_loss: 6.0342\n",
      "Epoch 8115/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.6180 - val_loss: 5.9686\n",
      "Epoch 8116/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.2675 - val_loss: 6.1864\n",
      "Epoch 8117/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.5836 - val_loss: 6.1807\n",
      "Epoch 8118/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.5332 - val_loss: 6.1149\n",
      "Epoch 8119/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.3649 - val_loss: 6.2966\n",
      "Epoch 8120/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2840 - val_loss: 6.1578\n",
      "Epoch 8121/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.2997 - val_loss: 5.7037\n",
      "Epoch 8122/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.4121 - val_loss: 6.0971\n",
      "Epoch 8123/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.7722 - val_loss: 6.1835\n",
      "Epoch 8124/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.0449 - val_loss: 6.5065\n",
      "Epoch 8125/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.2438 - val_loss: 6.2638\n",
      "Epoch 8126/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 6.8058 - val_loss: 5.9334\n",
      "Epoch 8127/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.3027 - val_loss: 5.8759\n",
      "Epoch 8128/10000\n",
      "90/90 [==============================] - 0s 260us/step - loss: 6.6098 - val_loss: 5.7445\n",
      "Epoch 8129/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.4719 - val_loss: 5.8840\n",
      "Epoch 8130/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.4732 - val_loss: 6.3456\n",
      "Epoch 8131/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.5413 - val_loss: 6.4853\n",
      "Epoch 8132/10000\n",
      "90/90 [==============================] - 0s 278us/step - loss: 6.7760 - val_loss: 6.1527\n",
      "Epoch 8133/10000\n",
      "90/90 [==============================] - 0s 203us/step - loss: 6.2671 - val_loss: 5.9157\n",
      "Epoch 8134/10000\n",
      "90/90 [==============================] - 0s 245us/step - loss: 6.3937 - val_loss: 5.8612\n",
      "Epoch 8135/10000\n",
      "90/90 [==============================] - 0s 249us/step - loss: 6.3830 - val_loss: 5.8117\n",
      "Epoch 8136/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.0970 - val_loss: 5.8645\n",
      "Epoch 8137/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.3431 - val_loss: 5.6837\n",
      "Epoch 8138/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.5777 - val_loss: 5.7872\n",
      "Epoch 8139/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 6.6176 - val_loss: 5.8729\n",
      "Epoch 8140/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.4273 - val_loss: 6.1249\n",
      "Epoch 8141/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.5140 - val_loss: 6.6209\n",
      "Epoch 8142/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.1380 - val_loss: 6.5858\n",
      "Epoch 8143/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.4651 - val_loss: 5.9754\n",
      "Epoch 8144/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 6.2855 - val_loss: 5.6930\n",
      "Epoch 8145/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 5.8908 - val_loss: 5.6401\n",
      "Epoch 8146/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.1576 - val_loss: 5.9010\n",
      "Epoch 8147/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 6.1738 - val_loss: 6.2358\n",
      "Epoch 8148/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.0343 - val_loss: 6.3591\n",
      "Epoch 8149/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 6.3871 - val_loss: 6.2117\n",
      "Epoch 8150/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.5454 - val_loss: 6.0166\n",
      "Epoch 8151/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.2257 - val_loss: 6.0307\n",
      "Epoch 8152/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 5.8960 - val_loss: 6.4511\n",
      "Epoch 8153/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.9728 - val_loss: 6.8470\n",
      "Epoch 8154/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.4187 - val_loss: 5.9404\n",
      "Epoch 8155/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.8030 - val_loss: 5.7623\n",
      "Epoch 8156/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.6538 - val_loss: 5.6404\n",
      "Epoch 8157/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.5479 - val_loss: 5.9107\n",
      "Epoch 8158/10000\n",
      "90/90 [==============================] - 0s 207us/step - loss: 6.3571 - val_loss: 5.9268\n",
      "Epoch 8159/10000\n",
      "90/90 [==============================] - 0s 199us/step - loss: 6.5194 - val_loss: 5.8540\n",
      "Epoch 8160/10000\n",
      "90/90 [==============================] - 0s 270us/step - loss: 6.2270 - val_loss: 5.9769\n",
      "Epoch 8161/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 6.5243 - val_loss: 6.3650\n",
      "Epoch 8162/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 6.4016 - val_loss: 6.5171\n",
      "Epoch 8163/10000\n",
      "90/90 [==============================] - 0s 218us/step - loss: 6.3001 - val_loss: 6.4408\n",
      "Epoch 8164/10000\n",
      "90/90 [==============================] - 0s 170us/step - loss: 6.5361 - val_loss: 5.9529\n",
      "Epoch 8165/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.5256 - val_loss: 5.7731\n",
      "Epoch 8166/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.1415 - val_loss: 6.0266\n",
      "Epoch 8167/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 6.4289 - val_loss: 6.2355\n",
      "Epoch 8168/10000\n",
      "90/90 [==============================] - 0s 173us/step - loss: 6.2499 - val_loss: 6.6997\n",
      "Epoch 8169/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.0485 - val_loss: 6.5132\n",
      "Epoch 8170/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2811 - val_loss: 5.8716\n",
      "Epoch 8171/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 109us/step - loss: 6.2488 - val_loss: 5.5711\n",
      "Epoch 8172/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.3407 - val_loss: 5.7048\n",
      "Epoch 8173/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.4675 - val_loss: 5.8476\n",
      "Epoch 8174/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.6970 - val_loss: 6.2870\n",
      "Epoch 8175/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5668 - val_loss: 6.1607\n",
      "Epoch 8176/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.1146 - val_loss: 6.1860\n",
      "Epoch 8177/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4666 - val_loss: 5.8545\n",
      "Epoch 8178/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.3867 - val_loss: 6.0038\n",
      "Epoch 8179/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.1642 - val_loss: 6.0757\n",
      "Epoch 8180/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.3146 - val_loss: 6.1735\n",
      "Epoch 8181/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.4537 - val_loss: 6.0458\n",
      "Epoch 8182/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.4999 - val_loss: 6.3344\n",
      "Epoch 8183/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.2042 - val_loss: 6.5203\n",
      "Epoch 8184/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.8181 - val_loss: 6.0509\n",
      "Epoch 8185/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.1120 - val_loss: 6.4232\n",
      "Epoch 8186/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 4.619 - 0s 157us/step - loss: 6.3771 - val_loss: 6.2115\n",
      "Epoch 8187/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.3978 - val_loss: 5.8706\n",
      "Epoch 8188/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.7978 - val_loss: 6.0047\n",
      "Epoch 8189/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.4663 - val_loss: 6.1784\n",
      "Epoch 8190/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.3579 - val_loss: 6.3467\n",
      "Epoch 8191/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.4439 - val_loss: 6.2189\n",
      "Epoch 8192/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.6150 - val_loss: 6.2689\n",
      "Epoch 8193/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 5.9961 - val_loss: 6.2949\n",
      "Epoch 8194/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.4326 - val_loss: 6.2863\n",
      "Epoch 8195/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.0308 - val_loss: 6.4111\n",
      "Epoch 8196/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3250 - val_loss: 6.2313\n",
      "Epoch 8197/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.2781 - val_loss: 6.3613\n",
      "Epoch 8198/10000\n",
      "90/90 [==============================] - 0s 174us/step - loss: 6.3495 - val_loss: 6.2285\n",
      "Epoch 8199/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.7812 - val_loss: 5.9400\n",
      "Epoch 8200/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 6.2572 - val_loss: 5.9851\n",
      "Epoch 8201/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.1969 - val_loss: 5.9329\n",
      "Epoch 8202/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.2632 - val_loss: 6.1881\n",
      "Epoch 8203/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.0636 - val_loss: 6.7982\n",
      "Epoch 8204/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.2763 - val_loss: 6.8385\n",
      "Epoch 8205/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 6.5976 - val_loss: 6.2583\n",
      "Epoch 8206/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.4647 - val_loss: 5.9040\n",
      "Epoch 8207/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 6.6021 - val_loss: 5.7445\n",
      "Epoch 8208/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.3614 - val_loss: 5.9654\n",
      "Epoch 8209/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 5.9240 - val_loss: 6.3297\n",
      "Epoch 8210/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.2716 - val_loss: 6.0018\n",
      "Epoch 8211/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.3210 - val_loss: 5.8301\n",
      "Epoch 8212/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.0248 - val_loss: 5.8489\n",
      "Epoch 8213/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.5234 - val_loss: 6.0883\n",
      "Epoch 8214/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.5012 - val_loss: 6.6913\n",
      "Epoch 8215/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.3763 - val_loss: 6.5920\n",
      "Epoch 8216/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.5271 - val_loss: 6.3534\n",
      "Epoch 8217/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 5.9840 - val_loss: 6.3262\n",
      "Epoch 8218/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.4315 - val_loss: 6.0938\n",
      "Epoch 8219/10000\n",
      "90/90 [==============================] - 0s 179us/step - loss: 6.6888 - val_loss: 5.7535\n",
      "Epoch 8220/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.2446 - val_loss: 6.1633\n",
      "Epoch 8221/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.2586 - val_loss: 6.2937\n",
      "Epoch 8222/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.1216 - val_loss: 6.5604\n",
      "Epoch 8223/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 6.1724 - val_loss: 6.2288\n",
      "Epoch 8224/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 12.65 - 0s 138us/step - loss: 6.2918 - val_loss: 6.0687\n",
      "Epoch 8225/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.6932 - val_loss: 6.1331\n",
      "Epoch 8226/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.4785 - val_loss: 6.3612\n",
      "Epoch 8227/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.2444 - val_loss: 6.7782\n",
      "Epoch 8228/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.2946 - val_loss: 6.4806\n",
      "Epoch 8229/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1641 - val_loss: 5.9931\n",
      "Epoch 8230/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.4235 - val_loss: 5.7982\n",
      "Epoch 8231/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.3856 - val_loss: 6.0637\n",
      "Epoch 8232/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.5024 - val_loss: 6.5023\n",
      "Epoch 8233/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.0509 - val_loss: 6.6569\n",
      "Epoch 8234/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.4338 - val_loss: 6.2178\n",
      "Epoch 8235/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2985 - val_loss: 6.1472\n",
      "Epoch 8236/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.1876 - val_loss: 6.2555\n",
      "Epoch 8237/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.4112 - val_loss: 6.5648\n",
      "Epoch 8238/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.1872 - val_loss: 6.3129\n",
      "Epoch 8239/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2309 - val_loss: 6.3509\n",
      "Epoch 8240/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.2262 - val_loss: 6.3381\n",
      "Epoch 8241/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.2511 - val_loss: 6.0112\n",
      "Epoch 8242/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.6053 - val_loss: 5.7381\n",
      "Epoch 8243/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.1321 - val_loss: 5.5457\n",
      "Epoch 8244/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.6270 - val_loss: 5.7279\n",
      "Epoch 8245/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.5042 - val_loss: 6.0496\n",
      "Epoch 8246/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 6.3977 - val_loss: 6.5211\n",
      "Epoch 8247/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 7.0143 - val_loss: 6.4544\n",
      "Epoch 8248/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 101us/step - loss: 6.5217 - val_loss: 6.3901\n",
      "Epoch 8249/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.6915 - val_loss: 6.4277\n",
      "Epoch 8250/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.5426 - val_loss: 6.1888\n",
      "Epoch 8251/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.9273 - val_loss: 6.2227\n",
      "Epoch 8252/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1866 - val_loss: 6.4032\n",
      "Epoch 8253/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.3323 - val_loss: 6.1601\n",
      "Epoch 8254/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1317 - val_loss: 6.4908\n",
      "Epoch 8255/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.4829 - val_loss: 6.6045\n",
      "Epoch 8256/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.7620 - val_loss: 6.5392\n",
      "Epoch 8257/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3079 - val_loss: 6.2495\n",
      "Epoch 8258/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.3781 - val_loss: 6.0406\n",
      "Epoch 8259/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.4277 - val_loss: 5.7280\n",
      "Epoch 8260/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.1954 - val_loss: 6.0761\n",
      "Epoch 8261/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.6144 - val_loss: 6.1758\n",
      "Epoch 8262/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2592 - val_loss: 6.4927\n",
      "Epoch 8263/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.3737 - val_loss: 6.3780\n",
      "Epoch 8264/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 5.9853 - val_loss: 5.9132\n",
      "Epoch 8265/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.5625 - val_loss: 5.9378\n",
      "Epoch 8266/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3012 - val_loss: 6.0526\n",
      "Epoch 8267/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.5200 - val_loss: 6.1548\n",
      "Epoch 8268/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.2115 - val_loss: 6.2141\n",
      "Epoch 8269/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2579 - val_loss: 6.4205\n",
      "Epoch 8270/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.2620 - val_loss: 6.5542\n",
      "Epoch 8271/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3909 - val_loss: 6.6293\n",
      "Epoch 8272/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.2139 - val_loss: 6.0973\n",
      "Epoch 8273/10000\n",
      "90/90 [==============================] - 0s 170us/step - loss: 6.5141 - val_loss: 6.2576\n",
      "Epoch 8274/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 5.9356 - val_loss: 6.3466\n",
      "Epoch 8275/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 6.6101 - val_loss: 6.3654\n",
      "Epoch 8276/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.4533 - val_loss: 6.4907\n",
      "Epoch 8277/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.1573 - val_loss: 6.6119\n",
      "Epoch 8278/10000\n",
      "90/90 [==============================] - 0s 219us/step - loss: 6.4266 - val_loss: 5.8266\n",
      "Epoch 8279/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.1573 - val_loss: 5.8657\n",
      "Epoch 8280/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.4031 - val_loss: 5.9450\n",
      "Epoch 8281/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.6297 - val_loss: 5.9867\n",
      "Epoch 8282/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.1252 - val_loss: 6.0953\n",
      "Epoch 8283/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.5009 - val_loss: 6.2261\n",
      "Epoch 8284/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.1980 - val_loss: 6.7011\n",
      "Epoch 8285/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.6220 - val_loss: 6.6513\n",
      "Epoch 8286/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2147 - val_loss: 6.5168\n",
      "Epoch 8287/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.5528 - val_loss: 6.5453\n",
      "Epoch 8288/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.5596 - val_loss: 6.5159\n",
      "Epoch 8289/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1065 - val_loss: 6.6083\n",
      "Epoch 8290/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.4618 - val_loss: 6.3985\n",
      "Epoch 8291/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0343 - val_loss: 6.2621\n",
      "Epoch 8292/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.9736 - val_loss: 6.0990\n",
      "Epoch 8293/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.5952 - val_loss: 5.9515\n",
      "Epoch 8294/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.8859 - val_loss: 6.0173\n",
      "Epoch 8295/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3106 - val_loss: 6.4572\n",
      "Epoch 8296/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.3008 - val_loss: 6.6977\n",
      "Epoch 8297/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.5578 - val_loss: 6.3125\n",
      "Epoch 8298/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.5752 - val_loss: 5.9870\n",
      "Epoch 8299/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1357 - val_loss: 6.1561\n",
      "Epoch 8300/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.9086 - val_loss: 6.2624\n",
      "Epoch 8301/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.6896 - val_loss: 5.8193\n",
      "Epoch 8302/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.2014 - val_loss: 5.9315\n",
      "Epoch 8303/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.4752 - val_loss: 5.8501\n",
      "Epoch 8304/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.1841 - val_loss: 6.1004\n",
      "Epoch 8305/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.6450 - val_loss: 6.5814\n",
      "Epoch 8306/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1923 - val_loss: 7.1355\n",
      "Epoch 8307/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.4347 - val_loss: 6.8154\n",
      "Epoch 8308/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.7486 - val_loss: 6.2587\n",
      "Epoch 8309/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3678 - val_loss: 6.0312\n",
      "Epoch 8310/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.5117 - val_loss: 6.0748\n",
      "Epoch 8311/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.4985 - val_loss: 6.5462\n",
      "Epoch 8312/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2189 - val_loss: 6.7659\n",
      "Epoch 8313/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2661 - val_loss: 6.5422\n",
      "Epoch 8314/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.7135 - val_loss: 5.9920\n",
      "Epoch 8315/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.2245 - val_loss: 6.0612\n",
      "Epoch 8316/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3226 - val_loss: 6.3154\n",
      "Epoch 8317/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.7125 - val_loss: 6.2106\n",
      "Epoch 8318/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.1373 - val_loss: 6.4048\n",
      "Epoch 8319/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 7.0722 - val_loss: 6.1796\n",
      "Epoch 8320/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2863 - val_loss: 6.1646\n",
      "Epoch 8321/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2359 - val_loss: 6.6281\n",
      "Epoch 8322/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2058 - val_loss: 6.4484\n",
      "Epoch 8323/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2212 - val_loss: 5.7974\n",
      "Epoch 8324/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1324 - val_loss: 5.9179\n",
      "Epoch 8325/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.2884 - val_loss: 6.1357\n",
      "Epoch 8326/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.5257 - val_loss: 6.4263\n",
      "Epoch 8327/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.2097 - val_loss: 6.3249\n",
      "Epoch 8328/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.5224 - val_loss: 6.0317\n",
      "Epoch 8329/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3024 - val_loss: 6.0159\n",
      "Epoch 8330/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.6297 - val_loss: 6.3128\n",
      "Epoch 8331/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1467 - val_loss: 6.8329\n",
      "Epoch 8332/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5185 - val_loss: 6.8148\n",
      "Epoch 8333/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3369 - val_loss: 6.2318\n",
      "Epoch 8334/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3849 - val_loss: 6.3132\n",
      "Epoch 8335/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.8009 - val_loss: 6.2286\n",
      "Epoch 8336/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.2051 - val_loss: 6.7697\n",
      "Epoch 8337/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.5058 - val_loss: 6.6682\n",
      "Epoch 8338/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.2950 - val_loss: 6.5267\n",
      "Epoch 8339/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2740 - val_loss: 5.8039\n",
      "Epoch 8340/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0686 - val_loss: 5.6486\n",
      "Epoch 8341/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3927 - val_loss: 5.6904\n",
      "Epoch 8342/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.9674 - val_loss: 6.1611\n",
      "Epoch 8343/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3860 - val_loss: 6.6958\n",
      "Epoch 8344/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.3181 - val_loss: 6.8272\n",
      "Epoch 8345/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.1746 - val_loss: 7.0052\n",
      "Epoch 8346/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 6.2482 - val_loss: 6.9114\n",
      "Epoch 8347/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.2216 - val_loss: 6.5184\n",
      "Epoch 8348/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.5959 - val_loss: 5.7843\n",
      "Epoch 8349/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0391 - val_loss: 5.7484\n",
      "Epoch 8350/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3551 - val_loss: 5.8693\n",
      "Epoch 8351/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.4414 - val_loss: 6.0377\n",
      "Epoch 8352/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.5738 - val_loss: 6.2826\n",
      "Epoch 8353/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.2290 - val_loss: 6.2594\n",
      "Epoch 8354/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9675 - val_loss: 6.5480\n",
      "Epoch 8355/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.1710 - val_loss: 6.2388\n",
      "Epoch 8356/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1000 - val_loss: 6.5358\n",
      "Epoch 8357/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.4129 - val_loss: 6.4507\n",
      "Epoch 8358/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.5449 - val_loss: 6.7156\n",
      "Epoch 8359/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.3260 - val_loss: 6.4455\n",
      "Epoch 8360/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5880 - val_loss: 6.2452\n",
      "Epoch 8361/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.3461 - val_loss: 5.7873\n",
      "Epoch 8362/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.0971 - val_loss: 5.6710\n",
      "Epoch 8363/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.4637 - val_loss: 5.7185\n",
      "Epoch 8364/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.1987 - val_loss: 6.2345\n",
      "Epoch 8365/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.6398 - val_loss: 6.3588\n",
      "Epoch 8366/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.7346 - val_loss: 6.6034\n",
      "Epoch 8367/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.0514 - val_loss: 6.4133\n",
      "Epoch 8368/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.6329 - val_loss: 6.4896\n",
      "Epoch 8369/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.0401 - val_loss: 6.5083\n",
      "Epoch 8370/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1769 - val_loss: 6.6523\n",
      "Epoch 8371/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.9678 - val_loss: 6.3268\n",
      "Epoch 8372/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.5250 - val_loss: 6.1085\n",
      "Epoch 8373/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.9624 - val_loss: 6.4444\n",
      "Epoch 8374/10000\n",
      "90/90 [==============================] - 0s 324us/step - loss: 6.0069 - val_loss: 6.2429\n",
      "Epoch 8375/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 6.2718 - val_loss: 6.2491\n",
      "Epoch 8376/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.3378 - val_loss: 6.3795\n",
      "Epoch 8377/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.0479 - val_loss: 6.3328\n",
      "Epoch 8378/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.3846 - val_loss: 6.2881\n",
      "Epoch 8379/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.4783 - val_loss: 6.3213\n",
      "Epoch 8380/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.7162 - val_loss: 6.1020\n",
      "Epoch 8381/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.6220 - val_loss: 6.0351\n",
      "Epoch 8382/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.3992 - val_loss: 6.2230\n",
      "Epoch 8383/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3030 - val_loss: 6.6278\n",
      "Epoch 8384/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.5763 - val_loss: 6.5173\n",
      "Epoch 8385/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2532 - val_loss: 6.7109\n",
      "Epoch 8386/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.1547 - val_loss: 6.3358\n",
      "Epoch 8387/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.4353 - val_loss: 6.4691\n",
      "Epoch 8388/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0136 - val_loss: 6.3405\n",
      "Epoch 8389/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.4436 - val_loss: 6.0032\n",
      "Epoch 8390/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.2769 - val_loss: 6.1303\n",
      "Epoch 8391/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.5881 - val_loss: 6.2745\n",
      "Epoch 8392/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.4504 - val_loss: 6.3848\n",
      "Epoch 8393/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2709 - val_loss: 6.4482\n",
      "Epoch 8394/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.0167 - val_loss: 6.3927\n",
      "Epoch 8395/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.1936 - val_loss: 6.2544\n",
      "Epoch 8396/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.0067 - val_loss: 6.2880\n",
      "Epoch 8397/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.6347 - val_loss: 6.1285\n",
      "Epoch 8398/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.4392 - val_loss: 6.3685\n",
      "Epoch 8399/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0770 - val_loss: 6.7704\n",
      "Epoch 8400/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.3407 - val_loss: 6.4273\n",
      "Epoch 8401/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9294 - val_loss: 5.8324\n",
      "Epoch 8402/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 126us/step - loss: 6.5248 - val_loss: 5.7354\n",
      "Epoch 8403/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.6793 - val_loss: 6.3120\n",
      "Epoch 8404/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.4478 - val_loss: 6.8418\n",
      "Epoch 8405/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3446 - val_loss: 6.7395\n",
      "Epoch 8406/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1392 - val_loss: 6.5650\n",
      "Epoch 8407/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.9141 - val_loss: 6.0035\n",
      "Epoch 8408/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.6044 - val_loss: 6.0877\n",
      "Epoch 8409/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3998 - val_loss: 6.3719\n",
      "Epoch 8410/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.0356 - val_loss: 6.8482\n",
      "Epoch 8411/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2912 - val_loss: 6.9848\n",
      "Epoch 8412/10000\n",
      "90/90 [==============================] - 0s 249us/step - loss: 6.5058 - val_loss: 7.0531\n",
      "Epoch 8413/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 6.7282 - val_loss: 6.0512\n",
      "Epoch 8414/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 6.5224 - val_loss: 5.9399\n",
      "Epoch 8415/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.7667 - val_loss: 6.2118\n",
      "Epoch 8416/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.8911 - val_loss: 6.7342\n",
      "Epoch 8417/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.3504 - val_loss: 6.2167\n",
      "Epoch 8418/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.4673 - val_loss: 6.2039\n",
      "Epoch 8419/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3702 - val_loss: 6.3812\n",
      "Epoch 8420/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.3621 - val_loss: 6.2492\n",
      "Epoch 8421/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.6502 - val_loss: 6.2949\n",
      "Epoch 8422/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2796 - val_loss: 6.0174\n",
      "Epoch 8423/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.3202 - val_loss: 6.2219\n",
      "Epoch 8424/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.9901 - val_loss: 6.1651\n",
      "Epoch 8425/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 5.8948 - val_loss: 6.4920\n",
      "Epoch 8426/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.2225 - val_loss: 6.4745\n",
      "Epoch 8427/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2585 - val_loss: 6.7187\n",
      "Epoch 8428/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.4432 - val_loss: 6.6504\n",
      "Epoch 8429/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.4574 - val_loss: 6.6041\n",
      "Epoch 8430/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.5473 - val_loss: 6.3332\n",
      "Epoch 8431/10000\n",
      "90/90 [==============================] - 0s 209us/step - loss: 6.5173 - val_loss: 6.5233\n",
      "Epoch 8432/10000\n",
      "90/90 [==============================] - 0s 261us/step - loss: 6.3329 - val_loss: 6.3202\n",
      "Epoch 8433/10000\n",
      "90/90 [==============================] - 0s 195us/step - loss: 5.7877 - val_loss: 6.1901\n",
      "Epoch 8434/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 6.1613 - val_loss: 6.4156\n",
      "Epoch 8435/10000\n",
      "90/90 [==============================] - 0s 171us/step - loss: 6.0002 - val_loss: 6.2624\n",
      "Epoch 8436/10000\n",
      "90/90 [==============================] - 0s 173us/step - loss: 6.5332 - val_loss: 6.4108\n",
      "Epoch 8437/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 6.5665 - val_loss: 6.3629\n",
      "Epoch 8438/10000\n",
      "90/90 [==============================] - 0s 176us/step - loss: 6.0001 - val_loss: 6.5611\n",
      "Epoch 8439/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.4943 - val_loss: 6.3507\n",
      "Epoch 8440/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 6.2703 - val_loss: 6.2234\n",
      "Epoch 8441/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.5392 - val_loss: 6.2460\n",
      "Epoch 8442/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 6.5237 - val_loss: 6.4808\n",
      "Epoch 8443/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.9996 - val_loss: 6.8765\n",
      "Epoch 8444/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.9605 - val_loss: 6.6368\n",
      "Epoch 8445/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.1672 - val_loss: 6.1940\n",
      "Epoch 8446/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.4606 - val_loss: 5.8950\n",
      "Epoch 8447/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0843 - val_loss: 6.4041\n",
      "Epoch 8448/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2143 - val_loss: 6.9196\n",
      "Epoch 8449/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2683 - val_loss: 7.0397\n",
      "Epoch 8450/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.1387 - val_loss: 6.6057\n",
      "Epoch 8451/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.6462 - val_loss: 6.1375\n",
      "Epoch 8452/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1814 - val_loss: 6.0330\n",
      "Epoch 8453/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2491 - val_loss: 6.2245\n",
      "Epoch 8454/10000\n",
      "90/90 [==============================] - 0s 227us/step - loss: 6.5832 - val_loss: 6.4388\n",
      "Epoch 8455/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.2332 - val_loss: 6.2078\n",
      "Epoch 8456/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.2668 - val_loss: 6.2265\n",
      "Epoch 8457/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 5.9810 - val_loss: 6.4063\n",
      "Epoch 8458/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.6313 - val_loss: 6.2602\n",
      "Epoch 8459/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3546 - val_loss: 6.5063\n",
      "Epoch 8460/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 6.4261 - val_loss: 6.5446\n",
      "Epoch 8461/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2964 - val_loss: 6.4295\n",
      "Epoch 8462/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.0253 - val_loss: 6.4423\n",
      "Epoch 8463/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.3683 - val_loss: 6.1742\n",
      "Epoch 8464/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.4311 - val_loss: 6.0912\n",
      "Epoch 8465/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3338 - val_loss: 6.5389\n",
      "Epoch 8466/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3970 - val_loss: 6.6596\n",
      "Epoch 8467/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.0590 - val_loss: 6.7415\n",
      "Epoch 8468/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.8088 - val_loss: 6.2414\n",
      "Epoch 8469/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.0913 - val_loss: 6.6090\n",
      "Epoch 8470/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.8112 - val_loss: 6.7921\n",
      "Epoch 8471/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2851 - val_loss: 6.6631\n",
      "Epoch 8472/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.3024 - val_loss: 6.4006\n",
      "Epoch 8473/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.5020 - val_loss: 6.3585\n",
      "Epoch 8474/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.2576 - val_loss: 6.4678\n",
      "Epoch 8475/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.7118 - val_loss: 6.2330\n",
      "Epoch 8476/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.8489 - val_loss: 6.1256\n",
      "Epoch 8477/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.1776 - val_loss: 6.1775\n",
      "Epoch 8478/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.5428 - val_loss: 6.1596\n",
      "Epoch 8479/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3568 - val_loss: 6.2657\n",
      "Epoch 8480/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.1032 - val_loss: 6.4117\n",
      "Epoch 8481/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.9289 - val_loss: 6.5530\n",
      "Epoch 8482/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.8981 - val_loss: 6.5318\n",
      "Epoch 8483/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.6216 - val_loss: 6.2638\n",
      "Epoch 8484/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.0230 - val_loss: 6.6222\n",
      "Epoch 8485/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.5227 - val_loss: 6.5686\n",
      "Epoch 8486/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1392 - val_loss: 6.8656\n",
      "Epoch 8487/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1169 - val_loss: 6.6073\n",
      "Epoch 8488/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.2102 - val_loss: 6.5122\n",
      "Epoch 8489/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1594 - val_loss: 6.5232\n",
      "Epoch 8490/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.9135 - val_loss: 6.6345\n",
      "Epoch 8491/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.0654 - val_loss: 6.8720\n",
      "Epoch 8492/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.4903 - val_loss: 6.3664\n",
      "Epoch 8493/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.0737 - val_loss: 6.1762\n",
      "Epoch 8494/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.8478 - val_loss: 5.9491\n",
      "Epoch 8495/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 6.3396 - val_loss: 5.9379\n",
      "Epoch 8496/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.0136 - val_loss: 6.3609\n",
      "Epoch 8497/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.6347 - val_loss: 6.6033\n",
      "Epoch 8498/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.5134 - val_loss: 6.6889\n",
      "Epoch 8499/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.0648 - val_loss: 6.9219\n",
      "Epoch 8500/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.2722 - val_loss: 6.8806\n",
      "Epoch 8501/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2312 - val_loss: 6.8213\n",
      "Epoch 8502/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2112 - val_loss: 6.6349\n",
      "Epoch 8503/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 5.9483 - val_loss: 6.6183\n",
      "Epoch 8504/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.8997 - val_loss: 6.3250\n",
      "Epoch 8505/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.3057 - val_loss: 5.9740\n",
      "Epoch 8506/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.0182 - val_loss: 5.8707\n",
      "Epoch 8507/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.9131 - val_loss: 5.9867\n",
      "Epoch 8508/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 5.9537 - val_loss: 6.0115\n",
      "Epoch 8509/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.1447 - val_loss: 6.1139\n",
      "Epoch 8510/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.4417 - val_loss: 6.4167\n",
      "Epoch 8511/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2029 - val_loss: 6.6774\n",
      "Epoch 8512/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0254 - val_loss: 6.7295\n",
      "Epoch 8513/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.4406 - val_loss: 6.5754\n",
      "Epoch 8514/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.9535 - val_loss: 6.2026\n",
      "Epoch 8515/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.7871 - val_loss: 6.6782\n",
      "Epoch 8516/10000\n",
      "90/90 [==============================] - 0s 175us/step - loss: 6.0891 - val_loss: 6.4895\n",
      "Epoch 8517/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.0852 - val_loss: 6.5459\n",
      "Epoch 8518/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2927 - val_loss: 6.3636\n",
      "Epoch 8519/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1125 - val_loss: 6.5527\n",
      "Epoch 8520/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.0560 - val_loss: 6.8201\n",
      "Epoch 8521/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.2407 - val_loss: 6.7591\n",
      "Epoch 8522/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0046 - val_loss: 7.0467\n",
      "Epoch 8523/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.5013 - val_loss: 6.5553\n",
      "Epoch 8524/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2140 - val_loss: 6.2820\n",
      "Epoch 8525/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1132 - val_loss: 6.0417\n",
      "Epoch 8526/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3928 - val_loss: 5.8583\n",
      "Epoch 8527/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.4338 - val_loss: 5.9344\n",
      "Epoch 8528/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2848 - val_loss: 6.2422\n",
      "Epoch 8529/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.3392 - val_loss: 6.2289\n",
      "Epoch 8530/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.5126 - val_loss: 6.2895\n",
      "Epoch 8531/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.3164 - val_loss: 6.7039\n",
      "Epoch 8532/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2143 - val_loss: 7.1556\n",
      "Epoch 8533/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.2700 - val_loss: 6.8085\n",
      "Epoch 8534/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3668 - val_loss: 6.3245\n",
      "Epoch 8535/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.1702 - val_loss: 6.0265\n",
      "Epoch 8536/10000\n",
      "90/90 [==============================] - 0s 192us/step - loss: 6.1524 - val_loss: 6.2983\n",
      "Epoch 8537/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.4224 - val_loss: 6.4877\n",
      "Epoch 8538/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.0097 - val_loss: 6.7334\n",
      "Epoch 8539/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 5.8026 - val_loss: 6.5354\n",
      "Epoch 8540/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.8627 - val_loss: 6.1548\n",
      "Epoch 8541/10000\n",
      "90/90 [==============================] - 0s 265us/step - loss: 6.1297 - val_loss: 6.0489\n",
      "Epoch 8542/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.5698 - val_loss: 6.3239\n",
      "Epoch 8543/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5482 - val_loss: 6.7261\n",
      "Epoch 8544/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2048 - val_loss: 6.6632\n",
      "Epoch 8545/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0975 - val_loss: 6.6638\n",
      "Epoch 8546/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.3295 - val_loss: 6.5359\n",
      "Epoch 8547/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.1040 - val_loss: 6.2432\n",
      "Epoch 8548/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.7042 - val_loss: 6.5522\n",
      "Epoch 8549/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 6.1168 - val_loss: 6.9819\n",
      "Epoch 8550/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.5845 - val_loss: 6.5298\n",
      "Epoch 8551/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1185 - val_loss: 6.3378\n",
      "Epoch 8552/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.3619 - val_loss: 6.0024\n",
      "Epoch 8553/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.3336 - val_loss: 6.2101\n",
      "Epoch 8554/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 6.3784 - val_loss: 6.3792\n",
      "Epoch 8555/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.2947 - val_loss: 6.8569\n",
      "Epoch 8556/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 112us/step - loss: 6.6267 - val_loss: 6.9057\n",
      "Epoch 8557/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.4047 - val_loss: 6.5691\n",
      "Epoch 8558/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.8310 - val_loss: 6.3049\n",
      "Epoch 8559/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.9710 - val_loss: 6.3272\n",
      "Epoch 8560/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.8333 - val_loss: 6.4079\n",
      "Epoch 8561/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2205 - val_loss: 6.3193\n",
      "Epoch 8562/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.3392 - val_loss: 6.6663\n",
      "Epoch 8563/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.3071 - val_loss: 6.8926\n",
      "Epoch 8564/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 6.3479 - val_loss: 6.4304\n",
      "Epoch 8565/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.9137 - val_loss: 6.3485\n",
      "Epoch 8566/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.8498 - val_loss: 6.3286\n",
      "Epoch 8567/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.0188 - val_loss: 6.3491\n",
      "Epoch 8568/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.0906 - val_loss: 6.5005\n",
      "Epoch 8569/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.3350 - val_loss: 6.3861\n",
      "Epoch 8570/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.1916 - val_loss: 6.6790\n",
      "Epoch 8571/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.3285 - val_loss: 6.8737\n",
      "Epoch 8572/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.3260 - val_loss: 6.4793\n",
      "Epoch 8573/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.4517 - val_loss: 6.0911\n",
      "Epoch 8574/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.7062 - val_loss: 6.4216\n",
      "Epoch 8575/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0802 - val_loss: 6.4087\n",
      "Epoch 8576/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.0977 - val_loss: 6.4796\n",
      "Epoch 8577/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.7931 - val_loss: 6.5599\n",
      "Epoch 8578/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.0141 - val_loss: 6.3926\n",
      "Epoch 8579/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1451 - val_loss: 6.6343\n",
      "Epoch 8580/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.0272 - val_loss: 6.8217\n",
      "Epoch 8581/10000\n",
      "90/90 [==============================] - 0s 177us/step - loss: 6.2949 - val_loss: 6.7860\n",
      "Epoch 8582/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1205 - val_loss: 6.6705\n",
      "Epoch 8583/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.7969 - val_loss: 6.4872\n",
      "Epoch 8584/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.8843 - val_loss: 6.7944\n",
      "Epoch 8585/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0876 - val_loss: 7.3237\n",
      "Epoch 8586/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.1621 - val_loss: 7.3377\n",
      "Epoch 8587/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.5061 - val_loss: 6.4987\n",
      "Epoch 8588/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.9725 - val_loss: 5.8824\n",
      "Epoch 8589/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.2029 - val_loss: 5.6889\n",
      "Epoch 8590/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.2805 - val_loss: 5.9173\n",
      "Epoch 8591/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.0307 - val_loss: 6.5271\n",
      "Epoch 8592/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.5788 - val_loss: 6.8197\n",
      "Epoch 8593/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.9172 - val_loss: 6.9434\n",
      "Epoch 8594/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.4257 - val_loss: 6.6899\n",
      "Epoch 8595/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.2069 - val_loss: 6.5538\n",
      "Epoch 8596/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.6454 - val_loss: 6.3347\n",
      "Epoch 8597/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.7505 - val_loss: 6.6816\n",
      "Epoch 8598/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.5124 - val_loss: 6.6416\n",
      "Epoch 8599/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9710 - val_loss: 6.6220\n",
      "Epoch 8600/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3815 - val_loss: 6.4642\n",
      "Epoch 8601/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.1745 - val_loss: 6.5252\n",
      "Epoch 8602/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 6.0701 - val_loss: 6.6745\n",
      "Epoch 8603/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.0008 - val_loss: 6.8140\n",
      "Epoch 8604/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1544 - val_loss: 7.0159\n",
      "Epoch 8605/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2675 - val_loss: 7.0622\n",
      "Epoch 8606/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.2371 - val_loss: 6.3730\n",
      "Epoch 8607/10000\n",
      "90/90 [==============================] - 0s 167us/step - loss: 6.1670 - val_loss: 6.0556\n",
      "Epoch 8608/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3571 - val_loss: 6.2416\n",
      "Epoch 8609/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.1416 - val_loss: 6.3832\n",
      "Epoch 8610/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.9103 - val_loss: 6.2043\n",
      "Epoch 8611/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1883 - val_loss: 6.4191\n",
      "Epoch 8612/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 6.4053 - val_loss: 6.3618\n",
      "Epoch 8613/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1606 - val_loss: 6.8420\n",
      "Epoch 8614/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.3039 - val_loss: 6.9754\n",
      "Epoch 8615/10000\n",
      "90/90 [==============================] - 0s 269us/step - loss: 6.4115 - val_loss: 6.4618\n",
      "Epoch 8616/10000\n",
      "90/90 [==============================] - 0s 765us/step - loss: 6.1241 - val_loss: 6.7975\n",
      "Epoch 8617/10000\n",
      "90/90 [==============================] - 0s 172us/step - loss: 6.7595 - val_loss: 6.1272\n",
      "Epoch 8618/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 6.4361 - val_loss: 5.9046\n",
      "Epoch 8619/10000\n",
      "90/90 [==============================] - 0s 421us/step - loss: 6.0301 - val_loss: 6.3765\n",
      "Epoch 8620/10000\n",
      "90/90 [==============================] - 0s 269us/step - loss: 6.2111 - val_loss: 6.8411\n",
      "Epoch 8621/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.9634 - val_loss: 6.7313\n",
      "Epoch 8622/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.9249 - val_loss: 6.7460\n",
      "Epoch 8623/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2447 - val_loss: 6.7224\n",
      "Epoch 8624/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.1619 - val_loss: 6.8082\n",
      "Epoch 8625/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1579 - val_loss: 6.5557\n",
      "Epoch 8626/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.1530 - val_loss: 6.3341\n",
      "Epoch 8627/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.8711 - val_loss: 6.3355\n",
      "Epoch 8628/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.0040 - val_loss: 6.3747\n",
      "Epoch 8629/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.2486 - val_loss: 6.0389\n",
      "Epoch 8630/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.1847 - val_loss: 6.1547\n",
      "Epoch 8631/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.0227 - val_loss: 6.1299\n",
      "Epoch 8632/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.3120 - val_loss: 6.4321\n",
      "Epoch 8633/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.8733 - val_loss: 7.0166\n",
      "Epoch 8634/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.1466 - val_loss: 6.5408\n",
      "Epoch 8635/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.8336 - val_loss: 6.8743\n",
      "Epoch 8636/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0638 - val_loss: 7.0350\n",
      "Epoch 8637/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5064 - val_loss: 6.3936\n",
      "Epoch 8638/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.1551 - val_loss: 6.2996\n",
      "Epoch 8639/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.9763 - val_loss: 6.1303\n",
      "Epoch 8640/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.0623 - val_loss: 6.4188\n",
      "Epoch 8641/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9901 - val_loss: 6.6611\n",
      "Epoch 8642/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2252 - val_loss: 6.6989\n",
      "Epoch 8643/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.0802 - val_loss: 6.6518\n",
      "Epoch 8644/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.8786 - val_loss: 7.0603\n",
      "Epoch 8645/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.9542 - val_loss: 6.6524\n",
      "Epoch 8646/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1928 - val_loss: 6.3822\n",
      "Epoch 8647/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.8124 - val_loss: 6.4086\n",
      "Epoch 8648/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.4388 - val_loss: 6.6879\n",
      "Epoch 8649/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.5456 - val_loss: 6.2986\n",
      "Epoch 8650/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.2571 - val_loss: 6.3285\n",
      "Epoch 8651/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.9650 - val_loss: 6.6152\n",
      "Epoch 8652/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.1281 - val_loss: 6.9090\n",
      "Epoch 8653/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.2419 - val_loss: 6.4973\n",
      "Epoch 8654/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1525 - val_loss: 5.9514\n",
      "Epoch 8655/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 5.9554 - val_loss: 5.6454\n",
      "Epoch 8656/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3622 - val_loss: 6.0302\n",
      "Epoch 8657/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.9339 - val_loss: 6.9166\n",
      "Epoch 8658/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.0795 - val_loss: 7.5920\n",
      "Epoch 8659/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1171 - val_loss: 6.7651\n",
      "Epoch 8660/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.8658 - val_loss: 6.0739\n",
      "Epoch 8661/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.5048 - val_loss: 5.8704\n",
      "Epoch 8662/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.1464 - val_loss: 6.2135\n",
      "Epoch 8663/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.0911 - val_loss: 6.9409\n",
      "Epoch 8664/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.6735 - val_loss: 7.2577\n",
      "Epoch 8665/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.3053 - val_loss: 7.0829\n",
      "Epoch 8666/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.3247 - val_loss: 6.5743\n",
      "Epoch 8667/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.5942 - val_loss: 6.6191\n",
      "Epoch 8668/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.3599 - val_loss: 6.6153\n",
      "Epoch 8669/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 6.4356 - val_loss: 6.5447\n",
      "Epoch 8670/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.0785 - val_loss: 6.3866\n",
      "Epoch 8671/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.2130 - val_loss: 6.4200\n",
      "Epoch 8672/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 5.8553 - val_loss: 6.1919\n",
      "Epoch 8673/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.2427 - val_loss: 6.1654\n",
      "Epoch 8674/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1563 - val_loss: 6.3032\n",
      "Epoch 8675/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.4842 - val_loss: 6.8694\n",
      "Epoch 8676/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.4448 - val_loss: 6.7506\n",
      "Epoch 8677/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 5.9164 - val_loss: 6.7814\n",
      "Epoch 8678/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.2242 - val_loss: 6.4938\n",
      "Epoch 8679/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.2779 - val_loss: 6.3609\n",
      "Epoch 8680/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 5.7451 - val_loss: 6.6360\n",
      "Epoch 8681/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 5.7827 - val_loss: 6.8170\n",
      "Epoch 8682/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.8819 - val_loss: 6.5537\n",
      "Epoch 8683/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2138 - val_loss: 6.2895\n",
      "Epoch 8684/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.4765 - val_loss: 6.5040\n",
      "Epoch 8685/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1055 - val_loss: 6.7772\n",
      "Epoch 8686/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.7616 - val_loss: 6.3630\n",
      "Epoch 8687/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.4128 - val_loss: 6.6413\n",
      "Epoch 8688/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.4194 - val_loss: 6.4308\n",
      "Epoch 8689/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 5.7144 - val_loss: 6.1947\n",
      "Epoch 8690/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3948 - val_loss: 6.1688\n",
      "Epoch 8691/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2301 - val_loss: 6.2282\n",
      "Epoch 8692/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2328 - val_loss: 6.8430\n",
      "Epoch 8693/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2076 - val_loss: 7.4347\n",
      "Epoch 8694/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.9140 - val_loss: 6.7183\n",
      "Epoch 8695/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.4723 - val_loss: 6.3251\n",
      "Epoch 8696/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0574 - val_loss: 6.1088\n",
      "Epoch 8697/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.7301 - val_loss: 6.1846\n",
      "Epoch 8698/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.3690 - val_loss: 6.1595\n",
      "Epoch 8699/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.3810 - val_loss: 6.4619\n",
      "Epoch 8700/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.9503 - val_loss: 6.8672\n",
      "Epoch 8701/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.3061 - val_loss: 6.9080\n",
      "Epoch 8702/10000\n",
      "90/90 [==============================] - 0s 179us/step - loss: 6.2501 - val_loss: 6.8475\n",
      "Epoch 8703/10000\n",
      "90/90 [==============================] - 0s 173us/step - loss: 6.1196 - val_loss: 6.5963\n",
      "Epoch 8704/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 5.9508 - val_loss: 6.1827\n",
      "Epoch 8705/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.3503 - val_loss: 6.1318\n",
      "Epoch 8706/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.1540 - val_loss: 6.5238\n",
      "Epoch 8707/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0535 - val_loss: 7.0959\n",
      "Epoch 8708/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.8870 - val_loss: 7.1525\n",
      "Epoch 8709/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 5.9309 - val_loss: 6.8785\n",
      "Epoch 8710/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 107us/step - loss: 6.3884 - val_loss: 6.5623\n",
      "Epoch 8711/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 5.7964 - val_loss: 6.4772\n",
      "Epoch 8712/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3768 - val_loss: 6.1382\n",
      "Epoch 8713/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.9008 - val_loss: 6.5056\n",
      "Epoch 8714/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.7905 - val_loss: 6.5069\n",
      "Epoch 8715/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.2290 - val_loss: 6.9113\n",
      "Epoch 8716/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.8067 - val_loss: 6.4257\n",
      "Epoch 8717/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 5.8903 - val_loss: 6.5673\n",
      "Epoch 8718/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.1081 - val_loss: 6.6624\n",
      "Epoch 8719/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.2241 - val_loss: 6.5106\n",
      "Epoch 8720/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2413 - val_loss: 6.3787\n",
      "Epoch 8721/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2230 - val_loss: 6.3308\n",
      "Epoch 8722/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.4543 - val_loss: 6.6440\n",
      "Epoch 8723/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3208 - val_loss: 6.8632\n",
      "Epoch 8724/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3554 - val_loss: 6.6418\n",
      "Epoch 8725/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3484 - val_loss: 6.5818\n",
      "Epoch 8726/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.4778 - val_loss: 6.6023\n",
      "Epoch 8727/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.3375 - val_loss: 6.8422\n",
      "Epoch 8728/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.4039 - val_loss: 6.6013\n",
      "Epoch 8729/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.7512 - val_loss: 7.0386\n",
      "Epoch 8730/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.5091 - val_loss: 6.5669\n",
      "Epoch 8731/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9077 - val_loss: 6.5488\n",
      "Epoch 8732/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.9739 - val_loss: 6.5774\n",
      "Epoch 8733/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.4500 - val_loss: 6.7032\n",
      "Epoch 8734/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.0811 - val_loss: 6.5866\n",
      "Epoch 8735/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.5812 - val_loss: 6.2721\n",
      "Epoch 8736/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0394 - val_loss: 6.3186\n",
      "Epoch 8737/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.5597 - val_loss: 6.7269\n",
      "Epoch 8738/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3076 - val_loss: 6.6341\n",
      "Epoch 8739/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.8747 - val_loss: 6.4210\n",
      "Epoch 8740/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0695 - val_loss: 6.3952\n",
      "Epoch 8741/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.4970 - val_loss: 6.5832\n",
      "Epoch 8742/10000\n",
      "90/90 [==============================] - 0s 272us/step - loss: 5.5067 - val_loss: 6.8849\n",
      "Epoch 8743/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.0876 - val_loss: 6.7567\n",
      "Epoch 8744/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.0626 - val_loss: 6.5210\n",
      "Epoch 8745/10000\n",
      "90/90 [==============================] - 0s 213us/step - loss: 5.7884 - val_loss: 6.6619\n",
      "Epoch 8746/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 5.9506 - val_loss: 6.4171\n",
      "Epoch 8747/10000\n",
      "90/90 [==============================] - 0s 207us/step - loss: 6.1816 - val_loss: 6.2510\n",
      "Epoch 8748/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.2926 - val_loss: 6.0003\n",
      "Epoch 8749/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 5.9857 - val_loss: 6.4952\n",
      "Epoch 8750/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.3296 - val_loss: 7.0402\n",
      "Epoch 8751/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.9631 - val_loss: 6.6802\n",
      "Epoch 8752/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2534 - val_loss: 6.2042\n",
      "Epoch 8753/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.0069 - val_loss: 6.3879\n",
      "Epoch 8754/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.3376 - val_loss: 6.6722\n",
      "Epoch 8755/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1033 - val_loss: 7.0259\n",
      "Epoch 8756/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.8714 - val_loss: 7.0501\n",
      "Epoch 8757/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 5.9938 - val_loss: 6.7516\n",
      "Epoch 8758/10000\n",
      "90/90 [==============================] - 0s 207us/step - loss: 5.9274 - val_loss: 6.0735\n",
      "Epoch 8759/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.1365 - val_loss: 6.2692\n",
      "Epoch 8760/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 5.9852 - val_loss: 6.6433\n",
      "Epoch 8761/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 5.9492 - val_loss: 6.9481\n",
      "Epoch 8762/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.1861 - val_loss: 6.6260\n",
      "Epoch 8763/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.1543 - val_loss: 6.1636\n",
      "Epoch 8764/10000\n",
      "90/90 [==============================] - 0s 177us/step - loss: 6.2790 - val_loss: 6.4143\n",
      "Epoch 8765/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 6.1648 - val_loss: 6.4067\n",
      "Epoch 8766/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2367 - val_loss: 6.9848\n",
      "Epoch 8767/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.9074 - val_loss: 6.8446\n",
      "Epoch 8768/10000\n",
      "90/90 [==============================] - 0s 154us/step - loss: 6.0258 - val_loss: 6.6512\n",
      "Epoch 8769/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.3801 - val_loss: 6.5511\n",
      "Epoch 8770/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.3154 - val_loss: 6.7961\n",
      "Epoch 8771/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.5123 - val_loss: 6.6530\n",
      "Epoch 8772/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.8773 - val_loss: 6.0898\n",
      "Epoch 8773/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.1699 - val_loss: 6.1003\n",
      "Epoch 8774/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.2984 - val_loss: 6.4180\n",
      "Epoch 8775/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.2555 - val_loss: 6.7829\n",
      "Epoch 8776/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 5.9078 - val_loss: 7.0071\n",
      "Epoch 8777/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2249 - val_loss: 6.8550\n",
      "Epoch 8778/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1373 - val_loss: 6.3546\n",
      "Epoch 8779/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.2030 - val_loss: 6.1774\n",
      "Epoch 8780/10000\n",
      "90/90 [==============================] - 0s 304us/step - loss: 6.5234 - val_loss: 6.6747\n",
      "Epoch 8781/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.3695 - val_loss: 7.0085\n",
      "Epoch 8782/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.0923 - val_loss: 6.9662\n",
      "Epoch 8783/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 5.8257 - val_loss: 6.5878\n",
      "Epoch 8784/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.1972 - val_loss: 6.1975\n",
      "Epoch 8785/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.5637 - val_loss: 6.1635\n",
      "Epoch 8786/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.8357 - val_loss: 6.6928\n",
      "Epoch 8787/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.8844 - val_loss: 6.9769\n",
      "Epoch 8788/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.2913 - val_loss: 6.3337\n",
      "Epoch 8789/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.1627 - val_loss: 6.0792\n",
      "Epoch 8790/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2988 - val_loss: 6.3093\n",
      "Epoch 8791/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.9347 - val_loss: 6.6566\n",
      "Epoch 8792/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.4537 - val_loss: 6.9157\n",
      "Epoch 8793/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.0272 - val_loss: 7.3239\n",
      "Epoch 8794/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.8957 - val_loss: 7.0398\n",
      "Epoch 8795/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.7263 - val_loss: 6.0897\n",
      "Epoch 8796/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.3161 - val_loss: 5.7417\n",
      "Epoch 8797/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.5297 - val_loss: 5.8843\n",
      "Epoch 8798/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0012 - val_loss: 6.8371\n",
      "Epoch 8799/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.0856 - val_loss: 6.9651\n",
      "Epoch 8800/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 5.9691 - val_loss: 6.4706\n",
      "Epoch 8801/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.9627 - val_loss: 6.2931\n",
      "Epoch 8802/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.5468 - val_loss: 6.3141\n",
      "Epoch 8803/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2170 - val_loss: 6.3616\n",
      "Epoch 8804/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1873 - val_loss: 6.6032\n",
      "Epoch 8805/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.1645 - val_loss: 6.7406\n",
      "Epoch 8806/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.7032 - val_loss: 6.8265\n",
      "Epoch 8807/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.6432 - val_loss: 6.4986\n",
      "Epoch 8808/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.2057 - val_loss: 6.3989\n",
      "Epoch 8809/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.0631 - val_loss: 7.0374\n",
      "Epoch 8810/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.0661 - val_loss: 7.5231\n",
      "Epoch 8811/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.5135 - val_loss: 7.0982\n",
      "Epoch 8812/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.3693 - val_loss: 6.6881\n",
      "Epoch 8813/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2349 - val_loss: 6.5028\n",
      "Epoch 8814/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1676 - val_loss: 6.3817\n",
      "Epoch 8815/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1593 - val_loss: 6.9044\n",
      "Epoch 8816/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.0980 - val_loss: 6.6338\n",
      "Epoch 8817/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.1389 - val_loss: 6.4688\n",
      "Epoch 8818/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1109 - val_loss: 6.5435\n",
      "Epoch 8819/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.3575 - val_loss: 6.4409\n",
      "Epoch 8820/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.8823 - val_loss: 6.4868\n",
      "Epoch 8821/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.7276 - val_loss: 6.5020\n",
      "Epoch 8822/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2873 - val_loss: 6.5438\n",
      "Epoch 8823/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3992 - val_loss: 6.5564\n",
      "Epoch 8824/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.4001 - val_loss: 6.5081\n",
      "Epoch 8825/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.8819 - val_loss: 6.5674\n",
      "Epoch 8826/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1258 - val_loss: 6.6696\n",
      "Epoch 8827/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1857 - val_loss: 6.7453\n",
      "Epoch 8828/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0086 - val_loss: 6.6304\n",
      "Epoch 8829/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.9821 - val_loss: 6.5340\n",
      "Epoch 8830/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2132 - val_loss: 6.4032\n",
      "Epoch 8831/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.5026 - val_loss: 6.4283\n",
      "Epoch 8832/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.8727 - val_loss: 7.0266\n",
      "Epoch 8833/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.5354 - val_loss: 6.9662\n",
      "Epoch 8834/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 5.8250 - val_loss: 6.9927\n",
      "Epoch 8835/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.0924 - val_loss: 6.5581\n",
      "Epoch 8836/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.3667 - val_loss: 6.6796\n",
      "Epoch 8837/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1506 - val_loss: 6.3293\n",
      "Epoch 8838/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1306 - val_loss: 6.5169\n",
      "Epoch 8839/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 5.9695 - val_loss: 6.6661\n",
      "Epoch 8840/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.8689 - val_loss: 6.5974\n",
      "Epoch 8841/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.0337 - val_loss: 6.9019\n",
      "Epoch 8842/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.2755 - val_loss: 6.5924\n",
      "Epoch 8843/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.0358 - val_loss: 6.5576\n",
      "Epoch 8844/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3550 - val_loss: 6.5066\n",
      "Epoch 8845/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2821 - val_loss: 6.3390\n",
      "Epoch 8846/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.4024 - val_loss: 6.4962\n",
      "Epoch 8847/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1863 - val_loss: 6.8030\n",
      "Epoch 8848/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.2030 - val_loss: 6.4673\n",
      "Epoch 8849/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.0590 - val_loss: 6.3460\n",
      "Epoch 8850/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.1994 - val_loss: 6.3321\n",
      "Epoch 8851/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.0761 - val_loss: 6.8180\n",
      "Epoch 8852/10000\n",
      "90/90 [==============================] - 0s 211us/step - loss: 6.2775 - val_loss: 6.8432\n",
      "Epoch 8853/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.1472 - val_loss: 6.6562\n",
      "Epoch 8854/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9959 - val_loss: 6.7285\n",
      "Epoch 8855/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.2326 - val_loss: 6.7714\n",
      "Epoch 8856/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 6.0919 - val_loss: 6.7106\n",
      "Epoch 8857/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 5.9272 - val_loss: 6.7519\n",
      "Epoch 8858/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.5016 - val_loss: 6.7309\n",
      "Epoch 8859/10000\n",
      "90/90 [==============================] - 0s 188us/step - loss: 6.1704 - val_loss: 7.0202\n",
      "Epoch 8860/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.4879 - val_loss: 6.9049\n",
      "Epoch 8861/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.8682 - val_loss: 7.1228\n",
      "Epoch 8862/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.4060 - val_loss: 6.7028\n",
      "Epoch 8863/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.0572 - val_loss: 6.4187\n",
      "Epoch 8864/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 117us/step - loss: 6.3912 - val_loss: 6.4813\n",
      "Epoch 8865/10000\n",
      "90/90 [==============================] - 0s 214us/step - loss: 6.3023 - val_loss: 6.5382\n",
      "Epoch 8866/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.1325 - val_loss: 6.5222\n",
      "Epoch 8867/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.0632 - val_loss: 6.7168\n",
      "Epoch 8868/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0367 - val_loss: 6.8132\n",
      "Epoch 8869/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.9242 - val_loss: 6.6144\n",
      "Epoch 8870/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1549 - val_loss: 6.6618\n",
      "Epoch 8871/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.2342 - val_loss: 6.6253\n",
      "Epoch 8872/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.9886 - val_loss: 6.3823\n",
      "Epoch 8873/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.6874 - val_loss: 6.1692\n",
      "Epoch 8874/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.4456 - val_loss: 6.3003\n",
      "Epoch 8875/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.0579 - val_loss: 6.4570\n",
      "Epoch 8876/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3258 - val_loss: 6.5459\n",
      "Epoch 8877/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.8339 - val_loss: 6.3253\n",
      "Epoch 8878/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.6389 - val_loss: 6.6574\n",
      "Epoch 8879/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2537 - val_loss: 6.8727\n",
      "Epoch 8880/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.3480 - val_loss: 6.5334\n",
      "Epoch 8881/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.0698 - val_loss: 6.1777\n",
      "Epoch 8882/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.9364 - val_loss: 6.3368\n",
      "Epoch 8883/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2542 - val_loss: 6.7827\n",
      "Epoch 8884/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.1434 - val_loss: 7.0727\n",
      "Epoch 8885/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.0361 - val_loss: 6.8822\n",
      "Epoch 8886/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2496 - val_loss: 6.7413\n",
      "Epoch 8887/10000\n",
      "90/90 [==============================] - ETA: 0s - loss: 12.94 - 0s 99us/step - loss: 6.4535 - val_loss: 6.5525\n",
      "Epoch 8888/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3572 - val_loss: 6.2401\n",
      "Epoch 8889/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.8781 - val_loss: 6.4340\n",
      "Epoch 8890/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.2448 - val_loss: 6.4065\n",
      "Epoch 8891/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3559 - val_loss: 6.5329\n",
      "Epoch 8892/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.0946 - val_loss: 6.7080\n",
      "Epoch 8893/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.4633 - val_loss: 6.7106\n",
      "Epoch 8894/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.6696 - val_loss: 6.5495\n",
      "Epoch 8895/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.0439 - val_loss: 6.9071\n",
      "Epoch 8896/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.1162 - val_loss: 6.8547\n",
      "Epoch 8897/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.0416 - val_loss: 6.7277\n",
      "Epoch 8898/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.1072 - val_loss: 6.4780\n",
      "Epoch 8899/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.5245 - val_loss: 6.4837\n",
      "Epoch 8900/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3356 - val_loss: 6.8801\n",
      "Epoch 8901/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2597 - val_loss: 6.7297\n",
      "Epoch 8902/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.7386 - val_loss: 6.6126\n",
      "Epoch 8903/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.3793 - val_loss: 6.2632\n",
      "Epoch 8904/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.5740 - val_loss: 6.6445\n",
      "Epoch 8905/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3423 - val_loss: 6.8778\n",
      "Epoch 8906/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.0301 - val_loss: 7.2958\n",
      "Epoch 8907/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.1450 - val_loss: 7.0751\n",
      "Epoch 8908/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3197 - val_loss: 6.1120\n",
      "Epoch 8909/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2345 - val_loss: 6.0673\n",
      "Epoch 8910/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.8720 - val_loss: 6.4570\n",
      "Epoch 8911/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.2340 - val_loss: 7.2509\n",
      "Epoch 8912/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.1805 - val_loss: 7.0419\n",
      "Epoch 8913/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.2193 - val_loss: 6.5268\n",
      "Epoch 8914/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 5.7870 - val_loss: 6.3242\n",
      "Epoch 8915/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.8451 - val_loss: 6.3952\n",
      "Epoch 8916/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.0027 - val_loss: 6.5411\n",
      "Epoch 8917/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.1539 - val_loss: 7.0866\n",
      "Epoch 8918/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.0098 - val_loss: 6.8959\n",
      "Epoch 8919/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1739 - val_loss: 6.9726\n",
      "Epoch 8920/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.3894 - val_loss: 6.8732\n",
      "Epoch 8921/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.8737 - val_loss: 6.6300\n",
      "Epoch 8922/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0011 - val_loss: 6.4314\n",
      "Epoch 8923/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.6613 - val_loss: 6.2748\n",
      "Epoch 8924/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2521 - val_loss: 6.7180\n",
      "Epoch 8925/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.8717 - val_loss: 6.6262\n",
      "Epoch 8926/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3813 - val_loss: 6.8032\n",
      "Epoch 8927/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0821 - val_loss: 6.3789\n",
      "Epoch 8928/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3171 - val_loss: 6.5295\n",
      "Epoch 8929/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.9386 - val_loss: 6.6091\n",
      "Epoch 8930/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.1342 - val_loss: 6.6980\n",
      "Epoch 8931/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.2262 - val_loss: 6.6413\n",
      "Epoch 8932/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.7238 - val_loss: 6.6585\n",
      "Epoch 8933/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.0149 - val_loss: 6.6152\n",
      "Epoch 8934/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.2978 - val_loss: 7.0731\n",
      "Epoch 8935/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2311 - val_loss: 7.0297\n",
      "Epoch 8936/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.5148 - val_loss: 6.4886\n",
      "Epoch 8937/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.2950 - val_loss: 6.2231\n",
      "Epoch 8938/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.4520 - val_loss: 6.5058\n",
      "Epoch 8939/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.9401 - val_loss: 6.7345\n",
      "Epoch 8940/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1519 - val_loss: 7.0714\n",
      "Epoch 8941/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9323 - val_loss: 6.9563\n",
      "Epoch 8942/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.1693 - val_loss: 6.6130\n",
      "Epoch 8943/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.5557 - val_loss: 6.3626\n",
      "Epoch 8944/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9756 - val_loss: 6.2518\n",
      "Epoch 8945/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1581 - val_loss: 6.0554\n",
      "Epoch 8946/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3335 - val_loss: 6.3045\n",
      "Epoch 8947/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0308 - val_loss: 6.7937\n",
      "Epoch 8948/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.0593 - val_loss: 7.2566\n",
      "Epoch 8949/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.2307 - val_loss: 6.9173\n",
      "Epoch 8950/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.1428 - val_loss: 6.5024\n",
      "Epoch 8951/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.9813 - val_loss: 6.3678\n",
      "Epoch 8952/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.9661 - val_loss: 6.5230\n",
      "Epoch 8953/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2209 - val_loss: 6.8650\n",
      "Epoch 8954/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2929 - val_loss: 7.3523\n",
      "Epoch 8955/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.5271 - val_loss: 7.1102\n",
      "Epoch 8956/10000\n",
      "90/90 [==============================] - 0s 201us/step - loss: 5.9217 - val_loss: 6.8563\n",
      "Epoch 8957/10000\n",
      "90/90 [==============================] - 0s 176us/step - loss: 6.1327 - val_loss: 6.6492\n",
      "Epoch 8958/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.7172 - val_loss: 6.3937\n",
      "Epoch 8959/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.4440 - val_loss: 6.2308\n",
      "Epoch 8960/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2401 - val_loss: 6.5208\n",
      "Epoch 8961/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3998 - val_loss: 7.0215\n",
      "Epoch 8962/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.9852 - val_loss: 7.2581\n",
      "Epoch 8963/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.2414 - val_loss: 6.9862\n",
      "Epoch 8964/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.1784 - val_loss: 6.5816\n",
      "Epoch 8965/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.7217 - val_loss: 6.8333\n",
      "Epoch 8966/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2376 - val_loss: 6.5178\n",
      "Epoch 8967/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.4118 - val_loss: 6.5087\n",
      "Epoch 8968/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0360 - val_loss: 6.5028\n",
      "Epoch 8969/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9721 - val_loss: 6.5698\n",
      "Epoch 8970/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2763 - val_loss: 6.8205\n",
      "Epoch 8971/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2520 - val_loss: 6.6737\n",
      "Epoch 8972/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3675 - val_loss: 6.7050\n",
      "Epoch 8973/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1455 - val_loss: 7.0278\n",
      "Epoch 8974/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.3992 - val_loss: 6.6847\n",
      "Epoch 8975/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 5.9716 - val_loss: 6.2489\n",
      "Epoch 8976/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 6.6333 - val_loss: 6.1737\n",
      "Epoch 8977/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3725 - val_loss: 6.3600\n",
      "Epoch 8978/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3778 - val_loss: 6.8784\n",
      "Epoch 8979/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.3027 - val_loss: 7.0441\n",
      "Epoch 8980/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.1641 - val_loss: 6.6607\n",
      "Epoch 8981/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9206 - val_loss: 6.6020\n",
      "Epoch 8982/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.0025 - val_loss: 6.6012\n",
      "Epoch 8983/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2371 - val_loss: 6.3518\n",
      "Epoch 8984/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.7496 - val_loss: 6.7822\n",
      "Epoch 8985/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2782 - val_loss: 6.6175\n",
      "Epoch 8986/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0234 - val_loss: 6.6830\n",
      "Epoch 8987/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 5.9781 - val_loss: 6.5549\n",
      "Epoch 8988/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.4294 - val_loss: 6.6456\n",
      "Epoch 8989/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.9930 - val_loss: 6.8041\n",
      "Epoch 8990/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.0762 - val_loss: 6.6190\n",
      "Epoch 8991/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.8799 - val_loss: 6.9013\n",
      "Epoch 8992/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9113 - val_loss: 6.8317\n",
      "Epoch 8993/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.6777 - val_loss: 6.7657\n",
      "Epoch 8994/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.8951 - val_loss: 7.2113\n",
      "Epoch 8995/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1682 - val_loss: 6.5094\n",
      "Epoch 8996/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.5623 - val_loss: 6.4476\n",
      "Epoch 8997/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.2678 - val_loss: 6.5962\n",
      "Epoch 8998/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.7737 - val_loss: 6.7216\n",
      "Epoch 8999/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.0735 - val_loss: 6.6280\n",
      "Epoch 9000/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 6.2320 - val_loss: 6.5718\n",
      "Epoch 9001/10000\n",
      "90/90 [==============================] - 0s 210us/step - loss: 6.3445 - val_loss: 6.2307\n",
      "Epoch 9002/10000\n",
      "90/90 [==============================] - 0s 281us/step - loss: 6.5296 - val_loss: 6.3816\n",
      "Epoch 9003/10000\n",
      "90/90 [==============================] - 0s 186us/step - loss: 5.9387 - val_loss: 6.7490\n",
      "Epoch 9004/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.3229 - val_loss: 6.8760\n",
      "Epoch 9005/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 6.2401 - val_loss: 6.7974\n",
      "Epoch 9006/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.2886 - val_loss: 6.8987\n",
      "Epoch 9007/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.2767 - val_loss: 6.7793\n",
      "Epoch 9008/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.0693 - val_loss: 6.5830\n",
      "Epoch 9009/10000\n",
      "90/90 [==============================] - 0s 164us/step - loss: 6.1802 - val_loss: 6.3671\n",
      "Epoch 9010/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.2547 - val_loss: 6.4449\n",
      "Epoch 9011/10000\n",
      "90/90 [==============================] - 0s 195us/step - loss: 5.8080 - val_loss: 6.7320\n",
      "Epoch 9012/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.1771 - val_loss: 6.9975\n",
      "Epoch 9013/10000\n",
      "90/90 [==============================] - 0s 161us/step - loss: 6.1714 - val_loss: 6.6399\n",
      "Epoch 9014/10000\n",
      "90/90 [==============================] - 0s 292us/step - loss: 6.3401 - val_loss: 6.3205\n",
      "Epoch 9015/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 5.8693 - val_loss: 6.5300\n",
      "Epoch 9016/10000\n",
      "90/90 [==============================] - 0s 153us/step - loss: 6.2461 - val_loss: 6.5502\n",
      "Epoch 9017/10000\n",
      "90/90 [==============================] - 0s 161us/step - loss: 6.4917 - val_loss: 6.6541\n",
      "Epoch 9018/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 132us/step - loss: 6.0225 - val_loss: 6.6983\n",
      "Epoch 9019/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.1274 - val_loss: 6.7309\n",
      "Epoch 9020/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.3548 - val_loss: 6.8886\n",
      "Epoch 9021/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3480 - val_loss: 6.7141\n",
      "Epoch 9022/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.0289 - val_loss: 6.2788\n",
      "Epoch 9023/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.5579 - val_loss: 6.3134\n",
      "Epoch 9024/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0459 - val_loss: 6.7175\n",
      "Epoch 9025/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.3128 - val_loss: 6.9303\n",
      "Epoch 9026/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.3297 - val_loss: 7.0516\n",
      "Epoch 9027/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.4578 - val_loss: 7.0242\n",
      "Epoch 9028/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.9307 - val_loss: 6.8985\n",
      "Epoch 9029/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.7512 - val_loss: 6.6892\n",
      "Epoch 9030/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4698 - val_loss: 6.4935\n",
      "Epoch 9031/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.9939 - val_loss: 6.6279\n",
      "Epoch 9032/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2050 - val_loss: 6.4488\n",
      "Epoch 9033/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0987 - val_loss: 6.4406\n",
      "Epoch 9034/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1109 - val_loss: 6.5562\n",
      "Epoch 9035/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 6.3490 - val_loss: 6.3759\n",
      "Epoch 9036/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.6209 - val_loss: 6.6760\n",
      "Epoch 9037/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.1893 - val_loss: 6.7659\n",
      "Epoch 9038/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 5.8037 - val_loss: 6.9335\n",
      "Epoch 9039/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.6446 - val_loss: 6.7054\n",
      "Epoch 9040/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.4284 - val_loss: 6.6037\n",
      "Epoch 9041/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.2362 - val_loss: 6.9235\n",
      "Epoch 9042/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.1734 - val_loss: 6.9327\n",
      "Epoch 9043/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.2389 - val_loss: 6.4965\n",
      "Epoch 9044/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.2116 - val_loss: 6.3675\n",
      "Epoch 9045/10000\n",
      "90/90 [==============================] - 0s 159us/step - loss: 6.0615 - val_loss: 6.5428\n",
      "Epoch 9046/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 6.3900 - val_loss: 6.7001\n",
      "Epoch 9047/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 5.9108 - val_loss: 6.6190\n",
      "Epoch 9048/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.0772 - val_loss: 6.7627\n",
      "Epoch 9049/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.1604 - val_loss: 6.5084\n",
      "Epoch 9050/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 6.2610 - val_loss: 6.5940\n",
      "Epoch 9051/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.1843 - val_loss: 6.3380\n",
      "Epoch 9052/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.2537 - val_loss: 6.7522\n",
      "Epoch 9053/10000\n",
      "90/90 [==============================] - 0s 160us/step - loss: 6.4220 - val_loss: 6.9809\n",
      "Epoch 9054/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 5.9313 - val_loss: 7.2035\n",
      "Epoch 9055/10000\n",
      "90/90 [==============================] - 0s 151us/step - loss: 6.4005 - val_loss: 7.0373\n",
      "Epoch 9056/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3026 - val_loss: 6.7171\n",
      "Epoch 9057/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.1605 - val_loss: 6.5989\n",
      "Epoch 9058/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.7819 - val_loss: 6.5982\n",
      "Epoch 9059/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2019 - val_loss: 6.3425\n",
      "Epoch 9060/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.4349 - val_loss: 6.4696\n",
      "Epoch 9061/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.5050 - val_loss: 6.3262\n",
      "Epoch 9062/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.2223 - val_loss: 6.5800\n",
      "Epoch 9063/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.2145 - val_loss: 6.8419\n",
      "Epoch 9064/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.2597 - val_loss: 6.9444\n",
      "Epoch 9065/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.4728 - val_loss: 6.9040\n",
      "Epoch 9066/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.3237 - val_loss: 6.7638\n",
      "Epoch 9067/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2137 - val_loss: 6.5427\n",
      "Epoch 9068/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.0712 - val_loss: 6.5072\n",
      "Epoch 9069/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.2950 - val_loss: 6.8869\n",
      "Epoch 9070/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0457 - val_loss: 6.8270\n",
      "Epoch 9071/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.4823 - val_loss: 6.6103\n",
      "Epoch 9072/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.4901 - val_loss: 6.5990\n",
      "Epoch 9073/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.9905 - val_loss: 6.7301\n",
      "Epoch 9074/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1165 - val_loss: 6.8366\n",
      "Epoch 9075/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.0380 - val_loss: 6.7460\n",
      "Epoch 9076/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.2666 - val_loss: 6.8231\n",
      "Epoch 9077/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.0336 - val_loss: 7.0017\n",
      "Epoch 9078/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1546 - val_loss: 6.5715\n",
      "Epoch 9079/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.1228 - val_loss: 6.1082\n",
      "Epoch 9080/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.0641 - val_loss: 6.0698\n",
      "Epoch 9081/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.9929 - val_loss: 6.7624\n",
      "Epoch 9082/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.4457 - val_loss: 6.8203\n",
      "Epoch 9083/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.9189 - val_loss: 7.5126\n",
      "Epoch 9084/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.8615 - val_loss: 7.0776\n",
      "Epoch 9085/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2577 - val_loss: 6.4765\n",
      "Epoch 9086/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0759 - val_loss: 6.4851\n",
      "Epoch 9087/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.2650 - val_loss: 6.7532\n",
      "Epoch 9088/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0520 - val_loss: 6.8866\n",
      "Epoch 9089/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9494 - val_loss: 6.8878\n",
      "Epoch 9090/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.4357 - val_loss: 6.4733\n",
      "Epoch 9091/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2610 - val_loss: 6.5816\n",
      "Epoch 9092/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2516 - val_loss: 7.0470\n",
      "Epoch 9093/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3764 - val_loss: 6.8032\n",
      "Epoch 9094/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.2471 - val_loss: 6.5186\n",
      "Epoch 9095/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.7837 - val_loss: 6.6041\n",
      "Epoch 9096/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.8237 - val_loss: 6.6788\n",
      "Epoch 9097/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.4571 - val_loss: 6.8607\n",
      "Epoch 9098/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3227 - val_loss: 6.7677\n",
      "Epoch 9099/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.6676 - val_loss: 6.4698\n",
      "Epoch 9100/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.1381 - val_loss: 6.8757\n",
      "Epoch 9101/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.1299 - val_loss: 6.6186\n",
      "Epoch 9102/10000\n",
      "90/90 [==============================] - 0s 207us/step - loss: 6.1494 - val_loss: 6.2649\n",
      "Epoch 9103/10000\n",
      "90/90 [==============================] - 0s 188us/step - loss: 5.7968 - val_loss: 6.3627\n",
      "Epoch 9104/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 6.4136 - val_loss: 6.6219\n",
      "Epoch 9105/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.3044 - val_loss: 6.9215\n",
      "Epoch 9106/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1922 - val_loss: 6.6531\n",
      "Epoch 9107/10000\n",
      "90/90 [==============================] - 0s 171us/step - loss: 5.9949 - val_loss: 6.7825\n",
      "Epoch 9108/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1871 - val_loss: 6.6467\n",
      "Epoch 9109/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.9380 - val_loss: 6.5077\n",
      "Epoch 9110/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.1331 - val_loss: 6.5155\n",
      "Epoch 9111/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1011 - val_loss: 6.5924\n",
      "Epoch 9112/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3960 - val_loss: 6.5246\n",
      "Epoch 9113/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3448 - val_loss: 6.3957\n",
      "Epoch 9114/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.0459 - val_loss: 6.9649\n",
      "Epoch 9115/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.4862 - val_loss: 7.1690\n",
      "Epoch 9116/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.0097 - val_loss: 6.5175\n",
      "Epoch 9117/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.1656 - val_loss: 6.2120\n",
      "Epoch 9118/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.9403 - val_loss: 6.1425\n",
      "Epoch 9119/10000\n",
      "90/90 [==============================] - 0s 380us/step - loss: 6.5892 - val_loss: 6.4921\n",
      "Epoch 9120/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2991 - val_loss: 6.8827\n",
      "Epoch 9121/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.8225 - val_loss: 7.2617\n",
      "Epoch 9122/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1911 - val_loss: 7.0316\n",
      "Epoch 9123/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.0180 - val_loss: 6.8723\n",
      "Epoch 9124/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1788 - val_loss: 6.5930\n",
      "Epoch 9125/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1392 - val_loss: 6.6121\n",
      "Epoch 9126/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.2231 - val_loss: 6.9425\n",
      "Epoch 9127/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.0762 - val_loss: 7.0227\n",
      "Epoch 9128/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.8185 - val_loss: 6.5648\n",
      "Epoch 9129/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 5.3380 - val_loss: 6.2188\n",
      "Epoch 9130/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.9974 - val_loss: 6.2548\n",
      "Epoch 9131/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.0517 - val_loss: 6.6302\n",
      "Epoch 9132/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.2162 - val_loss: 6.8592\n",
      "Epoch 9133/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.1464 - val_loss: 6.7688\n",
      "Epoch 9134/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.9734 - val_loss: 6.7133\n",
      "Epoch 9135/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.0774 - val_loss: 7.1838\n",
      "Epoch 9136/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3241 - val_loss: 7.1345\n",
      "Epoch 9137/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.4662 - val_loss: 7.1519\n",
      "Epoch 9138/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.2765 - val_loss: 6.8738\n",
      "Epoch 9139/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.8011 - val_loss: 6.4304\n",
      "Epoch 9140/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1560 - val_loss: 5.7064\n",
      "Epoch 9141/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.9079 - val_loss: 5.9115\n",
      "Epoch 9142/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.1297 - val_loss: 6.2944\n",
      "Epoch 9143/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.1273 - val_loss: 6.6598\n",
      "Epoch 9144/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9143 - val_loss: 6.9792\n",
      "Epoch 9145/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3079 - val_loss: 6.9180\n",
      "Epoch 9146/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4972 - val_loss: 6.6402\n",
      "Epoch 9147/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.7758 - val_loss: 6.4176\n",
      "Epoch 9148/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3191 - val_loss: 6.9884\n",
      "Epoch 9149/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 6.2405 - val_loss: 7.3529\n",
      "Epoch 9150/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.9899 - val_loss: 6.7587\n",
      "Epoch 9151/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1334 - val_loss: 6.2724\n",
      "Epoch 9152/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0521 - val_loss: 6.0393\n",
      "Epoch 9153/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.0179 - val_loss: 6.3123\n",
      "Epoch 9154/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.2036 - val_loss: 6.7484\n",
      "Epoch 9155/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.0918 - val_loss: 6.8945\n",
      "Epoch 9156/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 5.9335 - val_loss: 6.9489\n",
      "Epoch 9157/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0768 - val_loss: 6.8536\n",
      "Epoch 9158/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3919 - val_loss: 6.8060\n",
      "Epoch 9159/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.3234 - val_loss: 6.4306\n",
      "Epoch 9160/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1234 - val_loss: 6.7262\n",
      "Epoch 9161/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3968 - val_loss: 6.1919\n",
      "Epoch 9162/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.2955 - val_loss: 6.2614\n",
      "Epoch 9163/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.8678 - val_loss: 6.5727\n",
      "Epoch 9164/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1991 - val_loss: 6.9496\n",
      "Epoch 9165/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.0316 - val_loss: 6.9473\n",
      "Epoch 9166/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.0080 - val_loss: 6.8501\n",
      "Epoch 9167/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2811 - val_loss: 6.6187\n",
      "Epoch 9168/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9607 - val_loss: 6.3885\n",
      "Epoch 9169/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.3887 - val_loss: 6.2770\n",
      "Epoch 9170/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.3503 - val_loss: 6.5141\n",
      "Epoch 9171/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.8762 - val_loss: 6.8772\n",
      "Epoch 9172/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 99us/step - loss: 5.6972 - val_loss: 7.1581\n",
      "Epoch 9173/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.8958 - val_loss: 6.6892\n",
      "Epoch 9174/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.4746 - val_loss: 6.5109\n",
      "Epoch 9175/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.4261 - val_loss: 6.1813\n",
      "Epoch 9176/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.0725 - val_loss: 6.1539\n",
      "Epoch 9177/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3061 - val_loss: 6.5187\n",
      "Epoch 9178/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1413 - val_loss: 6.8626\n",
      "Epoch 9179/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.2299 - val_loss: 6.8151\n",
      "Epoch 9180/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.1560 - val_loss: 6.4481\n",
      "Epoch 9181/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.1625 - val_loss: 6.1725\n",
      "Epoch 9182/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2514 - val_loss: 6.1912\n",
      "Epoch 9183/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9820 - val_loss: 6.6423\n",
      "Epoch 9184/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.0450 - val_loss: 6.6863\n",
      "Epoch 9185/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.9345 - val_loss: 6.7842\n",
      "Epoch 9186/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2904 - val_loss: 7.2175\n",
      "Epoch 9187/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.4467 - val_loss: 6.5670\n",
      "Epoch 9188/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.6814 - val_loss: 6.5695\n",
      "Epoch 9189/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1742 - val_loss: 6.5507\n",
      "Epoch 9190/10000\n",
      "90/90 [==============================] - 0s 150us/step - loss: 6.0021 - val_loss: 6.9302\n",
      "Epoch 9191/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1849 - val_loss: 6.8736\n",
      "Epoch 9192/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.7219 - val_loss: 6.5677\n",
      "Epoch 9193/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9668 - val_loss: 6.9323\n",
      "Epoch 9194/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.7390 - val_loss: 6.8301\n",
      "Epoch 9195/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.9623 - val_loss: 6.8766\n",
      "Epoch 9196/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.5525 - val_loss: 6.7085\n",
      "Epoch 9197/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.3248 - val_loss: 6.1731\n",
      "Epoch 9198/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2078 - val_loss: 6.0738\n",
      "Epoch 9199/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0113 - val_loss: 6.3428\n",
      "Epoch 9200/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.7352 - val_loss: 6.7143\n",
      "Epoch 9201/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.4350 - val_loss: 6.4763\n",
      "Epoch 9202/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.7550 - val_loss: 6.9220\n",
      "Epoch 9203/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3959 - val_loss: 6.5473\n",
      "Epoch 9204/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.1249 - val_loss: 6.5789\n",
      "Epoch 9205/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.0428 - val_loss: 6.4459\n",
      "Epoch 9206/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.3688 - val_loss: 6.5062\n",
      "Epoch 9207/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.4584 - val_loss: 6.6097\n",
      "Epoch 9208/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3657 - val_loss: 6.9785\n",
      "Epoch 9209/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.4216 - val_loss: 7.4252\n",
      "Epoch 9210/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.9719 - val_loss: 7.1231\n",
      "Epoch 9211/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.0335 - val_loss: 6.8348\n",
      "Epoch 9212/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.4447 - val_loss: 6.5950\n",
      "Epoch 9213/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 6.6308 - val_loss: 6.4583\n",
      "Epoch 9214/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.0495 - val_loss: 6.4467\n",
      "Epoch 9215/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0906 - val_loss: 6.6352\n",
      "Epoch 9216/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.5944 - val_loss: 7.3228\n",
      "Epoch 9217/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4012 - val_loss: 7.1027\n",
      "Epoch 9218/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0875 - val_loss: 6.8310\n",
      "Epoch 9219/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3294 - val_loss: 6.6118\n",
      "Epoch 9220/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0795 - val_loss: 6.5465\n",
      "Epoch 9221/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1963 - val_loss: 6.5612\n",
      "Epoch 9222/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.9958 - val_loss: 6.6952\n",
      "Epoch 9223/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.3139 - val_loss: 6.4596\n",
      "Epoch 9224/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.0657 - val_loss: 6.5183\n",
      "Epoch 9225/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.2900 - val_loss: 6.7707\n",
      "Epoch 9226/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3783 - val_loss: 6.4608\n",
      "Epoch 9227/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3008 - val_loss: 6.1415\n",
      "Epoch 9228/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.2716 - val_loss: 6.3268\n",
      "Epoch 9229/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3684 - val_loss: 6.5694\n",
      "Epoch 9230/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.2875 - val_loss: 6.7654\n",
      "Epoch 9231/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 6.0531 - val_loss: 6.7896\n",
      "Epoch 9232/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 6.2699 - val_loss: 7.1260\n",
      "Epoch 9233/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 6.3643 - val_loss: 6.9184\n",
      "Epoch 9234/10000\n",
      "90/90 [==============================] - 0s 204us/step - loss: 5.9761 - val_loss: 6.8839\n",
      "Epoch 9235/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 5.9168 - val_loss: 7.0474\n",
      "Epoch 9236/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.5845 - val_loss: 6.6124\n",
      "Epoch 9237/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.5385 - val_loss: 5.9827\n",
      "Epoch 9238/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2847 - val_loss: 6.1925\n",
      "Epoch 9239/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.9916 - val_loss: 6.4313\n",
      "Epoch 9240/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.4164 - val_loss: 6.2929\n",
      "Epoch 9241/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.3417 - val_loss: 6.6937\n",
      "Epoch 9242/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1156 - val_loss: 7.0965\n",
      "Epoch 9243/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.5376 - val_loss: 7.0017\n",
      "Epoch 9244/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3751 - val_loss: 6.8112\n",
      "Epoch 9245/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0262 - val_loss: 6.7905\n",
      "Epoch 9246/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.3237 - val_loss: 6.6163\n",
      "Epoch 9247/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.8867 - val_loss: 6.6978\n",
      "Epoch 9248/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.1901 - val_loss: 6.5132\n",
      "Epoch 9249/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.1012 - val_loss: 6.5263\n",
      "Epoch 9250/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.3671 - val_loss: 6.6581\n",
      "Epoch 9251/10000\n",
      "90/90 [==============================] - 0s 193us/step - loss: 6.2727 - val_loss: 6.6087\n",
      "Epoch 9252/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.0674 - val_loss: 6.7557\n",
      "Epoch 9253/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.2951 - val_loss: 6.3246\n",
      "Epoch 9254/10000\n",
      "90/90 [==============================] - 0s 243us/step - loss: 6.1769 - val_loss: 6.3262\n",
      "Epoch 9255/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.0642 - val_loss: 6.6636\n",
      "Epoch 9256/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2164 - val_loss: 6.7418\n",
      "Epoch 9257/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0586 - val_loss: 6.4380\n",
      "Epoch 9258/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.2822 - val_loss: 6.5515\n",
      "Epoch 9259/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.0226 - val_loss: 6.8233\n",
      "Epoch 9260/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.3648 - val_loss: 7.0250\n",
      "Epoch 9261/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.1925 - val_loss: 6.9952\n",
      "Epoch 9262/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1197 - val_loss: 6.8955\n",
      "Epoch 9263/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.9701 - val_loss: 6.5407\n",
      "Epoch 9264/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.6919 - val_loss: 6.4306\n",
      "Epoch 9265/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9287 - val_loss: 6.5752\n",
      "Epoch 9266/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0573 - val_loss: 6.7796\n",
      "Epoch 9267/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.1768 - val_loss: 6.6331\n",
      "Epoch 9268/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.5440 - val_loss: 6.5683\n",
      "Epoch 9269/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.9737 - val_loss: 6.5888\n",
      "Epoch 9270/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1409 - val_loss: 6.7005\n",
      "Epoch 9271/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.2702 - val_loss: 6.5169\n",
      "Epoch 9272/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.0731 - val_loss: 6.7317\n",
      "Epoch 9273/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 5.9976 - val_loss: 6.6716\n",
      "Epoch 9274/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2968 - val_loss: 6.5024\n",
      "Epoch 9275/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9186 - val_loss: 6.5776\n",
      "Epoch 9276/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.0346 - val_loss: 6.6266\n",
      "Epoch 9277/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.2938 - val_loss: 6.8026\n",
      "Epoch 9278/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.8492 - val_loss: 6.6982\n",
      "Epoch 9279/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.9723 - val_loss: 6.2024\n",
      "Epoch 9280/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0274 - val_loss: 6.3925\n",
      "Epoch 9281/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.3470 - val_loss: 6.2327\n",
      "Epoch 9282/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 5.9341 - val_loss: 6.3980\n",
      "Epoch 9283/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.7753 - val_loss: 7.0476\n",
      "Epoch 9284/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.2327 - val_loss: 7.0339\n",
      "Epoch 9285/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1585 - val_loss: 6.3905\n",
      "Epoch 9286/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.5189 - val_loss: 6.2134\n",
      "Epoch 9287/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.2928 - val_loss: 6.3911\n",
      "Epoch 9288/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.4415 - val_loss: 6.6290\n",
      "Epoch 9289/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.4182 - val_loss: 6.9495\n",
      "Epoch 9290/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.9279 - val_loss: 7.1403\n",
      "Epoch 9291/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.1925 - val_loss: 6.3648\n",
      "Epoch 9292/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.3767 - val_loss: 6.1636\n",
      "Epoch 9293/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.2724 - val_loss: 6.2087\n",
      "Epoch 9294/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.1680 - val_loss: 6.7559\n",
      "Epoch 9295/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.0127 - val_loss: 6.6072\n",
      "Epoch 9296/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.8767 - val_loss: 6.3159\n",
      "Epoch 9297/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1745 - val_loss: 6.3470\n",
      "Epoch 9298/10000\n",
      "90/90 [==============================] - 0s 356us/step - loss: 6.1255 - val_loss: 6.3886\n",
      "Epoch 9299/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.0856 - val_loss: 6.7291\n",
      "Epoch 9300/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.3043 - val_loss: 6.7496\n",
      "Epoch 9301/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.9731 - val_loss: 7.0139\n",
      "Epoch 9302/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1321 - val_loss: 6.5202\n",
      "Epoch 9303/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.2705 - val_loss: 6.1015\n",
      "Epoch 9304/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.3200 - val_loss: 6.0545\n",
      "Epoch 9305/10000\n",
      "90/90 [==============================] - 0s 85us/step - loss: 6.0391 - val_loss: 6.6065\n",
      "Epoch 9306/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.2618 - val_loss: 6.9526\n",
      "Epoch 9307/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.4551 - val_loss: 7.2847\n",
      "Epoch 9308/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1512 - val_loss: 7.1610\n",
      "Epoch 9309/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 5.9456 - val_loss: 6.6614\n",
      "Epoch 9310/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1811 - val_loss: 6.2889\n",
      "Epoch 9311/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 6.0002 - val_loss: 6.4634\n",
      "Epoch 9312/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.2837 - val_loss: 6.3845\n",
      "Epoch 9313/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.1325 - val_loss: 6.5901\n",
      "Epoch 9314/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.9886 - val_loss: 6.5311\n",
      "Epoch 9315/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 5.8158 - val_loss: 6.6199\n",
      "Epoch 9316/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.0003 - val_loss: 7.0558\n",
      "Epoch 9317/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1137 - val_loss: 6.8703\n",
      "Epoch 9318/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.8042 - val_loss: 6.7537\n",
      "Epoch 9319/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.0245 - val_loss: 6.6849\n",
      "Epoch 9320/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.0445 - val_loss: 6.6410\n",
      "Epoch 9321/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0278 - val_loss: 6.5844\n",
      "Epoch 9322/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.9101 - val_loss: 6.7069\n",
      "Epoch 9323/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2777 - val_loss: 6.4970\n",
      "Epoch 9324/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.9058 - val_loss: 6.4306\n",
      "Epoch 9325/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1673 - val_loss: 6.4670\n",
      "Epoch 9326/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 106us/step - loss: 6.3497 - val_loss: 6.3166\n",
      "Epoch 9327/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.7177 - val_loss: 6.9714\n",
      "Epoch 9328/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.2367 - val_loss: 6.7650\n",
      "Epoch 9329/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.2062 - val_loss: 6.4845\n",
      "Epoch 9330/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.1220 - val_loss: 6.5255\n",
      "Epoch 9331/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.0472 - val_loss: 6.9417\n",
      "Epoch 9332/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.2560 - val_loss: 6.3858\n",
      "Epoch 9333/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.6381 - val_loss: 6.3105\n",
      "Epoch 9334/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.3395 - val_loss: 6.5013\n",
      "Epoch 9335/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.3512 - val_loss: 6.9035\n",
      "Epoch 9336/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 5.8631 - val_loss: 6.6116\n",
      "Epoch 9337/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1764 - val_loss: 6.3575\n",
      "Epoch 9338/10000\n",
      "90/90 [==============================] - 0s 196us/step - loss: 6.0459 - val_loss: 6.4069\n",
      "Epoch 9339/10000\n",
      "90/90 [==============================] - 0s 164us/step - loss: 6.0879 - val_loss: 6.5963\n",
      "Epoch 9340/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.0081 - val_loss: 6.7282\n",
      "Epoch 9341/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.1623 - val_loss: 6.6072\n",
      "Epoch 9342/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.2128 - val_loss: 7.0379\n",
      "Epoch 9343/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.1454 - val_loss: 6.7881\n",
      "Epoch 9344/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1417 - val_loss: 6.7450\n",
      "Epoch 9345/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 5.9798 - val_loss: 6.9339\n",
      "Epoch 9346/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 5.8482 - val_loss: 6.6759\n",
      "Epoch 9347/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.8713 - val_loss: 6.7751\n",
      "Epoch 9348/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0637 - val_loss: 6.4961\n",
      "Epoch 9349/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.0595 - val_loss: 6.7347\n",
      "Epoch 9350/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3713 - val_loss: 6.5625\n",
      "Epoch 9351/10000\n",
      "90/90 [==============================] - 0s 162us/step - loss: 6.0472 - val_loss: 6.7401\n",
      "Epoch 9352/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 5.8524 - val_loss: 6.8523\n",
      "Epoch 9353/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.7124 - val_loss: 6.8189\n",
      "Epoch 9354/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.2868 - val_loss: 6.5038\n",
      "Epoch 9355/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.9465 - val_loss: 6.3481\n",
      "Epoch 9356/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.6046 - val_loss: 6.9604\n",
      "Epoch 9357/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.9627 - val_loss: 6.7944\n",
      "Epoch 9358/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 6.1804 - val_loss: 6.6092\n",
      "Epoch 9359/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9266 - val_loss: 6.3962\n",
      "Epoch 9360/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.6213 - val_loss: 6.1196\n",
      "Epoch 9361/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.0250 - val_loss: 6.2475\n",
      "Epoch 9362/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2364 - val_loss: 6.3400\n",
      "Epoch 9363/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2336 - val_loss: 6.5297\n",
      "Epoch 9364/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.0312 - val_loss: 6.7541\n",
      "Epoch 9365/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 5.8869 - val_loss: 6.7591\n",
      "Epoch 9366/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 5.9252 - val_loss: 6.5537\n",
      "Epoch 9367/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4115 - val_loss: 6.3014\n",
      "Epoch 9368/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.1628 - val_loss: 6.5730\n",
      "Epoch 9369/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.7452 - val_loss: 6.9684\n",
      "Epoch 9370/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9821 - val_loss: 6.8128\n",
      "Epoch 9371/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1707 - val_loss: 6.7963\n",
      "Epoch 9372/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1228 - val_loss: 6.5012\n",
      "Epoch 9373/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 5.8002 - val_loss: 6.7230\n",
      "Epoch 9374/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3831 - val_loss: 6.7870\n",
      "Epoch 9375/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1005 - val_loss: 6.6645\n",
      "Epoch 9376/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.7998 - val_loss: 6.5632\n",
      "Epoch 9377/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.8729 - val_loss: 6.6842\n",
      "Epoch 9378/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1318 - val_loss: 7.0776\n",
      "Epoch 9379/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.9948 - val_loss: 6.7662\n",
      "Epoch 9380/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.6418 - val_loss: 6.4437\n",
      "Epoch 9381/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.4431 - val_loss: 6.0581\n",
      "Epoch 9382/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0417 - val_loss: 6.1941\n",
      "Epoch 9383/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.3549 - val_loss: 6.6015\n",
      "Epoch 9384/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.8650 - val_loss: 6.6659\n",
      "Epoch 9385/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.9226 - val_loss: 6.5592\n",
      "Epoch 9386/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.1616 - val_loss: 6.5642\n",
      "Epoch 9387/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.9932 - val_loss: 6.5197\n",
      "Epoch 9388/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.2151 - val_loss: 6.8610\n",
      "Epoch 9389/10000\n",
      "90/90 [==============================] - 0s 199us/step - loss: 6.0624 - val_loss: 7.0208\n",
      "Epoch 9390/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2291 - val_loss: 6.7472\n",
      "Epoch 9391/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0570 - val_loss: 6.6156\n",
      "Epoch 9392/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.2837 - val_loss: 6.3249\n",
      "Epoch 9393/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9099 - val_loss: 6.7908\n",
      "Epoch 9394/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.5403 - val_loss: 6.4181\n",
      "Epoch 9395/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9294 - val_loss: 6.4859\n",
      "Epoch 9396/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.9761 - val_loss: 6.6626\n",
      "Epoch 9397/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.0529 - val_loss: 6.8236\n",
      "Epoch 9398/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.9235 - val_loss: 6.5771\n",
      "Epoch 9399/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3575 - val_loss: 6.1919\n",
      "Epoch 9400/10000\n",
      "90/90 [==============================] - 0s 182us/step - loss: 6.0756 - val_loss: 5.9722\n",
      "Epoch 9401/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.7122 - val_loss: 6.0739\n",
      "Epoch 9402/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1860 - val_loss: 6.7262\n",
      "Epoch 9403/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1067 - val_loss: 7.0975\n",
      "Epoch 9404/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.5453 - val_loss: 7.5041\n",
      "Epoch 9405/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.5257 - val_loss: 6.6281\n",
      "Epoch 9406/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.5312 - val_loss: 6.1851\n",
      "Epoch 9407/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 7.2351 - val_loss: 6.2690\n",
      "Epoch 9408/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 5.9842 - val_loss: 6.9849\n",
      "Epoch 9409/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.6043 - val_loss: 7.3183\n",
      "Epoch 9410/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.0908 - val_loss: 6.6242\n",
      "Epoch 9411/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0386 - val_loss: 5.8738\n",
      "Epoch 9412/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1292 - val_loss: 5.8735\n",
      "Epoch 9413/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2566 - val_loss: 6.2393\n",
      "Epoch 9414/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3010 - val_loss: 7.1192\n",
      "Epoch 9415/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.0923 - val_loss: 6.9382\n",
      "Epoch 9416/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.0987 - val_loss: 6.6690\n",
      "Epoch 9417/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2508 - val_loss: 6.4320\n",
      "Epoch 9418/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.1819 - val_loss: 6.3149\n",
      "Epoch 9419/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.0532 - val_loss: 6.4820\n",
      "Epoch 9420/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 5.9609 - val_loss: 6.4531\n",
      "Epoch 9421/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.4130 - val_loss: 6.6424\n",
      "Epoch 9422/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.4115 - val_loss: 6.8824\n",
      "Epoch 9423/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1832 - val_loss: 6.7815\n",
      "Epoch 9424/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.0331 - val_loss: 6.8731\n",
      "Epoch 9425/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0019 - val_loss: 6.7483\n",
      "Epoch 9426/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.0289 - val_loss: 6.4991\n",
      "Epoch 9427/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.1893 - val_loss: 6.5764\n",
      "Epoch 9428/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.3845 - val_loss: 6.5821\n",
      "Epoch 9429/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.8653 - val_loss: 7.1574\n",
      "Epoch 9430/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 5.8955 - val_loss: 7.0803\n",
      "Epoch 9431/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.4062 - val_loss: 6.7873\n",
      "Epoch 9432/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 5.9574 - val_loss: 6.2588\n",
      "Epoch 9433/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.1920 - val_loss: 6.1015\n",
      "Epoch 9434/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.0989 - val_loss: 6.4780\n",
      "Epoch 9435/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.3104 - val_loss: 6.8402\n",
      "Epoch 9436/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.1593 - val_loss: 6.4632\n",
      "Epoch 9437/10000\n",
      "90/90 [==============================] - 0s 148us/step - loss: 6.0029 - val_loss: 6.6066\n",
      "Epoch 9438/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.1376 - val_loss: 6.7480\n",
      "Epoch 9439/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.4330 - val_loss: 6.1671\n",
      "Epoch 9440/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0514 - val_loss: 6.0301\n",
      "Epoch 9441/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.7412 - val_loss: 6.3129\n",
      "Epoch 9442/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3260 - val_loss: 6.3953\n",
      "Epoch 9443/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3110 - val_loss: 6.4727\n",
      "Epoch 9444/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.4676 - val_loss: 6.7916\n",
      "Epoch 9445/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.8689 - val_loss: 7.0265\n",
      "Epoch 9446/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.0643 - val_loss: 6.5971\n",
      "Epoch 9447/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1252 - val_loss: 6.6140\n",
      "Epoch 9448/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.9994 - val_loss: 6.8149\n",
      "Epoch 9449/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.0915 - val_loss: 6.8326\n",
      "Epoch 9450/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0676 - val_loss: 6.9883\n",
      "Epoch 9451/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.5186 - val_loss: 6.4497\n",
      "Epoch 9452/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0872 - val_loss: 6.3028\n",
      "Epoch 9453/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.2526 - val_loss: 6.4186\n",
      "Epoch 9454/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.0984 - val_loss: 6.4384\n",
      "Epoch 9455/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4341 - val_loss: 6.4361\n",
      "Epoch 9456/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3203 - val_loss: 6.7561\n",
      "Epoch 9457/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.4674 - val_loss: 6.6286\n",
      "Epoch 9458/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.1469 - val_loss: 6.6263\n",
      "Epoch 9459/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.0782 - val_loss: 6.5704\n",
      "Epoch 9460/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.1308 - val_loss: 6.5857\n",
      "Epoch 9461/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.1635 - val_loss: 6.8446\n",
      "Epoch 9462/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.4291 - val_loss: 6.7072\n",
      "Epoch 9463/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 5.9038 - val_loss: 6.7676\n",
      "Epoch 9464/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.0796 - val_loss: 6.2627\n",
      "Epoch 9465/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 5.8878 - val_loss: 6.2458\n",
      "Epoch 9466/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.8076 - val_loss: 6.3719\n",
      "Epoch 9467/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.0773 - val_loss: 6.7250\n",
      "Epoch 9468/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2637 - val_loss: 6.7432\n",
      "Epoch 9469/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.2575 - val_loss: 6.7930\n",
      "Epoch 9470/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0963 - val_loss: 6.5765\n",
      "Epoch 9471/10000\n",
      "90/90 [==============================] - 0s 147us/step - loss: 6.0281 - val_loss: 7.0278\n",
      "Epoch 9472/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.8863 - val_loss: 6.7020\n",
      "Epoch 9473/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1355 - val_loss: 6.2851\n",
      "Epoch 9474/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1505 - val_loss: 6.2300\n",
      "Epoch 9475/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.4087 - val_loss: 6.3460\n",
      "Epoch 9476/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 5.9597 - val_loss: 6.3827\n",
      "Epoch 9477/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1422 - val_loss: 6.4897\n",
      "Epoch 9478/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.8064 - val_loss: 6.6312\n",
      "Epoch 9479/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 5.7646 - val_loss: 6.7838\n",
      "Epoch 9480/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 117us/step - loss: 6.6484 - val_loss: 6.3685\n",
      "Epoch 9481/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2380 - val_loss: 6.6281\n",
      "Epoch 9482/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.0863 - val_loss: 6.6945\n",
      "Epoch 9483/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.8210 - val_loss: 6.6450\n",
      "Epoch 9484/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.1671 - val_loss: 6.8536\n",
      "Epoch 9485/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.1718 - val_loss: 6.7972\n",
      "Epoch 9486/10000\n",
      "90/90 [==============================] - 0s 156us/step - loss: 6.1155 - val_loss: 6.7436\n",
      "Epoch 9487/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.8159 - val_loss: 6.6808\n",
      "Epoch 9488/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.2861 - val_loss: 6.2812\n",
      "Epoch 9489/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 6.1861 - val_loss: 6.3665\n",
      "Epoch 9490/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1835 - val_loss: 6.6746\n",
      "Epoch 9491/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.4037 - val_loss: 6.9023\n",
      "Epoch 9492/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 6.3780 - val_loss: 6.6157\n",
      "Epoch 9493/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.1562 - val_loss: 6.0250\n",
      "Epoch 9494/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 5.9827 - val_loss: 6.3088\n",
      "Epoch 9495/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.9942 - val_loss: 6.5016\n",
      "Epoch 9496/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 5.8497 - val_loss: 6.5012\n",
      "Epoch 9497/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 5.8834 - val_loss: 6.4535\n",
      "Epoch 9498/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.8890 - val_loss: 6.4032\n",
      "Epoch 9499/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 5.8708 - val_loss: 6.9661\n",
      "Epoch 9500/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 6.2490 - val_loss: 6.8752\n",
      "Epoch 9501/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 5.8700 - val_loss: 6.2267\n",
      "Epoch 9502/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.3012 - val_loss: 6.0329\n",
      "Epoch 9503/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.5669 - val_loss: 6.3875\n",
      "Epoch 9504/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 5.9091 - val_loss: 6.8783\n",
      "Epoch 9505/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.2958 - val_loss: 7.0068\n",
      "Epoch 9506/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.3777 - val_loss: 7.0518\n",
      "Epoch 9507/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.1313 - val_loss: 7.0626\n",
      "Epoch 9508/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.7227 - val_loss: 6.4243\n",
      "Epoch 9509/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 5.6740 - val_loss: 6.3147\n",
      "Epoch 9510/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.9398 - val_loss: 6.5874\n",
      "Epoch 9511/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 5.8349 - val_loss: 6.6072\n",
      "Epoch 9512/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.1193 - val_loss: 6.7675\n",
      "Epoch 9513/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.1481 - val_loss: 6.4969\n",
      "Epoch 9514/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.2456 - val_loss: 6.2737\n",
      "Epoch 9515/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.1799 - val_loss: 6.4785\n",
      "Epoch 9516/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 5.8603 - val_loss: 6.5866\n",
      "Epoch 9517/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2728 - val_loss: 6.2248\n",
      "Epoch 9518/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 6.1712 - val_loss: 6.0267\n",
      "Epoch 9519/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.4082 - val_loss: 6.2577\n",
      "Epoch 9520/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 5.9793 - val_loss: 6.8398\n",
      "Epoch 9521/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 6.2107 - val_loss: 6.9511\n",
      "Epoch 9522/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 5.7077 - val_loss: 6.8293\n",
      "Epoch 9523/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3277 - val_loss: 6.7453\n",
      "Epoch 9524/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1370 - val_loss: 6.7692\n",
      "Epoch 9525/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.4226 - val_loss: 6.8331\n",
      "Epoch 9526/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.1017 - val_loss: 6.8714\n",
      "Epoch 9527/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1049 - val_loss: 7.1963\n",
      "Epoch 9528/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.9885 - val_loss: 6.8898\n",
      "Epoch 9529/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2192 - val_loss: 6.3993\n",
      "Epoch 9530/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.9719 - val_loss: 6.5179\n",
      "Epoch 9531/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 5.9323 - val_loss: 6.6076\n",
      "Epoch 9532/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 5.6957 - val_loss: 6.4640\n",
      "Epoch 9533/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.8560 - val_loss: 6.7697\n",
      "Epoch 9534/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.4617 - val_loss: 6.6207\n",
      "Epoch 9535/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 5.9654 - val_loss: 6.6072\n",
      "Epoch 9536/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.8161 - val_loss: 6.4110\n",
      "Epoch 9537/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.7199 - val_loss: 6.6648\n",
      "Epoch 9538/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.9462 - val_loss: 6.2038\n",
      "Epoch 9539/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.5189 - val_loss: 6.0951\n",
      "Epoch 9540/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.2911 - val_loss: 6.2788\n",
      "Epoch 9541/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1464 - val_loss: 6.5393\n",
      "Epoch 9542/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5212 - val_loss: 6.7609\n",
      "Epoch 9543/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3234 - val_loss: 6.9794\n",
      "Epoch 9544/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 5.9327 - val_loss: 7.1576\n",
      "Epoch 9545/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.4069 - val_loss: 6.9857\n",
      "Epoch 9546/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.0507 - val_loss: 6.6401\n",
      "Epoch 9547/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.7309 - val_loss: 6.3826\n",
      "Epoch 9548/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.4908 - val_loss: 6.2461\n",
      "Epoch 9549/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2703 - val_loss: 6.2036\n",
      "Epoch 9550/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.1300 - val_loss: 6.0824\n",
      "Epoch 9551/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 6.1136 - val_loss: 6.6332\n",
      "Epoch 9552/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 5.9713 - val_loss: 6.7935\n",
      "Epoch 9553/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.7364 - val_loss: 6.7750\n",
      "Epoch 9554/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 5.8159 - val_loss: 6.7666\n",
      "Epoch 9555/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.2613 - val_loss: 6.8159\n",
      "Epoch 9556/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 6.1012 - val_loss: 6.8084\n",
      "Epoch 9557/10000\n",
      "90/90 [==============================] - 0s 189us/step - loss: 6.2256 - val_loss: 6.6124\n",
      "Epoch 9558/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.1049 - val_loss: 6.7678\n",
      "Epoch 9559/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9202 - val_loss: 6.6104\n",
      "Epoch 9560/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3501 - val_loss: 6.6089\n",
      "Epoch 9561/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3244 - val_loss: 6.7069\n",
      "Epoch 9562/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.2691 - val_loss: 6.3670\n",
      "Epoch 9563/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.1942 - val_loss: 6.4492\n",
      "Epoch 9564/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.8703 - val_loss: 6.6743\n",
      "Epoch 9565/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.2341 - val_loss: 6.7850\n",
      "Epoch 9566/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 5.8159 - val_loss: 7.0313\n",
      "Epoch 9567/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.3098 - val_loss: 6.8137\n",
      "Epoch 9568/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 5.9382 - val_loss: 6.3655\n",
      "Epoch 9569/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 5.5805 - val_loss: 5.9707\n",
      "Epoch 9570/10000\n",
      "90/90 [==============================] - 0s 176us/step - loss: 6.2794 - val_loss: 6.0504\n",
      "Epoch 9571/10000\n",
      "90/90 [==============================] - 0s 201us/step - loss: 6.2829 - val_loss: 6.5300\n",
      "Epoch 9572/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 6.1616 - val_loss: 6.8532\n",
      "Epoch 9573/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.2355 - val_loss: 6.6570\n",
      "Epoch 9574/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 6.1569 - val_loss: 6.4408\n",
      "Epoch 9575/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.5208 - val_loss: 6.2792\n",
      "Epoch 9576/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.3038 - val_loss: 6.4036\n",
      "Epoch 9577/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1524 - val_loss: 6.9878\n",
      "Epoch 9578/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.2322 - val_loss: 6.9667\n",
      "Epoch 9579/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.2331 - val_loss: 6.6298\n",
      "Epoch 9580/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.2680 - val_loss: 6.6702\n",
      "Epoch 9581/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.8697 - val_loss: 6.3829\n",
      "Epoch 9582/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.0995 - val_loss: 6.3959\n",
      "Epoch 9583/10000\n",
      "90/90 [==============================] - 0s 169us/step - loss: 6.0730 - val_loss: 6.5159\n",
      "Epoch 9584/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.3112 - val_loss: 6.5334\n",
      "Epoch 9585/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3581 - val_loss: 6.7966\n",
      "Epoch 9586/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 5.7932 - val_loss: 6.8310\n",
      "Epoch 9587/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.9294 - val_loss: 6.5029\n",
      "Epoch 9588/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.8563 - val_loss: 6.5091\n",
      "Epoch 9589/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0511 - val_loss: 6.4446\n",
      "Epoch 9590/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.2214 - val_loss: 6.3818\n",
      "Epoch 9591/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.9986 - val_loss: 6.7214\n",
      "Epoch 9592/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1569 - val_loss: 6.7676\n",
      "Epoch 9593/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.2683 - val_loss: 6.5044\n",
      "Epoch 9594/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.3029 - val_loss: 6.2924\n",
      "Epoch 9595/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.1356 - val_loss: 6.2327\n",
      "Epoch 9596/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.9545 - val_loss: 6.5030\n",
      "Epoch 9597/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 5.8573 - val_loss: 6.9287\n",
      "Epoch 9598/10000\n",
      "90/90 [==============================] - 0s 89us/step - loss: 6.1584 - val_loss: 6.8367\n",
      "Epoch 9599/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.4468 - val_loss: 6.5635\n",
      "Epoch 9600/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3642 - val_loss: 6.6304\n",
      "Epoch 9601/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0478 - val_loss: 6.7918\n",
      "Epoch 9602/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.1346 - val_loss: 6.6361\n",
      "Epoch 9603/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3116 - val_loss: 6.3876\n",
      "Epoch 9604/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.6784 - val_loss: 6.3621\n",
      "Epoch 9605/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.8267 - val_loss: 6.6699\n",
      "Epoch 9606/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1005 - val_loss: 6.2197\n",
      "Epoch 9607/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.6020 - val_loss: 6.4718\n",
      "Epoch 9608/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.5624 - val_loss: 6.7779\n",
      "Epoch 9609/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1448 - val_loss: 6.7436\n",
      "Epoch 9610/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.9197 - val_loss: 6.4588\n",
      "Epoch 9611/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.7892 - val_loss: 6.3265\n",
      "Epoch 9612/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.9951 - val_loss: 6.3614\n",
      "Epoch 9613/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 6.0545 - val_loss: 6.4880\n",
      "Epoch 9614/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 6.0552 - val_loss: 6.6345\n",
      "Epoch 9615/10000\n",
      "90/90 [==============================] - 0s 194us/step - loss: 6.2378 - val_loss: 6.8627\n",
      "Epoch 9616/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.1088 - val_loss: 6.9027\n",
      "Epoch 9617/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.1194 - val_loss: 6.7490\n",
      "Epoch 9618/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.9969 - val_loss: 6.9728\n",
      "Epoch 9619/10000\n",
      "90/90 [==============================] - 0s 127us/step - loss: 6.3742 - val_loss: 6.5026\n",
      "Epoch 9620/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.2154 - val_loss: 5.9978\n",
      "Epoch 9621/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 6.1516 - val_loss: 5.9099\n",
      "Epoch 9622/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 6.0642 - val_loss: 6.5083\n",
      "Epoch 9623/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.9040 - val_loss: 6.8087\n",
      "Epoch 9624/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.0541 - val_loss: 6.6694\n",
      "Epoch 9625/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0214 - val_loss: 6.4386\n",
      "Epoch 9626/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.4248 - val_loss: 6.5015\n",
      "Epoch 9627/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.7631 - val_loss: 6.8372\n",
      "Epoch 9628/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.0314 - val_loss: 6.5745\n",
      "Epoch 9629/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3604 - val_loss: 6.6523\n",
      "Epoch 9630/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.4987 - val_loss: 6.4800\n",
      "Epoch 9631/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1978 - val_loss: 6.6198\n",
      "Epoch 9632/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.2544 - val_loss: 6.7713\n",
      "Epoch 9633/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.2841 - val_loss: 6.5609\n",
      "Epoch 9634/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 101us/step - loss: 5.7215 - val_loss: 6.9438\n",
      "Epoch 9635/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.3298 - val_loss: 6.5893\n",
      "Epoch 9636/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.2308 - val_loss: 6.3626\n",
      "Epoch 9637/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.0407 - val_loss: 6.3460\n",
      "Epoch 9638/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.0848 - val_loss: 6.3382\n",
      "Epoch 9639/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0057 - val_loss: 6.3024\n",
      "Epoch 9640/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.1560 - val_loss: 6.8110\n",
      "Epoch 9641/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.8461 - val_loss: 6.6462\n",
      "Epoch 9642/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 5.8303 - val_loss: 6.6049\n",
      "Epoch 9643/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.4821 - val_loss: 6.5301\n",
      "Epoch 9644/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.4725 - val_loss: 6.5813\n",
      "Epoch 9645/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.2601 - val_loss: 6.9453\n",
      "Epoch 9646/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 5.9609 - val_loss: 6.8272\n",
      "Epoch 9647/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.1462 - val_loss: 6.5522\n",
      "Epoch 9648/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1889 - val_loss: 6.4481\n",
      "Epoch 9649/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3783 - val_loss: 6.4219\n",
      "Epoch 9650/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.0093 - val_loss: 6.8167\n",
      "Epoch 9651/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1275 - val_loss: 6.7573\n",
      "Epoch 9652/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.7303 - val_loss: 6.7339\n",
      "Epoch 9653/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 5.8425 - val_loss: 6.6950\n",
      "Epoch 9654/10000\n",
      "90/90 [==============================] - 0s 185us/step - loss: 6.1620 - val_loss: 6.5508\n",
      "Epoch 9655/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.9421 - val_loss: 6.6270\n",
      "Epoch 9656/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0267 - val_loss: 6.4193\n",
      "Epoch 9657/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 5.8610 - val_loss: 6.6302\n",
      "Epoch 9658/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2019 - val_loss: 6.3790\n",
      "Epoch 9659/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.5015 - val_loss: 6.5746\n",
      "Epoch 9660/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.3487 - val_loss: 6.6125\n",
      "Epoch 9661/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9375 - val_loss: 6.2594\n",
      "Epoch 9662/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.5045 - val_loss: 6.0314\n",
      "Epoch 9663/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3690 - val_loss: 6.3981\n",
      "Epoch 9664/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.4057 - val_loss: 6.7728\n",
      "Epoch 9665/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.3994 - val_loss: 6.8063\n",
      "Epoch 9666/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.9309 - val_loss: 7.1322\n",
      "Epoch 9667/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.2741 - val_loss: 6.2877\n",
      "Epoch 9668/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1876 - val_loss: 6.0747\n",
      "Epoch 9669/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0131 - val_loss: 6.0850\n",
      "Epoch 9670/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.2738 - val_loss: 6.4417\n",
      "Epoch 9671/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.4906 - val_loss: 6.6562\n",
      "Epoch 9672/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3917 - val_loss: 6.6032\n",
      "Epoch 9673/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9359 - val_loss: 6.5852\n",
      "Epoch 9674/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.5768 - val_loss: 6.5576\n",
      "Epoch 9675/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.4030 - val_loss: 6.7842\n",
      "Epoch 9676/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1825 - val_loss: 6.7512\n",
      "Epoch 9677/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.4366 - val_loss: 7.0147\n",
      "Epoch 9678/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.8789 - val_loss: 7.1979\n",
      "Epoch 9679/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 6.1571 - val_loss: 6.4392\n",
      "Epoch 9680/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.0529 - val_loss: 6.6020\n",
      "Epoch 9681/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.1413 - val_loss: 6.2690\n",
      "Epoch 9682/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.8747 - val_loss: 6.2799\n",
      "Epoch 9683/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 6.0369 - val_loss: 6.4107\n",
      "Epoch 9684/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 6.2350 - val_loss: 6.7435\n",
      "Epoch 9685/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.7868 - val_loss: 6.8360\n",
      "Epoch 9686/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.2113 - val_loss: 6.3780\n",
      "Epoch 9687/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0868 - val_loss: 6.3133\n",
      "Epoch 9688/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0856 - val_loss: 6.4249\n",
      "Epoch 9689/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.3941 - val_loss: 6.4388\n",
      "Epoch 9690/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1373 - val_loss: 6.7420\n",
      "Epoch 9691/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.0163 - val_loss: 7.1902\n",
      "Epoch 9692/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.4014 - val_loss: 6.8120\n",
      "Epoch 9693/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.1123 - val_loss: 6.3205\n",
      "Epoch 9694/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 5.9289 - val_loss: 6.3851\n",
      "Epoch 9695/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3174 - val_loss: 6.1763\n",
      "Epoch 9696/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9968 - val_loss: 6.2195\n",
      "Epoch 9697/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.0258 - val_loss: 6.7253\n",
      "Epoch 9698/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.0351 - val_loss: 6.6222\n",
      "Epoch 9699/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.2944 - val_loss: 6.2780\n",
      "Epoch 9700/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.8849 - val_loss: 6.3448\n",
      "Epoch 9701/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.2641 - val_loss: 6.0991\n",
      "Epoch 9702/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2234 - val_loss: 6.4632\n",
      "Epoch 9703/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.0223 - val_loss: 6.7137\n",
      "Epoch 9704/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9146 - val_loss: 7.0478\n",
      "Epoch 9705/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.5766 - val_loss: 6.3468\n",
      "Epoch 9706/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.1584 - val_loss: 6.6503\n",
      "Epoch 9707/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.0650 - val_loss: 6.6716\n",
      "Epoch 9708/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.5497 - val_loss: 6.8037\n",
      "Epoch 9709/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.1360 - val_loss: 6.3814\n",
      "Epoch 9710/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.2270 - val_loss: 6.4552\n",
      "Epoch 9711/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.0313 - val_loss: 6.4603\n",
      "Epoch 9712/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0709 - val_loss: 6.5938\n",
      "Epoch 9713/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.4223 - val_loss: 6.4786\n",
      "Epoch 9714/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.1230 - val_loss: 6.0521\n",
      "Epoch 9715/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.1469 - val_loss: 6.0445\n",
      "Epoch 9716/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.9524 - val_loss: 6.5966\n",
      "Epoch 9717/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.1481 - val_loss: 6.6341\n",
      "Epoch 9718/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 6.1429 - val_loss: 6.6787\n",
      "Epoch 9719/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.2022 - val_loss: 6.6131\n",
      "Epoch 9720/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.0926 - val_loss: 6.5365\n",
      "Epoch 9721/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1658 - val_loss: 6.5575\n",
      "Epoch 9722/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.0814 - val_loss: 6.9054\n",
      "Epoch 9723/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 5.9710 - val_loss: 6.8731\n",
      "Epoch 9724/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 5.8862 - val_loss: 6.4008\n",
      "Epoch 9725/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1717 - val_loss: 6.2782\n",
      "Epoch 9726/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 5.8125 - val_loss: 6.2771\n",
      "Epoch 9727/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.8339 - val_loss: 6.6640\n",
      "Epoch 9728/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.1917 - val_loss: 6.5478\n",
      "Epoch 9729/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 6.6758 - val_loss: 6.6254\n",
      "Epoch 9730/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 5.7542 - val_loss: 6.4669\n",
      "Epoch 9731/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 6.0076 - val_loss: 6.7191\n",
      "Epoch 9732/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 5.9684 - val_loss: 6.6260\n",
      "Epoch 9733/10000\n",
      "90/90 [==============================] - 0s 179us/step - loss: 6.1293 - val_loss: 6.5340\n",
      "Epoch 9734/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.1826 - val_loss: 6.4100\n",
      "Epoch 9735/10000\n",
      "90/90 [==============================] - 0s 149us/step - loss: 5.9211 - val_loss: 6.4189\n",
      "Epoch 9736/10000\n",
      "90/90 [==============================] - 0s 183us/step - loss: 5.7835 - val_loss: 6.7839\n",
      "Epoch 9737/10000\n",
      "90/90 [==============================] - 0s 193us/step - loss: 5.7916 - val_loss: 6.9347\n",
      "Epoch 9738/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1709 - val_loss: 6.6417\n",
      "Epoch 9739/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3570 - val_loss: 6.4694\n",
      "Epoch 9740/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.8155 - val_loss: 6.5688\n",
      "Epoch 9741/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1347 - val_loss: 6.5041\n",
      "Epoch 9742/10000\n",
      "90/90 [==============================] - 0s 180us/step - loss: 6.3065 - val_loss: 6.5591\n",
      "Epoch 9743/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.9554 - val_loss: 6.5410\n",
      "Epoch 9744/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.7271 - val_loss: 6.6808\n",
      "Epoch 9745/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.1743 - val_loss: 6.7535\n",
      "Epoch 9746/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0802 - val_loss: 6.5122\n",
      "Epoch 9747/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 5.9985 - val_loss: 6.3656\n",
      "Epoch 9748/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1337 - val_loss: 6.3603\n",
      "Epoch 9749/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 5.7697 - val_loss: 6.3874\n",
      "Epoch 9750/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.8596 - val_loss: 6.4880\n",
      "Epoch 9751/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.3037 - val_loss: 6.7624\n",
      "Epoch 9752/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 5.6455 - val_loss: 6.7428\n",
      "Epoch 9753/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.7238 - val_loss: 6.5715\n",
      "Epoch 9754/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.3148 - val_loss: 6.3349\n",
      "Epoch 9755/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.0439 - val_loss: 6.3985\n",
      "Epoch 9756/10000\n",
      "90/90 [==============================] - 0s 140us/step - loss: 6.0494 - val_loss: 6.7301\n",
      "Epoch 9757/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.0649 - val_loss: 7.1279\n",
      "Epoch 9758/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 6.1198 - val_loss: 6.9012\n",
      "Epoch 9759/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1838 - val_loss: 6.5740\n",
      "Epoch 9760/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.3724 - val_loss: 6.7228\n",
      "Epoch 9761/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.0379 - val_loss: 6.8126\n",
      "Epoch 9762/10000\n",
      "90/90 [==============================] - 0s 91us/step - loss: 5.8259 - val_loss: 6.4199\n",
      "Epoch 9763/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.0635 - val_loss: 5.9579\n",
      "Epoch 9764/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3287 - val_loss: 6.0374\n",
      "Epoch 9765/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0598 - val_loss: 6.1870\n",
      "Epoch 9766/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.4695 - val_loss: 6.2460\n",
      "Epoch 9767/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.9167 - val_loss: 6.6378\n",
      "Epoch 9768/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 6.1000 - val_loss: 6.6660\n",
      "Epoch 9769/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.8548 - val_loss: 6.5816\n",
      "Epoch 9770/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 5.9995 - val_loss: 6.7953\n",
      "Epoch 9771/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.9059 - val_loss: 6.5101\n",
      "Epoch 9772/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.2006 - val_loss: 6.5930\n",
      "Epoch 9773/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0180 - val_loss: 6.6555\n",
      "Epoch 9774/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.6737 - val_loss: 6.7127\n",
      "Epoch 9775/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.3865 - val_loss: 6.5905\n",
      "Epoch 9776/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.9999 - val_loss: 6.5426\n",
      "Epoch 9777/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.0619 - val_loss: 6.3723\n",
      "Epoch 9778/10000\n",
      "90/90 [==============================] - 0s 166us/step - loss: 5.9553 - val_loss: 6.3378\n",
      "Epoch 9779/10000\n",
      "90/90 [==============================] - 0s 152us/step - loss: 5.9698 - val_loss: 6.3421\n",
      "Epoch 9780/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.1071 - val_loss: 6.1807\n",
      "Epoch 9781/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.0247 - val_loss: 6.4598\n",
      "Epoch 9782/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.1108 - val_loss: 6.7957\n",
      "Epoch 9783/10000\n",
      "90/90 [==============================] - 0s 145us/step - loss: 6.3887 - val_loss: 7.0824\n",
      "Epoch 9784/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0837 - val_loss: 7.0131\n",
      "Epoch 9785/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1012 - val_loss: 6.8260\n",
      "Epoch 9786/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.3185 - val_loss: 7.0197\n",
      "Epoch 9787/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.1247 - val_loss: 6.3457\n",
      "Epoch 9788/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 168us/step - loss: 6.5899 - val_loss: 6.1122\n",
      "Epoch 9789/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 5.8510 - val_loss: 6.3856\n",
      "Epoch 9790/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0956 - val_loss: 6.6151\n",
      "Epoch 9791/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.7202 - val_loss: 6.5171\n",
      "Epoch 9792/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.6453 - val_loss: 6.6777\n",
      "Epoch 9793/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.1937 - val_loss: 6.7555\n",
      "Epoch 9794/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2613 - val_loss: 6.6997\n",
      "Epoch 9795/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.0780 - val_loss: 6.4755\n",
      "Epoch 9796/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.4296 - val_loss: 6.7702\n",
      "Epoch 9797/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.0843 - val_loss: 6.8750\n",
      "Epoch 9798/10000\n",
      "90/90 [==============================] - 0s 177us/step - loss: 5.7186 - val_loss: 6.4579\n",
      "Epoch 9799/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.9516 - val_loss: 6.1192\n",
      "Epoch 9800/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.3543 - val_loss: 6.0951\n",
      "Epoch 9801/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.2918 - val_loss: 6.4323\n",
      "Epoch 9802/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.3761 - val_loss: 6.5857\n",
      "Epoch 9803/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1752 - val_loss: 6.5418\n",
      "Epoch 9804/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2000 - val_loss: 6.1520\n",
      "Epoch 9805/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 5.6931 - val_loss: 6.4324\n",
      "Epoch 9806/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 6.0898 - val_loss: 6.5827\n",
      "Epoch 9807/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 5.8075 - val_loss: 6.8577\n",
      "Epoch 9808/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.9722 - val_loss: 6.8670\n",
      "Epoch 9809/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.0855 - val_loss: 6.5376\n",
      "Epoch 9810/10000\n",
      "90/90 [==============================] - 0s 87us/step - loss: 6.3768 - val_loss: 6.3987\n",
      "Epoch 9811/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3246 - val_loss: 6.3261\n",
      "Epoch 9812/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.2090 - val_loss: 6.7993\n",
      "Epoch 9813/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.0323 - val_loss: 7.0028\n",
      "Epoch 9814/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 5.8791 - val_loss: 7.0173\n",
      "Epoch 9815/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1671 - val_loss: 6.4111\n",
      "Epoch 9816/10000\n",
      "90/90 [==============================] - 0s 94us/step - loss: 6.0291 - val_loss: 5.9818\n",
      "Epoch 9817/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.0649 - val_loss: 6.1107\n",
      "Epoch 9818/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.0540 - val_loss: 6.4992\n",
      "Epoch 9819/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.3332 - val_loss: 6.4013\n",
      "Epoch 9820/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.0593 - val_loss: 6.4823\n",
      "Epoch 9821/10000\n",
      "90/90 [==============================] - 0s 198us/step - loss: 6.1132 - val_loss: 6.3079\n",
      "Epoch 9822/10000\n",
      "90/90 [==============================] - 0s 172us/step - loss: 5.8571 - val_loss: 6.3217\n",
      "Epoch 9823/10000\n",
      "90/90 [==============================] - 0s 369us/step - loss: 5.9686 - val_loss: 6.1589\n",
      "Epoch 9824/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 5.7127 - val_loss: 6.2597\n",
      "Epoch 9825/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.2549 - val_loss: 6.4088\n",
      "Epoch 9826/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 5.7516 - val_loss: 6.6571\n",
      "Epoch 9827/10000\n",
      "90/90 [==============================] - 0s 155us/step - loss: 6.5642 - val_loss: 6.3379\n",
      "Epoch 9828/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.9546 - val_loss: 6.4224\n",
      "Epoch 9829/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.0706 - val_loss: 6.8174\n",
      "Epoch 9830/10000\n",
      "90/90 [==============================] - 0s 158us/step - loss: 6.1169 - val_loss: 6.9241\n",
      "Epoch 9831/10000\n",
      "90/90 [==============================] - 0s 117us/step - loss: 5.8586 - val_loss: 6.9844\n",
      "Epoch 9832/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 6.3374 - val_loss: 6.8514\n",
      "Epoch 9833/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1032 - val_loss: 6.8960\n",
      "Epoch 9834/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1661 - val_loss: 6.5919\n",
      "Epoch 9835/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2177 - val_loss: 6.4510\n",
      "Epoch 9836/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.2747 - val_loss: 6.1765\n",
      "Epoch 9837/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.2577 - val_loss: 6.5608\n",
      "Epoch 9838/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 5.7510 - val_loss: 6.4669\n",
      "Epoch 9839/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.6001 - val_loss: 6.3810\n",
      "Epoch 9840/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2017 - val_loss: 6.6133\n",
      "Epoch 9841/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.8988 - val_loss: 6.7432\n",
      "Epoch 9842/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1501 - val_loss: 6.2462\n",
      "Epoch 9843/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.7232 - val_loss: 6.0327\n",
      "Epoch 9844/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 5.9528 - val_loss: 6.2567\n",
      "Epoch 9845/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 5.8064 - val_loss: 6.2537\n",
      "Epoch 9846/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.1048 - val_loss: 6.4861\n",
      "Epoch 9847/10000\n",
      "90/90 [==============================] - 0s 124us/step - loss: 5.9228 - val_loss: 6.7320\n",
      "Epoch 9848/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0319 - val_loss: 6.6235\n",
      "Epoch 9849/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3032 - val_loss: 6.4500\n",
      "Epoch 9850/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 5.9297 - val_loss: 6.4632\n",
      "Epoch 9851/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.1676 - val_loss: 6.4687\n",
      "Epoch 9852/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 6.0153 - val_loss: 6.9109\n",
      "Epoch 9853/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 5.8531 - val_loss: 6.6866\n",
      "Epoch 9854/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.0929 - val_loss: 6.2176\n",
      "Epoch 9855/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.5498 - val_loss: 6.4560\n",
      "Epoch 9856/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.3135 - val_loss: 6.7283\n",
      "Epoch 9857/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1353 - val_loss: 7.1040\n",
      "Epoch 9858/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2704 - val_loss: 6.9040\n",
      "Epoch 9859/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.0807 - val_loss: 6.5763\n",
      "Epoch 9860/10000\n",
      "90/90 [==============================] - 0s 157us/step - loss: 6.1023 - val_loss: 6.1549\n",
      "Epoch 9861/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.5330 - val_loss: 6.2056\n",
      "Epoch 9862/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 6.2494 - val_loss: 6.5669\n",
      "Epoch 9863/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 5.8721 - val_loss: 6.8725\n",
      "Epoch 9864/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.6588 - val_loss: 6.7780\n",
      "Epoch 9865/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 5.8563 - val_loss: 6.9282\n",
      "Epoch 9866/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 5.9417 - val_loss: 6.6970\n",
      "Epoch 9867/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0156 - val_loss: 6.4717\n",
      "Epoch 9868/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.6606 - val_loss: 6.4099\n",
      "Epoch 9869/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.2327 - val_loss: 6.1863\n",
      "Epoch 9870/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.9717 - val_loss: 6.1026\n",
      "Epoch 9871/10000\n",
      "90/90 [==============================] - 0s 171us/step - loss: 6.1519 - val_loss: 6.1110\n",
      "Epoch 9872/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 5.9222 - val_loss: 6.6386\n",
      "Epoch 9873/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3983 - val_loss: 6.8718\n",
      "Epoch 9874/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.0513 - val_loss: 6.9953\n",
      "Epoch 9875/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 6.2219 - val_loss: 6.8437\n",
      "Epoch 9876/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2180 - val_loss: 6.2087\n",
      "Epoch 9877/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.8997 - val_loss: 6.2495\n",
      "Epoch 9878/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1383 - val_loss: 6.2945\n",
      "Epoch 9879/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.1715 - val_loss: 6.2683\n",
      "Epoch 9880/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.0494 - val_loss: 6.5787\n",
      "Epoch 9881/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 5.8644 - val_loss: 6.7982\n",
      "Epoch 9882/10000\n",
      "90/90 [==============================] - 0s 97us/step - loss: 5.9486 - val_loss: 6.7656\n",
      "Epoch 9883/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.0700 - val_loss: 6.1964\n",
      "Epoch 9884/10000\n",
      "90/90 [==============================] - 0s 200us/step - loss: 5.6976 - val_loss: 6.2295\n",
      "Epoch 9885/10000\n",
      "90/90 [==============================] - 0s 209us/step - loss: 6.4277 - val_loss: 6.2158\n",
      "Epoch 9886/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.0130 - val_loss: 6.4463\n",
      "Epoch 9887/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 6.2568 - val_loss: 6.3795\n",
      "Epoch 9888/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.8148 - val_loss: 6.4963\n",
      "Epoch 9889/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.1390 - val_loss: 6.6868\n",
      "Epoch 9890/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 5.6986 - val_loss: 6.3582\n",
      "Epoch 9891/10000\n",
      "90/90 [==============================] - 0s 86us/step - loss: 5.6493 - val_loss: 6.0365\n",
      "Epoch 9892/10000\n",
      "90/90 [==============================] - 0s 105us/step - loss: 5.7907 - val_loss: 6.1186\n",
      "Epoch 9893/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.3705 - val_loss: 6.3577\n",
      "Epoch 9894/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 6.2126 - val_loss: 6.7846\n",
      "Epoch 9895/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3474 - val_loss: 6.8832\n",
      "Epoch 9896/10000\n",
      "90/90 [==============================] - 0s 90us/step - loss: 6.1465 - val_loss: 6.9420\n",
      "Epoch 9897/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2729 - val_loss: 6.8286\n",
      "Epoch 9898/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1202 - val_loss: 6.6313\n",
      "Epoch 9899/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 5.7061 - val_loss: 6.7523\n",
      "Epoch 9900/10000\n",
      "90/90 [==============================] - 0s 100us/step - loss: 6.3272 - val_loss: 6.4385\n",
      "Epoch 9901/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 6.1232 - val_loss: 6.4644\n",
      "Epoch 9902/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.1759 - val_loss: 6.2632\n",
      "Epoch 9903/10000\n",
      "90/90 [==============================] - 0s 139us/step - loss: 5.7136 - val_loss: 6.3055\n",
      "Epoch 9904/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1549 - val_loss: 6.5285\n",
      "Epoch 9905/10000\n",
      "90/90 [==============================] - 0s 119us/step - loss: 6.0641 - val_loss: 6.5113\n",
      "Epoch 9906/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 6.4896 - val_loss: 6.5850\n",
      "Epoch 9907/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1013 - val_loss: 6.6958\n",
      "Epoch 9908/10000\n",
      "90/90 [==============================] - 0s 129us/step - loss: 6.0717 - val_loss: 6.5914\n",
      "Epoch 9909/10000\n",
      "90/90 [==============================] - 0s 165us/step - loss: 6.1566 - val_loss: 6.1679\n",
      "Epoch 9910/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.1401 - val_loss: 6.3837\n",
      "Epoch 9911/10000\n",
      "90/90 [==============================] - 0s 130us/step - loss: 5.9105 - val_loss: 6.6138\n",
      "Epoch 9912/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.7655 - val_loss: 7.0397\n",
      "Epoch 9913/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 6.3950 - val_loss: 6.5449\n",
      "Epoch 9914/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 6.2650 - val_loss: 6.4884\n",
      "Epoch 9915/10000\n",
      "90/90 [==============================] - 0s 136us/step - loss: 6.3412 - val_loss: 6.5492\n",
      "Epoch 9916/10000\n",
      "90/90 [==============================] - 0s 141us/step - loss: 5.4913 - val_loss: 6.4751\n",
      "Epoch 9917/10000\n",
      "90/90 [==============================] - 0s 102us/step - loss: 6.1036 - val_loss: 6.4110\n",
      "Epoch 9918/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.1471 - val_loss: 6.7474\n",
      "Epoch 9919/10000\n",
      "90/90 [==============================] - 0s 122us/step - loss: 6.2649 - val_loss: 6.6187\n",
      "Epoch 9920/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.1474 - val_loss: 6.6517\n",
      "Epoch 9921/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 5.6185 - val_loss: 6.4784\n",
      "Epoch 9922/10000\n",
      "90/90 [==============================] - 0s 131us/step - loss: 5.9307 - val_loss: 6.2233\n",
      "Epoch 9923/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.0258 - val_loss: 6.2575\n",
      "Epoch 9924/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 5.8914 - val_loss: 6.2568\n",
      "Epoch 9925/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0789 - val_loss: 6.3672\n",
      "Epoch 9926/10000\n",
      "90/90 [==============================] - 0s 126us/step - loss: 6.0723 - val_loss: 6.5935\n",
      "Epoch 9927/10000\n",
      "90/90 [==============================] - 0s 178us/step - loss: 5.9248 - val_loss: 6.6250\n",
      "Epoch 9928/10000\n",
      "90/90 [==============================] - 0s 186us/step - loss: 5.8924 - val_loss: 6.3410\n",
      "Epoch 9929/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.2023 - val_loss: 6.5077\n",
      "Epoch 9930/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.5623 - val_loss: 6.6745\n",
      "Epoch 9931/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 6.0611 - val_loss: 6.6940\n",
      "Epoch 9932/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.1723 - val_loss: 6.8446\n",
      "Epoch 9933/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.0973 - val_loss: 6.3106\n",
      "Epoch 9934/10000\n",
      "90/90 [==============================] - 0s 118us/step - loss: 5.8882 - val_loss: 6.2452\n",
      "Epoch 9935/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 5.6704 - val_loss: 5.9633\n",
      "Epoch 9936/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.8380 - val_loss: 6.1272\n",
      "Epoch 9937/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 6.2276 - val_loss: 6.5412\n",
      "Epoch 9938/10000\n",
      "90/90 [==============================] - 0s 144us/step - loss: 5.5870 - val_loss: 6.7722\n",
      "Epoch 9939/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 5.9182 - val_loss: 6.6594\n",
      "Epoch 9940/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.9314 - val_loss: 6.4304\n",
      "Epoch 9941/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 5.9317 - val_loss: 6.3722\n",
      "Epoch 9942/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 135us/step - loss: 5.8164 - val_loss: 6.4825\n",
      "Epoch 9943/10000\n",
      "90/90 [==============================] - 0s 163us/step - loss: 6.1463 - val_loss: 6.5476\n",
      "Epoch 9944/10000\n",
      "90/90 [==============================] - 0s 180us/step - loss: 5.9319 - val_loss: 6.8675\n",
      "Epoch 9945/10000\n",
      "90/90 [==============================] - 0s 137us/step - loss: 5.8616 - val_loss: 6.5893\n",
      "Epoch 9946/10000\n",
      "90/90 [==============================] - 0s 135us/step - loss: 6.3673 - val_loss: 6.2895\n",
      "Epoch 9947/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.4458 - val_loss: 6.2372\n",
      "Epoch 9948/10000\n",
      "90/90 [==============================] - 0s 120us/step - loss: 5.9977 - val_loss: 6.3158\n",
      "Epoch 9949/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.1033 - val_loss: 6.6132\n",
      "Epoch 9950/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.1471 - val_loss: 6.5886\n",
      "Epoch 9951/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 5.9984 - val_loss: 6.4301\n",
      "Epoch 9952/10000\n",
      "90/90 [==============================] - 0s 92us/step - loss: 5.7461 - val_loss: 6.1866\n",
      "Epoch 9953/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1296 - val_loss: 6.3503\n",
      "Epoch 9954/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9286 - val_loss: 6.2818\n",
      "Epoch 9955/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1289 - val_loss: 6.3417\n",
      "Epoch 9956/10000\n",
      "90/90 [==============================] - 0s 138us/step - loss: 6.1492 - val_loss: 6.5027\n",
      "Epoch 9957/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.9204 - val_loss: 6.9711\n",
      "Epoch 9958/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 5.9768 - val_loss: 6.8478\n",
      "Epoch 9959/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.0991 - val_loss: 6.6358\n",
      "Epoch 9960/10000\n",
      "90/90 [==============================] - 0s 115us/step - loss: 6.0174 - val_loss: 6.5349\n",
      "Epoch 9961/10000\n",
      "90/90 [==============================] - 0s 95us/step - loss: 6.1195 - val_loss: 6.7803\n",
      "Epoch 9962/10000\n",
      "90/90 [==============================] - 0s 111us/step - loss: 6.1620 - val_loss: 6.5004\n",
      "Epoch 9963/10000\n",
      "90/90 [==============================] - 0s 113us/step - loss: 6.0278 - val_loss: 6.3149\n",
      "Epoch 9964/10000\n",
      "90/90 [==============================] - 0s 101us/step - loss: 6.4010 - val_loss: 6.3148\n",
      "Epoch 9965/10000\n",
      "90/90 [==============================] - 0s 110us/step - loss: 5.9955 - val_loss: 6.3763\n",
      "Epoch 9966/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1057 - val_loss: 6.3719\n",
      "Epoch 9967/10000\n",
      "90/90 [==============================] - 0s 121us/step - loss: 6.2024 - val_loss: 6.1976\n",
      "Epoch 9968/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.1340 - val_loss: 6.5301\n",
      "Epoch 9969/10000\n",
      "90/90 [==============================] - 0s 132us/step - loss: 5.9037 - val_loss: 6.7604\n",
      "Epoch 9970/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 5.8879 - val_loss: 6.8055\n",
      "Epoch 9971/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.0326 - val_loss: 6.7193\n",
      "Epoch 9972/10000\n",
      "90/90 [==============================] - 0s 96us/step - loss: 5.6898 - val_loss: 6.6734\n",
      "Epoch 9973/10000\n",
      "90/90 [==============================] - 0s 104us/step - loss: 6.2838 - val_loss: 6.1335\n",
      "Epoch 9974/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.9086 - val_loss: 6.3333\n",
      "Epoch 9975/10000\n",
      "90/90 [==============================] - 0s 103us/step - loss: 6.1841 - val_loss: 6.4909\n",
      "Epoch 9976/10000\n",
      "90/90 [==============================] - 0s 106us/step - loss: 6.2197 - val_loss: 6.5258\n",
      "Epoch 9977/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.2513 - val_loss: 6.3470\n",
      "Epoch 9978/10000\n",
      "90/90 [==============================] - 0s 108us/step - loss: 6.1588 - val_loss: 6.5935\n",
      "Epoch 9979/10000\n",
      "90/90 [==============================] - 0s 125us/step - loss: 6.0126 - val_loss: 6.4665\n",
      "Epoch 9980/10000\n",
      "90/90 [==============================] - 0s 93us/step - loss: 5.7510 - val_loss: 6.3421\n",
      "Epoch 9981/10000\n",
      "90/90 [==============================] - 0s 133us/step - loss: 6.1331 - val_loss: 6.3745\n",
      "Epoch 9982/10000\n",
      "90/90 [==============================] - 0s 98us/step - loss: 6.3542 - val_loss: 6.4099\n",
      "Epoch 9983/10000\n",
      "90/90 [==============================] - 0s 107us/step - loss: 5.8664 - val_loss: 6.7906\n",
      "Epoch 9984/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 6.1858 - val_loss: 7.1508\n",
      "Epoch 9985/10000\n",
      "90/90 [==============================] - 0s 146us/step - loss: 6.4192 - val_loss: 7.0625\n",
      "Epoch 9986/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 5.7726 - val_loss: 6.5854\n",
      "Epoch 9987/10000\n",
      "90/90 [==============================] - 0s 128us/step - loss: 5.8807 - val_loss: 6.3923\n",
      "Epoch 9988/10000\n",
      "90/90 [==============================] - 0s 114us/step - loss: 6.0964 - val_loss: 6.4033\n",
      "Epoch 9989/10000\n",
      "90/90 [==============================] - 0s 116us/step - loss: 6.3294 - val_loss: 6.4171\n",
      "Epoch 9990/10000\n",
      "90/90 [==============================] - 0s 143us/step - loss: 5.9438 - val_loss: 6.8822\n",
      "Epoch 9991/10000\n",
      "90/90 [==============================] - 0s 142us/step - loss: 6.1531 - val_loss: 6.8347\n",
      "Epoch 9992/10000\n",
      "90/90 [==============================] - 0s 134us/step - loss: 6.1883 - val_loss: 6.3024\n",
      "Epoch 9993/10000\n",
      "90/90 [==============================] - 0s 112us/step - loss: 6.3616 - val_loss: 5.9499\n",
      "Epoch 9994/10000\n",
      "90/90 [==============================] - 0s 253us/step - loss: 5.7428 - val_loss: 5.9441\n",
      "Epoch 9995/10000\n",
      "90/90 [==============================] - 0s 181us/step - loss: 5.9924 - val_loss: 6.0139\n",
      "Epoch 9996/10000\n",
      "90/90 [==============================] - 0s 123us/step - loss: 5.7231 - val_loss: 6.5365\n",
      "Epoch 9997/10000\n",
      "90/90 [==============================] - 0s 88us/step - loss: 6.2480 - val_loss: 6.3608\n",
      "Epoch 9998/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 5.8513 - val_loss: 6.4720\n",
      "Epoch 9999/10000\n",
      "90/90 [==============================] - 0s 109us/step - loss: 6.3327 - val_loss: 6.4465\n",
      "Epoch 10000/10000\n",
      "90/90 [==============================] - 0s 99us/step - loss: 5.7086 - val_loss: 6.6352\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train.reshape(-1,1), \n",
    "          np.repeat(y_train.reshape(-1,1),2,axis=1), \n",
    "          epochs=10000, validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and visualize the results. Have we catched the heteroscedastic uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = np.arange(low,high,0.01).reshape(-1,1)\n",
    "y_test = f(X_test) + np.random.normal(\n",
    "    0,std,size=(X_test.shape[0])).reshape(-1,1)+[\n",
    "    np.random.normal(0,i) for i in (np.sin(X_test*-10))+1]\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "plt.errorbar(X_test,prediction[:,0],\n",
    "             yerr=2*prediction[:,1],\n",
    "             color='#0A5FB4',\n",
    "             alpha=0.8,\n",
    "             label='prediction')\n",
    "\n",
    "plt.plot(X_test, y_test,'x',c='r',\n",
    "         alpha=0.8, label='real generated data')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the Epistemic uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the *uncertainty quantification* field there is an approach that classifies uncertainty in two different categories:\n",
    "\n",
    "1. Aleatoric Uncertainty: If there exists a variability in our possible correct predictions for the same initial inputs (see previous section).\n",
    "\n",
    "2. Epistemic Uncertainty: related to our ignorance:\n",
    "\n",
    "     2.1. We are not using the correct model ϕw to approximate the hyphothetical function f\n",
    "     \n",
    "     2.2. Our incomplete knowledge of the types of data that exists.\n",
    "\n",
    "There are different techniques in the literature for modeling the epistemic uncertainty. Here, we will cover the MCMC approach using tensorflow and the tensorflow_probability library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**. Consider a data set $\\{(\\mathbf{x}_n, y_n)\\}$, where each data point comprises of features $\\mathbf{x}_n\\in\\mathbb{R}^D$ and output $y_n\\in\\mathbb{R}$. Define the likelihood for each data point as $$\\begin{aligned} p(y_n \\mid \\mathbf{w}, \\mathbf{x}_n, \\sigma^2) &= \\text{Normal}(y_n \\mid \\mathrm{NN}(\\mathbf{x}_n\\;;\\;\\mathbf{w}), \\sigma^2),\\end{aligned}$$\n",
    "\n",
    "where $\\mathrm{NN}$ is a neural network whose weights and biases form the latent variables $\\mathbf{w}$. Assume $\\sigma^2$ is a known variance.\n",
    "\n",
    "Define the prior on the weights and biases $\\mathbf{w}$ to be the standard normal $$\\begin{aligned} p(\\mathbf{w}) &= \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\mathbf{I}).\\end{aligned}$$\n",
    "\n",
    "The question here is: how these parameters (now random variables) are distributed (~posterior distribution)? Could you give an estimation of these variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the inference problem using MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted in the introduction of this notebook, for most practical problems of interest exact inference is hard or can not be performed analytically. That is why some form of approximation need to be done. \n",
    "\n",
    "In this section we consider approximate inference methods based of numerical sampling to get the distribution of the latent variables.\n",
    "The main idea behing MCMC is to generate samples from the posterior distribution by constructing a reversible Markov-chain that has as its equilibrium distribution the target posterior distribution. In essence, MCMC will allow us finding the expectation of some function with repect to a probability distribution -- for instance, mean and variance of the latent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Using the Hamiltonian Monte Carlo `tfp.mcmc.HamiltonianMonteCarlo` method estimate the weights and biases of a linear regression problem.\n",
    "\n",
    "Use as data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w, noise_std=0.1):\n",
    "    x = np.random.randn(N)\n",
    "    y = x * w + np.random.normal(0, noise_std, size=N)\n",
    "    return x, y\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 1  # number of features\n",
    "\n",
    "w_true = np.random.randn()\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(N, w_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "def linear_model(x_data=X_train):\n",
    "    A = ed.Normal(loc=0., scale=10., name=\"A\")  \n",
    "    b = ed.Normal(loc=0., scale=10., name=\"b\")  \n",
    "    mu = A * x_data + b\n",
    "    y_data = ed.Normal(loc=mu, scale=tf.ones(N), name=\"y_data\")  # `y` above\n",
    "    return y_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the log-joint probability function using `ed.make_log_joint_fn`, and implements MCMC with the `tfp.mcmc.sample_chain` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_joint = ed.make_log_joint_fn(linear_model)\n",
    "\n",
    "\n",
    "def target_log_prob_fn(A, b):\n",
    "    return log_joint(\n",
    "      x_data=X_train,\n",
    "      A=A,\n",
    "      b=b,\n",
    "      y_data=y_train)\n",
    "\n",
    "\n",
    "num_results = 5000\n",
    "num_burnin_steps = 3000\n",
    "\n",
    "states, kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    current_state=[\n",
    "        tf.zeros([], name='init_A'),\n",
    "        tf.zeros([], name='init_b'),\n",
    "    ],\n",
    "    kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=target_log_prob_fn,\n",
    "        step_size=0.008,\n",
    "        num_leapfrog_steps=5))\n",
    "\n",
    "A, b = states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Coefficient:  0.88024026 +- 0.031151734 \n",
      "b Coefficient:  0.00092236785 +- 0.026711533\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "[A_mcmc, b_mcmc] = sess.run([A, b])\n",
    "\n",
    "print(\"A Coefficient: \", A_mcmc.mean(), '+-', A_mcmc.var()  \n",
    "      , \"\\nb Coefficient: \", b_mcmc.mean(), '+-', b_mcmc.var() )\n",
    "\n",
    "def visualise(X_train, y_train, X_test, y_test, w, b, n_samples=10):\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(X_test, y_test)\n",
    "    inputs = np.linspace(min(X_train.min(), X_test.min()),\n",
    "                         max(X_train.max(), X_test.max()), \n",
    "                         num=400)\n",
    "    for ns in range(n_samples):\n",
    "        #output = inputs * w[ns] + b[ns]\n",
    "        output = inputs * w.mean() + b.mean()\n",
    "        plt.plot(inputs, np.random.normal(output), 'r+')\n",
    "        \n",
    "visualise(X_train, y_train, X_test, y_test, A_mcmc, b_mcmc, n_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach for solving the inference problem at hand is the Variational inference one. In this approach, the density function is estimated by choosing a distribution we know (eg. Gaussian) and progressively changing its parameters via optimization until it looks like the one we want to compute, the posterior. \n",
    "This “made-up” distribution we are optimizing is called variational distribution. \n",
    "\n",
    "Ex. Derive mathematically the equivalence between choosing the optimal parameters for the variational distribution and maximizing a lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout (Hinton et al) is a technique used to avoid over-fitting in our model. In essence, dropout technique zeros out neurons randomly according to a Bernoulli distribution. \n",
    "\n",
    "In the context of Bayesian Deep Learning, dropout can be seen as a Gaussian process approximation. In order to get uncertainty estimates from dropout, we just have to apply it both when performing training and prediction steps.\n",
    "\n",
    "Predictive mean and variance can be obtained from the following equations:\n",
    "\n",
    "$$ \\mathbb{E}(y) \\sim \\frac{1}{T} \\sum_{t=0}^{t=T} \\hat{y}_t (x)$$\n",
    "\n",
    "$$ Var(y) \\sim \\tau^{-1} \\mathbb{I}_D + \\frac{1}{T} \\sum_{t=0}^{t=T} \\hat{y}_t (x)^T \\hat{y}_t (x) - \\mathbb{E}(y)^T\\mathbb{E}(y)$$\n",
    "\n",
    "where $\\hat{y}_t$ are the predictions and \n",
    "\n",
    "$$\\tau = \\frac{l^2 p}{2N\\lambda}$$\n",
    "\n",
    "summarizes our Gaussian process precision, with $l$ a prior length-scale that captures our belief over the function frequency, $p$ the probability of the units not being dropped, $N$ the number of points and $\\lambda$ the weight decay parameter.\n",
    "\n",
    "Let's see how to apply all these concepts taken our cosine regression problem (seen in the first classes of the course) as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Implement $\\tau$ parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tau(l, p, N, weight_decay):\n",
    "    return l**2 * (1 - p) / (2 * N * weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c4878f2b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "f = lambda x: np.cos(x)\n",
    "x_train = np.linspace(-2*np.pi, 1.5*np.pi,10000)\n",
    "y_train = f(x_train)\n",
    "\n",
    "x_test = np.linspace(1.505*np.pi,2.5*np.pi,50)\n",
    "y_test = f(x_test)\n",
    "\n",
    "x_train_norm = (x_train-x_train.mean())/(x_train.std())\n",
    "y_train_norm = (y_train-y_train.mean())/(y_train.std())\n",
    "\n",
    "plt.plot(x_train, y_train, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 3s 369us/step - loss: 1.1651 - val_loss: 2.0022\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 0.7168 - val_loss: 1.5652\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.4903 - val_loss: 0.9719\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1s 67us/step - loss: 0.3627 - val_loss: 0.6759\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.3049 - val_loss: 0.4943\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.2635 - val_loss: 0.3906\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 60us/step - loss: 0.2302 - val_loss: 0.3235\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.2179 - val_loss: 0.3039\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.2044 - val_loss: 0.3018\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.1911 - val_loss: 0.2843\n"
     ]
    }
   ],
   "source": [
    "i = Input(name=\"input\", shape=(1,), dtype='float32')\n",
    "l = layers.Dense(32, input_shape=(1,), activation='relu')(i)\n",
    "l = layers.BatchNormalization()(l)\n",
    "l = layers.Dropout(0.3)(l, training=True)\n",
    "l = layers.Dense(32, activation='relu')(l)\n",
    "l = layers.BatchNormalization()(l)\n",
    "l = layers.Dropout(0.3)(l, training=True)\n",
    "l = layers.Dense(32, activation='relu')(l)\n",
    "l = layers.BatchNormalization()(l)\n",
    "l = layers.Dropout(0.7)(l, training=True)\n",
    "out = layers.Dense(1, activation='tanh')(l)\n",
    "\n",
    "m = Model(inputs=[i], outputs=out)\n",
    "\n",
    "m.compile(optimizer=optimizers.rmsprop(), loss=losses.mean_squared_error)\n",
    "\n",
    "h = m.fit(x_train_norm, y_train_norm, batch_size=128, epochs=10, validation_split=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = []\n",
    "for _ in range(100):\n",
    "    probs += [m.predict(x_train_norm)]\n",
    "predictive_mean = np.mean(probs, axis=0)\n",
    "predictive_variance = np.var(probs, axis=0)\n",
    "#t = tau(l, p, N, weight_decay)\n",
    "#predictive_variance += tau**-1\n",
    "plt.plot(x_train_norm, y_train_norm, 'r')\n",
    "\n",
    "plt.errorbar(x_train_norm, predictive_mean,\n",
    "             yerr=2*predictive_variance,\n",
    "             color='#0A5FB4',\n",
    "             alpha=0.8,\n",
    "             label='prediction')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the session, get the mean values of the weight and the bias and visualize the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://medium.com/@joeDiHare/deep-bayesian-neural-networks-952763a9537\n",
    "\n",
    "http://pyro.ai/examples/bayesian_regression.html\n",
    "\n",
    "http://edwardlib.org/tutorials/supervised-regression\n",
    "\n",
    "https://docs.pymc.io/notebooks/bayesian_neural_network_advi.html\n",
    "\n",
    "https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245\n",
    "\n",
    "https://github.com/arturzeitler/Bayes-and-MC/blob/master/Bayesian_NN_Example.ipynb\n",
    "\n",
    "https://arxiv.org/pdf/1610.09787.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl_win)",
   "language": "python",
   "name": "dl_win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
