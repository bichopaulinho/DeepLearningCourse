{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement Learning (DRL) -- i.e., the application of deep neural networks into the Reinforcement Learning (RL) framework -- has recently attracted a lot of attention in the machine learning community due to the significant amount of progress that has been made in the past few years. For instance, DRL is the theory behind AlphaGo, the program which was able to defeat world champion Lee Sedol in the ancient game of Go; or the theory that\n",
    "allows computers learning to play ATARI games at a superhuman level! \n",
    "\n",
    "In this class we will learn about DRL, first exploring two of the most popular classes of reinforcement learning algorithms -- Q Learning, a value iteration method, and the Vanilla Policy Gradient, a method to directly optimize in the action space -- and then focusing in the Deep Q-Network algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "from datetime import timedelta\n",
    "\n",
    "start = int(timedelta(seconds=22).total_seconds())\n",
    "\n",
    "YouTubeVideo(\"W_gxLKSsSIE\", start=start, autoplay=0, theme=\"light\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Reinforcement Learning"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is an area of machine learning that consists of learning from interaction how to behave in order to achieve a goal. An agent -- or learner -- must discover by trial-and-error which actions are the best in terms of the rewards that it receives and can receive in the future from the environmet.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A popular example of reinforcement learning is a chess engine. Here, the\n",
    "agent decides upon a series of moves depending on the state of the board (the environment), and the reward can be defined as win or lose at the end of the game.\n",
    "\n",
    "A maze is another good example. In this case\n",
    "\n",
    "• The agent is the intelligent program\n",
    "\n",
    "• The environment is the maze\n",
    "\n",
    "• The state is the place in the maze where the agent is\n",
    "\n",
    "• The action is the move we take to move to the next state\n",
    "\n",
    "• The reward is the points associated with reaching a particular state. It can be positive, negative, or zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.samyzaf.com/ML/rl/images/dcq.gif)\n",
    "Fig. Maze example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration vs Exploitation dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"_The general lesson learnt in reinforcement learning is that neither pure exploitation nor pure exploration are effective: they should be blended and, for effective learning, an optimal tradeoff must be found._\", by M. Vergassola, E. Villermaux, B. I. Shraiman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"520\" height=\"380\" controls>\n",
    "  <source src=\"../imgs/filmshr.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Multi-armed Bandit problem.__ Imagine you are in a casino facing multiple slot machines and each is configured with an unknown probability of how likely you can get a reward at one play. \n",
    "\n",
    "What is the best strategy to achieve highest long-term rewards?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Elements\n",
    "\n",
    "A reinforcement learning system should have the following elements in order to be defined: \n",
    "\n",
    "- State: it defines “how the environment is” at a particular time. \n",
    "\n",
    "- Policy: gives the behaviour of the agent at a given time. The policy maps states $S$ of the environment to actions $A$ to be taken. \n",
    "\n",
    "$$ \\pi (S): S \\rightarrow A $$\n",
    "\n",
    "- Model of the environment (optional). It allows inferences to be made about how the environment will behave.\n",
    "Methods for solving reinforcement learning problems when models are available\n",
    "are called model-based methods. In other cases, the method is said to be model-free (trial-and-error learners).\n",
    "\n",
    "We assume from now on the MDP property in our experiments, i.e., the future only depends on the current state, not the history.\n",
    "\n",
    "- Reward signal: It is the feedback that the environment sends to the agent after executing an action. The goal in RL is to maximize the total long run reward\n",
    "\n",
    "- Value function: specifies what is good in the long run, i.e., predicts the amount of long-term reward based on the selected action in the current state. \n",
    "\n",
    "If the sequence of rewards received after time step t is denoted\n",
    "$r_{t+1}$, $r_{t+2}$, $r_{t+3}$,..., then the expected return, denoted $G_t$, is\n",
    "defined as\n",
    "\n",
    "$$G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ··· + r_T = \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1}$$\n",
    "\n",
    "where the discount rate $\\gamma$ determines the present value of future rewards (it is worse as k grows).\n",
    "\n",
    "Assuming the MDP property, the value function of a state $s$ under a policy $\\pi$, denoted $V_{\\pi}(s)$, can be then computed as the expected return\n",
    "\n",
    "$$V_{\\pi}(s) = E_{\\pi} \\{G_t | s_t = s\\} = E_{\\pi}\\left \\{\\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} | s_t = s\\right \\}$$\n",
    "\n",
    "where $E_{\\pi}\\{\\}$ denotes the expected value of a random variable given that the agent follows\n",
    "policy $\\pi$, and $t$ is any time step. We call the function $V_{\\pi}$ the state-value function for policy $\\pi$.\n",
    "\n",
    "In the same way, we can define the action-value function $Q_{\\pi}(s,a)$ as the value of taking action $a$ in state $s$ under a policy $\\pi$:\n",
    "\n",
    "$$Q_{\\pi}(s, a) = E_{\\pi} \\{G_t | s_t = s, a_t = a\\} = E_{\\pi}\\left \\{\\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} | s_t = s, a_t = a\\right \\}$$\n",
    "\n",
    "Most of the reinforcement learning methods involve the estimation of these value functions, as we will see later in the Q-Learning section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../imgs/gridworld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But before, let's see how to easily simulate a wide set of environments with the help of  the OpenAI Gym toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenGym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym is a Python-based framework that provides an environment to simulate many popular games in order to implement and practice the reinforcement learning algorithms.\n",
    "\n",
    "All Gym environments expose a common interface, allowing you:\n",
    "- to write general algorithms and \n",
    "- to compare them easily\n",
    "\n",
    "Moreover, Gym is compatible with any numerical computation library, such as TensorFlow or Theano.\n",
    "\n",
    "Next lines cover the basics to set and run a Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the list of all the available environments in OpenAI Gym, use the following piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.animation as anm\n",
    "from matplotlib import rc\n",
    "rc('animation', html='html5')\n",
    "#rc('animation', ffmpeg_path='/usr/bin/ffmpeg')\n",
    "\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenAI Gym, each environment is represented by an env object. Each env object contains the following main functions:\n",
    " - make: creates the environment by passing an id string (environment name).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reset: resets the environment to the original state. It is the function you need to call when run a process for the first time, so it returns an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- render: creates a visual representation of the environment. From now on, we will use the following function to render an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_render(env_vis):\n",
    "    \"\"\"\n",
    "    param env_vis: List of env.render(mode = 'rgb_array') objects\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plot = plt.imshow(env_vis[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        plot.set_data(env_vis[i])\n",
    "\n",
    "    anim = anm.FuncAnimation(plt.gcf(),\n",
    "                             animate,\n",
    "                             frames=len(env_vis),\n",
    "                             interval=20,\n",
    "                             repeat=True,\n",
    "                             repeat_delay=20)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render([env.render(mode = 'rgb_array')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions are of type 'Space', and they are useful for describing the format of valid actions and observations:\n",
    "- env.action_space: defines the set of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- env.observation_space: as before, this function defines the set of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space can be discrete (gym.spaces.Discrete) or continous (gym.spaces.Box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, observations and rewards are obtained using the step function. Let us review it:\n",
    "- step: takes an action object as an argument and returns four objects:\n",
    "    - observation: An object implemented by the environment, representing the observation of the environment.\n",
    "    - reward: A signed float value indicating the gain (or loss) from the previous action.\n",
    "    - done: A Boolean value representing if the scenario is finished. \n",
    "    - info: A Python dictionary object representing the diagnostic information.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our use cases: CartPole-v0 and Pong-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpole is an inverted pendulum with a center of gravity above its pivot point. \n",
    "In essence, a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "The system is controlled by applying a force of +1 or -1 to the cart. This force moves the pivot point under the center of mass to keep the pole balanced, preventing it from falling over.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "In the above figure, the violet square indicates the pivot point. Red and green arrows show possible horizontal forces that can be applied to the pivot point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A failure is said to occur if the pole falls past 15 degrees from the vertical, or the cart moves more than 2.4 units from the center. Therefore, the reward in this case could be +1 for every time step on which failure did not occur,   and the return at each time could be computed as the number of steps until failure. Naturally, the return is maximized by keeping the pole balanced for as long as possible.\n",
    "\n",
    "Take a look at a video below with a real-life demonstration of a cartpole problem learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"XiigTGKZfks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above definitions for OpenAI Gym, print the state space dimension and her bounds for the CartPole-v0 environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run and visualize the result of applying a random policy to the cartPole-vo problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE: Do the same for the Pong-v0 environment for one episode and 300 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "Q-Learning is a model-free, off-policy temporal difference control algorithm developed by Watkins in 1989.\n",
    "This means that the agent learns from experience adopting the best Q value independently of the current policy.\n",
    "\n",
    "In essence, this method learns an action-value function and update it according to the Bellman equation using the relation:\n",
    "\n",
    "$$Q\\left(s_t, a_t\\right) = Q\\left(s_t, a_t\\right) + \\alpha \\left(R_{t+1} + \\gamma  max_a Q(s_{t+1}, a) - Q\\left(s_t, a_t\\right) \\right)$$\n",
    "\n",
    "The Bellman equation is a recurrent equation that expresses\n",
    "a relationship between the value of a state and the values of\n",
    "its successor states, and allows for finding the optimal policy in a RL problem.\n",
    "\n",
    "In this case, the learned action-value function, $Q$, directly approximates the optimal\n",
    "action-value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The algorithm**\n",
    "\n",
    "We can break the algorithm into the next 4 steps:\n",
    "\n",
    "1.-At time step $t$, we start from state $S_t$ and pick action according to the Q values, \n",
    "\n",
    "$$A_t=argmax_{a\\in A} Q\\left(S_t,a\\right)$$ ; $\\epsilon$-greedy is commonly applied.\n",
    "\n",
    "2.- With action $A_t$, we observe reward $R_{t+1}$ and get into the next state $s_{t+1}$.\n",
    "\n",
    "3.- Update the action-value function: \n",
    "\n",
    "$$Q\\left(S_t,A_t\\right) \\leftarrow Q\\left(S_t,A_t\\right)+\\alpha(R_{t+1}+\\gamma max_{a \\in A} Q\\left(S_{t+1},a\\right)−Q\\left(S_t,A_t\\right))$$\n",
    "\n",
    "4.- $t = t+1$ and repeat from step 1.\n",
    "\n",
    "$Q(s, a)$ can be implemented as a Q-Table or as a neural network (known as a Q-Network). In both cases, the task of the Q-Table or the Q-Network is to provide the best possible action based on the Q value of the given input. The Q-Table-based approach generally becomes intractable as the Q-Table becomes large, thus making neural networks the best candidate for approximating the Q-function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Q-learning with a Q-Table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Q-table, each row represent a possible state in which the agent could be. Columns represent the possible actions in the environment. Each cell of the table keep a q-value.\n",
    "\n",
    "The steps required to implement the Q-Table solution are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- Discretize the state space with the help of the [wrapper](https://github.com/openai/gym/tree/master/gym/wrappers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"This wrapper converts a Box observation into a single integer.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins=10, low=None, high=None):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "\n",
    "        low = self.observation_space.low if low is None else low\n",
    "        high = self.observation_space.high if high is None else high\n",
    "\n",
    "        self.n_bins = n_bins\n",
    "        self.val_bins = [np.linspace(l, h, n_bins + 1) for l, h in\n",
    "                         zip(low, high)]\n",
    "        self.observation_space = gym.spaces.Discrete(n_bins ** np.array(low).shape[0])\n",
    "\n",
    "    def _convert_to_one_number(self, digits):\n",
    "        return sum([d * ((self.n_bins + 1) ** i) for i, d in enumerate(digits)])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        digits = [np.digitize([x], bins)[0]\n",
    "                  for x, bins in zip(observation.flatten(), self.val_bins)]\n",
    "        return self._convert_to_one_number(digits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the state space using the DiscretizedObservationWrapper class\n",
    "env = gym.make('CartPole-v0')\n",
    "num_states = 10\n",
    "env = DiscretizedObservationWrapper(env, n_bins=num_states, \n",
    "                                    low=[-2.4, -2.0, -0.42, -3.5], \n",
    "                                    high=[2.4, 2.0, 0.42, 3.5])\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- Implement the Q-value function $Q(s,a)$ using either a python dict or a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "Q = defaultdict(float)\n",
    "\n",
    "def update_Q(env, s, r, a, s_next, done, params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Define an $\\epsilon$-greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_q_table(state, env, params):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.- Define a function to run an episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(env, params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5.- Collect all episodes in a function that runs over a given number of episodes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(env, params):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.- To conclude, run an experiment with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'gamma': 0.99, \n",
    "          'alpha': 0.5,\n",
    "          'learning_rate': 0.8,\n",
    "          'discount_rate': 0.9,\n",
    "          'explore_rate': 0.2,\n",
    "          'exploration_decay': 0.1,\n",
    "          'n_episodes': 100,\n",
    "          'r_max': 0, \n",
    "          't_max': 0\n",
    "         }\n",
    "\n",
    "experiment(env, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Network**\n",
    "\n",
    "As it said before, the Q value can be approximated using an Artifical Neural Network, that can be a multi-layer dense neural network, a convolutional network, or a recurrent network, depending on the problem.\n",
    "In our case, we will simulate a multi-layer neural network, with loss function:\n",
    "\n",
    "$$ L = \\left( r + \\gamma max_{a'}{\\hat{Q}\\left( s, a' \\right)} - Q\\left( s, a \\right)\\right )^2$$\n",
    "\n",
    "The steps necessary to implement the neural network solution are described now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.- Build a neural network with two hidden layers (24 neurons and relu activation function). The input layer has dimension equal to the number of states, while the output dimension is equal to the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules from keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tb = TensorBoard(log_dir='./QN_logs', histogram_freq=1)\n",
    "#tensorboard --logdir=./logs\n",
    "\n",
    "# Neural Net for Deep-Q learning Model\n",
    "def q_network(num_states, num_actions):\n",
    "    #return model\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- Redefine the e-greedy policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_q_nn(env, state, params):\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_network(state, action, reward, next_state, done):\n",
    "    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2.- Adapt the episode function to run Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_episode(env, params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.- Modify the experiment function and run the code with the following set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.8,\n",
    "          'discount_rate': 0.9,\n",
    "          'epsilon': 1.0,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_decay': 0.995,\n",
    "          'n_episodes': 100,\n",
    "          'r_max': 0,\n",
    "          't_max': 0\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "def experiment_dqn(env, params):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "#state = np.reshape(state, [1, state_size])\n",
    "model = q_network(state_size, action_size)\n",
    "model.compile(loss='mse', optimizer=Adam(lr=params['learning_rate']))\n",
    "experiment_dqn(env, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe from the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "In order to smooth the learning procedure and remove the possible correlations in the observation sequence, we will use the so called experience replay technique.\n",
    "\n",
    "Shortly, experience replay stores the agent’s experience at each time step $e_t=(s_t,a_t,r_t,s_{t+1})$ in a replay memory that is accessed to perform the weight updates. As samples are drawn at random from the replay memory during Q-learning updates, one sample could be used multiple times. \n",
    "Experience replay improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution.\n",
    "\n",
    "Let us implement it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast with the classical action-value methods, policy gradients (PG) do not need to learn the values of actions in order to define a policy. Instead, PG learn directly a parameterized policy -- i.e, can select actions without consulting a value function -- pushing up the probabilities of actions that lead to higher return, and down the probabilities of actions that lead to lower return. PG are appropiate, for instance, when the state space of the problem at hand is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In policy gradient methods, the probability that action $a$ is taken at time $t$ given that the environment is in state $s$ at time $t$ with parameter $\\theta$ can be written then as:\n",
    "\n",
    "$$\\pi(a|s, \\theta) = Pr \\left\\{A_t = a | S_t = s, \\theta_t = \\theta \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy gradient algorithm works by updating policy parameters via stochastic gradient ascent on policy performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\widehat{\\nabla_{\\theta} J(\\pi_{\\theta_t})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\widehat{\\nabla J(\\pi_{\\theta_t})}$ is a stochastic estimate whose expectation approximates the gradient of the performance measure with respect to its argument $\\theta_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla J(\\pi_{\\theta_t}) = E_{\\tau \\sim \\pi_{\\theta}} \\left \\{ \\sum_{t=0}^T \\nabla_{\\theta} log \\pi_{\\theta} (a_t|s_t) A^{\\pi_{\\theta}} (s_t, a_t) \\right \\} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\tau$ is a trajectory and $A^{\\pi_{\\theta}}$ is the advantage function for the current policy:\n",
    "\n",
    "$$A^{\\pi} (s, a) = Q^{\\pi}(s, a) - V^{\\pi}(s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action preferences can be parameterized arbitrarily. For instance, they might be computed by an artificial neural network (ANN), where $\\theta$ is the vector of all the connection weights of the network. Let's see how it works using the Pong-v0 environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving Pong-v0 with VPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we just code a neural network that outputs a probability of moving up (define the policy), and takes as input simply the sequence of frames it gets from observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://blog.floydhub.com/content/images/2018/12/image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE: Implement a policy network consisting of a 3 fully connected layers. The first one, the input, has dimensions $80x80$. The hidden layer should have 200 units with relu activation. Initialize this layer using glorot uniform. The output consists of one neuron activated with a sigmoid function (take kernel_initializer='RandomNormal').\n",
    "\n",
    "Which loss function should be applied in this problem?\n",
    "\n",
    "Note 1: The 80 * 80 input dimension comes from a pre-processing step to filter important pixels (those which define the balls and the paddle) and to express things like the direction of the ball (computed from the difference between two consecutive frames).\n",
    "\n",
    "Note 2: We use a sigmoid activation in the output layer because it must predict a probability of choosing the action UP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and compile the network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the preprocessing step and the discounted rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# preprocessing used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "# reward discount used by Karpathy (cf. https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)\n",
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    discounted_r /= np.std(discounted_r) #idem\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialize the gym environment, the main variables and the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym initialization\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE: Implement and run the main loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Frameworks for keras: keras-rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find more info [here](https://github.com/keras-rl/keras-rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ks_dl_course]",
   "language": "python",
   "name": "conda-env-ks_dl_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
