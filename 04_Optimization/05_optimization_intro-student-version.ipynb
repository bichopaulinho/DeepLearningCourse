{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to optimization\n",
    "\n",
    "A [real loss landscape](https://www.cs.umd.edu/~tomg/projects/landscapes/) for an image classification problem with a 51 layer DNN (Left and right images correspond to the same DNN with and without architecture optimization, i.e. residual connections):\n",
    "\n",
    "<img src=\"https://www.cs.umd.edu/~tomg/img/landscapes/noshort.png\" alt=\"GD variants\" style=\"float:left; width: 300px\"/>\n",
    "\n",
    "<img src=\"https://www.cs.umd.edu/~tomg/img/landscapes/shortHighRes.png\" alt=\"GD variants\" style=\" width: 300px\"/>\n",
    "<br>\n",
    "\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. What do we want to solve? The objective function (20 min)\n",
    "2. So, how would you solve this (optimization) problem? A brief overview of methods (30-40 min)\n",
    "3. Simple Gradient descend implementation for one-dimensional inputs (10 min)<br> \n",
    "    3.1 Adapting the gradient size (20 min)\n",
    "4. From derivatives to gradient: $n$-dimensional function minimization. (30-40 min)<br> \n",
    "    4.1 The learning rate (10 min)<br>\n",
    "    \n",
    "Break\n",
    "\n",
    "\n",
    "5. Learning from data (10-15 min)<br> \n",
    "    5.1 Batch gradient descent (25 min)<br> \n",
    "    5.2 Stochastic gradient descent (25 min)<br> \n",
    "    5.3 Example: Linear regresion with SGD (20-30 min)<br> \n",
    "    5.4 Mini-batch gradient descent (10-20 min)<br> \n",
    "    5.5 Advanced gradient descent (30 min)<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "from scipy import stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What do we want to solve? The objective function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the most simple case, a linear regression. Data is generated through the (unknown, if we were given just the data) process:\n",
    "\n",
    "$\\displaystyle y = 3*x + 2 + {\\rm noise}$\n",
    "\n",
    "Our network will be:\n",
    "\n",
    "$\\displaystyle \\hat y = w \\cdot x + b$\n",
    "\n",
    "Finally, we want to minimize:\n",
    "\n",
    "$\\mathcal{L} = \\displaystyle \\sum_{\\rm (x,y)} (y-\\hat y)^2 = \\sum_{\\rm (x,y)} (y- w \\cdot x - b)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "We want to find the values of w and b that make $\\mathcal{L}$ (the cost function) minumum. Generally speaking, what we are doing is to learn a model from data, by optimizing the set of parameters that solve the problem.\n",
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "Recall that we do not know how the output depends on the input!! In other words, we don't know y=f(x), that's precisely what we want to find out.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out what the cost function $\\mathcal{L}(w,b)$ looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "x_train = np.linspace(0,100,1000)\n",
    "y_train = 3*x_train + 2\n",
    "\n",
    "x_test = np.linspace(100.01, 110.01, 50)\n",
    "y_test = 3*x_test+2\n",
    "plt.plot(x_train, y_train, 'r')\n",
    "plt.plot(x_test, y_test, 'g')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the loss for fixed x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random pair (x0, y0)\n",
    "index = \n",
    "x0 = \n",
    "y0 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss \n",
    "def loss(x, y, w, b):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give values to the parameters w and b\n",
    "w = \n",
    "b = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot w and b\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.plot(w, loss(x0, y0, w, 2), label='weight');\n",
    "plt.plot(b, loss(x0, y0, 2, b), label='bias');\n",
    "plt.legend(loc=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mesh grid, to see how the loss varies as function of w and b\n",
    "X,Y = np.meshgrid(w, b)\n",
    "Z = loss(x_train[index], y_train[index], X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "\n",
    "p = ax.pcolor(X, Y, Z, cmap=plt.cm.RdBu, vmin=abs(Z).min(), vmax=abs(Z).max())\n",
    "cb = fig.colorbar(p)\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "# p = ax.plot_surface(X, Y, Z, rstride=4, cstride=4, linewidth=0)\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, linewidth=0,  cmap='viridis')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. So, how would you solve this (optimization) problem? \n",
    "## A brief overview of methods, before  we get our hands dirty\n",
    "\n",
    "The mathematical discipline that studies this kind of problems is called [Automatic Differentiation (AD)](https://en.wikipedia.org/wiki/Automatic_differentiation), and the [Back-propagation algorithm](https://en.wikipedia.org/wiki/Backpropagation) used to optimize Artificial Neural Networks is a special case of AD (we'll get back to this later). \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6e/Grafico_3d_x2%2Bxy%2By2.png\" alt=\"convex problem\" style=\"float: center; width: 300px\"/>\n",
    "\n",
    "\n",
    "The optimization problems are divided in two main parts: linear and [non-linear programming](https://en.wikipedia.org/wiki/Nonlinear_programming), depending on whether the constraints defining the objective function and its parameters are linear or not. Among nonlinear problems, the [convex ones](https://en.wikipedia.org/wiki/Convex_optimization) are the only ones who have guaranteed a solution, because they have a global minima (a convex function is shown in the figure below; any segment between two points of the functions lies aboove or on the graph itself). The linear regression example we saw earlier on this notebook is an example of convex optimization (a quite simple one). \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/ConvexFunction.svg\" alt=\"convex function\" style=\"width: 450px\"/>\n",
    "\n",
    "\n",
    "**Unfortunately, most problems of interest are not convex and, therefore, they present multiple local minima**. Even a convex problem can become non-convex in the presence of noise, see the figures below!\n",
    "\n",
    "<img src=\"http://www.turingfinance.com/wp-content/uploads/2015/07/Quadratic-Function.png\" alt=\"convex problem\" style=\"float: left; width: 300px\"/>\n",
    "\n",
    "<img src=\"http://www.turingfinance.com/wp-content/uploads/2014/09/Noisy-Quadratic-Surface.png\" alt=\"non convex problem\" style=\"width: 300px\"/>\n",
    "\n",
    "<br>\n",
    "But, as Yann LeCun put it, [\"Who is afraid of non-convex functions?\"](https://cs.nyu.edu/~yann/talks/lecun-20071207-nonconvex.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Even the simplest NN can constitute a non-convex optimization problem, because of the non-linearity of the activations, the objective function or the data itself! Thus, we don't have any guarantee that a solution exist. And this is why training NN is so difficult (no matter what they tell you ;) )\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AD is solved by means of **iterative methods** that approximate the minimum or maximum of an objective function in a multidimensional space. There are many iterative methods, being some of the most well known:\n",
    "\n",
    "* [Nelder-Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method): kind of brute force searching mechanism. It has severe problems as the number of dimensions of the problem increases.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e4/Nelder-Mead_Rosenbrock.gif\" alt=\"convex problem\" style=\"float: center; width: 300px\"/>\n",
    "\n",
    "* [Newthon method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization): very accurate, but it requires information on the second derivative (the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix), that describes the local curvature of the surface). It's very expensive to calculate, though!\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/da/Newton_optimization_vs_grad_descent.svg\" alt=\"newton vs GD\" style=\"float: center; width: 300px\"/>\n",
    "\n",
    "\n",
    "* [Quasi-netownian methods](https://en.wikipedia.org/wiki/Quasi-Newton_method): since calculating 2nd order derivatives (the Hessian) is expensive, these family of methods try to approximate the Hessian in different forms =) The most well known algorithms are [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) and [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS). They work pretty well, but they have severe memory problems and  do not scale to large datasets (there are some studies trying to incorporate batches).\n",
    "\n",
    "\n",
    "* [Gradient-descent](https://en.wikipedia.org/wiki/Gradient_descent) Ok, forget about the second derivative! Just use the first-order approximation ;) It's slower than previous methods, but it works for any number of variables/dimensions. \n",
    "\n",
    "<img src=\"https://blog.paperspace.com/content/images/2018/06/firstorder.png\" alt=\"stupid GD\" style=\"float: center; width: 600px\"/>\n",
    "\n",
    "\n",
    "...[Subgradient methods](https://en.wikipedia.org/wiki/Subgradient_method)\n",
    "\n",
    "\n",
    "Other resources:\n",
    "\n",
    "http://www.benfrederickson.com/numerical-optimization/\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "\n",
    "https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0\n",
    "\n",
    "https://github.com/DeepLearningUB/DeepLearningMaster/blob/master/1.%20Basic%20Concepts.ipynb\n",
    "\n",
    "Other methods, in case you are interested:\n",
    "\n",
    "* [Simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing): a Monte Carlo like search, in which the state of minimum \"energy\" (a.k.a. cost) is find by performing probabilistic jumps between states.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/10/Travelling_salesman_problem_solved_with_simulated_annealing.gif\" alt=\"stupid GD\" style=\"float: center; width: 600px\"/>\n",
    "\n",
    "\n",
    "* [Genetic algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm): monte carlo search, inspired by the mechanism of natural selection observed in Nature (genetic errors, marriage of individuals, and so on). It has regained attention recently in the AI community, after [OpenAI](https://openai.com/) showed its success in solving  [Reinforcement Learning problems](https://blog.openai.com/evolution-strategies/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "In the following we will try to understand gradient descent and variants of it, but you should keep in mind the problems and assumptions we have highligthed!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple Gradient descend implementation\n",
    "\n",
    "Some parts of the following code are taken from [the UB Master in DL](https://github.com/DeepLearningUB/DeepLearningMaster/blob/master/1.%20Basic%20Concepts.ipynb).\n",
    "\n",
    "Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.Gradient descent is used to find the minimum error by minimizing a “cost” function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/w7ARo.png\" alt=\"down the hill\" style=\"float: right; width: 700px\"/>\n",
    "\n",
    "\n",
    "To find the local minimum using **gradient descend**: you start at a random point, and move into the direction of steepest **descent** relative to the derivative:\n",
    "\n",
    "+ Start from a random $x$ value.\n",
    "+ Compute the derivative $f'(x)$ analitically.\n",
    "+ Walk a small step in the **opposite** direction of the derivative. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, let's consider the function $f(x) = x^2-6x+5$. The minimum occurs at $x=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,5,100)\n",
    "f = x**2-6*x+5\n",
    "plt.plot(x, f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the steps above, suppose we start at $x=15$. The derivative at this point is $2×15−6=24$. \n",
    "\n",
    "Because we're using gradient descent, we need to subtract the gradient from our $x$-coordinate: the new position is $x1 =x - f'(x) = 15-24 = -9$. However, notice that $15−24$ gives us $−9$, clearly overshooting over target of $3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "start = 15\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(x,y, 'r-')\n",
    "plt.plot([start],[start**2 - 6*start + 5],'o')\n",
    "ax.text(start,\n",
    "        start**2 - 6*start + 35,\n",
    "        'Start',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "\n",
    "d = 2 * start - 6\n",
    "end = start - d\n",
    "\n",
    "plt.plot([end],[end**2 - 6*end + 5],'o')\n",
    "plt.ylim([-10,250])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "ax.text(end,\n",
    "        start**2 - 6*start + 35,\n",
    "        'End',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['green'],\n",
    "       )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Adapting the gradient size\n",
    "\n",
    "To fix this, we multiply the gradient by a step size. This step size (often called **alpha**) has to be chosen carefully, as a value too small will result in a long computation time, while a value too large will not give you the right result (by overshooting) or even fail to converge. \n",
    "\n",
    "In this example, we'll set the step size to 0.01, which means we'll subtract $24×0.01$ from $15$, which is $14.76$. \n",
    "\n",
    "This is now our new temporary local minimum: We continue this method until we either don't see a change after we subtracted the derivative step size, or until we've completed a pre-set number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_min = 0  # old minima\n",
    "temp_min = 15  # tempora minima\n",
    "step_size = 0.01  # size of the steps we make i nthe direction of the gradient\n",
    "precision = 0.0001  # a tolerance criteria for stopping the iterative process\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 6*x + 5\n",
    "    \n",
    "def f_derivative(x):\n",
    "    return \n",
    "\n",
    "mins = []\n",
    "cost = []\n",
    "\n",
    "while abs(temp_min - old_min) > precision:\n",
    "    old_min = temp_min \n",
    "    gradient =  # calculate the derivative\n",
    "    move =  # make a step in the direction of the gradient\n",
    "    temp_min =  # new minima\n",
    "    cost.append((3-temp_min)**2) # just the squared differnece with the known minima (3.0, in this example)\n",
    "    mins.append(temp_min)\n",
    "\n",
    "# rounding the result to 2 digits because of the step size\n",
    "print(\"Local minimum occurs at {:3.6f}.\".format(round(temp_min,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important feature of gradient descent is that **there should be a visible improvement over time**: In this example, we simply plotted the squared distance from the local minima calculated by gradient descent and the true local minimum,  ``cost``, against the iteration during which it was calculated. As we can see, the distance gets smaller over time, but barely changes in later iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "\n",
    "x, y = (zip(*enumerate(cost)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(x,y, 'r-', alpha=0.7)\n",
    "plt.ylim([-10,150])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show();\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.semilogy(x,y, 'r-', alpha=0.7)\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.ylabel('Cost (log scale)')\n",
    "plt.xlabel('Iterations')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,20,100)\n",
    "y = x**2 - 6*x + 5\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(x,y, 'r-')\n",
    "plt.ylim([-10,250])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.plot(mins,cost,'o', alpha=0.3)\n",
    "ax.text(start,\n",
    "        start**2 - 6*start + 25,\n",
    "        'Start',\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "ax.text(mins[-1],\n",
    "        cost[-1]+20,\n",
    "        'End (%s steps)' % len(mins),\n",
    "        ha='center',\n",
    "        color=sns.xkcd_rgb['blue'],\n",
    "       )\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 From derivatives to gradient: $n$-dimensional function minimization.\n",
    "\n",
    "Let's consider a $n$-dimensional function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$. Note this is actually a quite common case: it can be, for instance, the mapping from a image to a label \"dog\" or \"no dog\"; or the result of a sentiment analysis, that maps a sentence to a \"positve\" or \"negative\" sentiment. \n",
    "\n",
    "As a simple exmaple, we shall take:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sum_{n} x_n^2$$\n",
    "\n",
    "Our objective is to find the argument  $\\mathbf{x}$ that minimizes this function.\n",
    "\n",
    "The **gradient** of $f$ is the vector whose components are the $n$ partial derivatives of $f$. It **is thus a vector-valued function**. \n",
    "\n",
    "The gradient points in the direction of the greatest rate of **increase** of the function.\n",
    "\n",
    "$$\\nabla {f} = (\\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement \\sum x_i^2\n",
    "def f(x):\n",
    "    return \n",
    "\n",
    "def fin_dif_partial_centered(x, \n",
    "                             f, \n",
    "                             i, \n",
    "                             h=1e-6):\n",
    "    '''\n",
    "    This method returns the partial derivative of the i-th component of f at x\n",
    "    by using the centered finite difference method\n",
    "    \n",
    "    (f(x+h) - f(x-h))/2h\n",
    "    '''\n",
    "    w1 =  # vector at x+h\n",
    "    w2 =  # vector at x-h\n",
    "    diff = # difference of the function at those points\n",
    "    return diff/(2*h)\n",
    "\n",
    "def gradient_centered(x, \n",
    "                      f, \n",
    "                      h=1e-6):\n",
    "    '''\n",
    "    This method returns the gradient vector of f at x\n",
    "    by using the centered finite difference method\n",
    "    '''\n",
    "    return[round(fin_dif_partial_centered(x,f,i,h), 10) for i,_ in enumerate(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we have evaluated, $f({\\mathbf x}) = x_1^2+x_2^2+x_3^2$, is $3$ at $(1,1,1)$ and the gradient vector at this point is $(2,2,2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.0,1.0,1.0]\n",
    "\n",
    "print('{:.6f}'.format(f(x)), gradient_centered(x,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can follow this steps to maximize (or minimize) the function:\n",
    "\n",
    "+ Start from a random $\\mathbf{x}$ vector.\n",
    "+ Compute the gradient vector.\n",
    "+ Walk a small step in the opposite direction of the gradient vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is important to be aware that this gradient computation is very expensive: if $\\mathbf{x}$ has dimension $n$, we have to evaluate $f$ at $2*n$ points.\n",
    "\n",
    "\n",
    "### How to use the gradient.\n",
    "\n",
    "$f(x) = \\sum_i x_i^2$, takes its mimimum value when all $x$ are 0. \n",
    "\n",
    "Let's check it for $n=3$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this funtion to evaluate the distance between two points.\n",
    "def euc_dist(v1,v2):\n",
    "    v = np.array(v1)-np.array(v2)\n",
    "    return math.sqrt(sum(v_i ** 2 for v_i in v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by choosing a random vector and then walking a step in the opposite direction of the gradient vector. We will stop when the difference between the new solution and the old solution is less than a tolerance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing a random vector\n",
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x,\n",
    "         grad,\n",
    "         alpha):\n",
    "    '''\n",
    "    This function makes a step in the opposite direction of the gradient vector \n",
    "    in order to compute a new value for the target function.\n",
    "    '''\n",
    "    return \n",
    "\n",
    "tol = 1e-15\n",
    "alpha = 0.01\n",
    "k = 0 # to count the number of steps\n",
    "old_x = x\n",
    "while True:\n",
    "    k += 1\n",
    "    grad = gradient_centered(old_x,f)\n",
    "    next_x = step(old_x,grad,alpha)\n",
    "    if euc_dist(next_x, old_x) < tol:\n",
    "        break\n",
    "    old_x = next_x\n",
    "print(\"Found a minimum at \", [round(i,10) for i in next_x], \" in %s steps\" %k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 The learning rate alpha\n",
    "\n",
    "The step size, **alpha**, is a slippy concept: if it is too small we will slowly converge to the solution, if it is too large we can diverge from the solution. \n",
    "\n",
    "There are several policies to follow when selecting the step size:\n",
    "\n",
    "+ Constant size steps. In this case, the size step determines the precision of the solution.\n",
    "+ Decreasing step sizes.\n",
    "+ At each step, select the optimal step.\n",
    "\n",
    "The last policy is good, but too expensive. Try, for instance, dividing the step size by 2 at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1e-15\n",
    "alpha = 0.01\n",
    "k = 0 # to count the number of steps\n",
    "old_x = x\n",
    "while True:\n",
    "    k += 1\n",
    "    grad = gradient_centered(old_x,f)\n",
    "    next_x = step(old_x,grad,alpha)\n",
    "    if euc_dist(next_x, old_x) < tol:\n",
    "        break\n",
    "    old_x = next_x\n",
    "    alpha /= 2.0\n",
    "print(\"Found a minimum at \", [round(i,10) for i in next_x], \" in %s steps.\" %k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, take a look [here](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Learning from data\n",
    "\n",
    "In general, we have:\n",
    "\n",
    "+ A dataset $(\\mathbf{x},y)$ of $n$ examples. \n",
    "+ An objective function $J_{\\boldsymbol \\theta}$, that we want to minimize, representing the **discrepancy between our data and the model** we want to fit. The model is represented by a set of parameters ${\\boldsymbol \\theta}$. \n",
    "+ The gradient of the target function, $g_f$. \n",
    "\n",
    "\n",
    "In the most common case $f$ represents the errors from a data representation model $M$. To fit the model is to find the optimal parameters $\\boldsymbol \\theta$ that minimize the following expression:\n",
    "\n",
    "$$ J_{\\boldsymbol \\theta} = \\frac{1}{n} \\sum_{i} (y_i - M(\\mathbf{x}_i,{\\boldsymbol \\theta}))^2 $$\n",
    "\n",
    "For example, $(\\mathbf{x},y)$ can represent:\n",
    "\n",
    "+ $\\mathbf{x}$: the behavior of a \"Candy Crush\" player; $y$: monthly payments. \n",
    "+ $\\mathbf{x}$: sensor data about your car engine; $y$: probability of engine error.\n",
    "+ $\\mathbf{x}$: finantial data of a bank customer; $y$: customer rating.\n",
    "\n",
    "> If $y$ is a real value, it is called a *regression* problem.\n",
    "\n",
    "> If $y$ is binary/categorical, it is called a *classification* problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Batch gradient descend\n",
    "\n",
    "Let's suppose that our model is a one-dimensional linear model $M(\\mathbf{x},{\\boldsymbol \\theta}) = w \\cdot x $. \n",
    "\n",
    "**Batch gradient descend** consists of calculating the gradients for all the points, averaging, and updating the parameters accordingly.\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_j}=\\frac{2}{n} \\sum_{i} \\frac{\\partial M}{\\partial \\theta_j} (M-y_i) $$\n",
    "\n",
    "$$\\theta_j \\leftarrow \\theta_j - \\alpha\\cdot\\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "\n",
    "$${\\boldsymbol \\theta} \\leftarrow {\\boldsymbol\\theta} - \\alpha\\nabla J_{\\boldsymbol\\theta}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# f = 2x\n",
    "x = np.arange(10)\n",
    "y = \n",
    "\n",
    "# f_target = 1/n Sum (y - wx)**2\n",
    "def target_f(x,y,w):\n",
    "    return \n",
    "\n",
    "# gradient respect to w: dJ_dw = 2/n Sum (wx-y)*x\n",
    "def gradient_f(x,y,w):\n",
    "    return \n",
    "\n",
    "def step(w,grad,alpha):\n",
    "    # one dimensional updates\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        y, \n",
    "        toler = 1e-6, \n",
    "        alpha=0.01):\n",
    "    '''\n",
    "    Batch gradient descend by using a given step\n",
    "    '''\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.6f}'.format(BGD(target_f, gradient_f, x, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that selecting the step size is tricky\n",
    "for alpha in [1e-4, 1e-3, 1e-2, 1.2e-2, 1.5e-2, 1.7e-2, 1.75e-2]:\n",
    "    %%timeit BGD(target_f, gradient_f, x, y, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use a multi-step approach for the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantages of BGD are:\n",
    "\n",
    "* We can use fixed step size.\n",
    "* It has a straight trajectory towards the minimum and it is guaranteed to converge in theory to the global minimum if the loss function is convex and to a local minimum if the loss function is not convex. In other words, we are using the tru gradient. \n",
    "* It has unbiased estimate of gradients. The more the examples, the lower the standard error.\n",
    "\n",
    "The main disadvantages:\n",
    "\n",
    "* Even though we can use vectorized implementation, it may still be slow to go over all examples especially when we have large datasets.\n",
    "* Each step of learning happens after going over all examples where some examples may be redundant and don’t contribute much to the update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Note that the number of data pairs $({\\bf x}, y)$ can be very large (millions, billions, etc.). Thus, **the calculation the cost $J_{\\bf w}$ for all points can be very costly, and sometimes, unfeasible for memory reasons!** Different implementations of BGD are required, hence. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Stochastic Gradient Descend\n",
    "\n",
    "The last function evals the whole dataset $(\\mathbf{x}_i,y_i)$ at every step. If the dataset is large, this methodology is way too costly. Instead of that, we will use a strategy called **SGD** (*Stochastic Gradient Descend*): the idea is to perform parameter updates at each point.\n",
    "\n",
    "When learning from data, the cost function is additive: it is computed by adding sample reconstruction errors. Therefore, we can compute an estimate of the gradient (and move towards the minimum) by using only **one data sample** (or a small data sample). We will find the minimum by iterating this gradient estimation over the dataset.\n",
    "\n",
    "A full iteration over the dataset is called **epoch**. Before each epoch, data must be randomly shuffled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "So, if we are not using the true gradient, but rather a noisy version, can we find the minimum?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://ruder.io/content/images/2016/09/sgd_fluctuation.png\">\n",
    "<center>(Fluctuations in the loss with SGD, from Wikipedia)</center>\n",
    "\n",
    "If we apply this method we have some theoretical guarantees to find a good minimum:\n",
    "+ SGD essentially uses the inaccurate gradient per iteration. Since there is no free lunch, what is the cost by using approximate gradient? The answer is that the convergence rate is slower than the gradient descent algorithm.\n",
    "+ The convergence of SGD has been analyzed using the theories of convex minimization and of stochastic approximation: it converges almost surely to a global minimum when the objective function is convex or pseudoconvex, and otherwise converges almost surely to a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "There is an unexpected advantage when using SGD in large dimensional data: the stochastic update of parameters prevents the optimization from ending up in saddle points!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = np.array([2*i for i in x])\n",
    "data = np.vstack((x, y)).T\n",
    "\n",
    "for xi in data:\n",
    "    print('{:3d} {:3d}'.format(xi[0], xi[1]))\n",
    "print()\n",
    "\n",
    "def in_random_order(data):\n",
    "    '''\n",
    "    Random data generator\n",
    "    '''\n",
    "    d = data.copy()\n",
    "    np.random.shuffle(d)\n",
    "    return d\n",
    "\n",
    "for xi in in_random_order(data):\n",
    "    print('{:3d} {:3d}'.format(xi[0], xi[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        y, \n",
    "        toler = 1e-6, \n",
    "        epochs=100, \n",
    "        alpha=0.01):\n",
    "    '''\n",
    "    Stochastic gradient descend\n",
    "    '''\n",
    "    # stack the data\n",
    "    data = np.vstack((x, y)).T\n",
    "    # random initialization of the weight\n",
    "    w = random.random()\n",
    "    \n",
    "    min_w, min_val = float('inf'), float('inf')\n",
    "    epoch = 0\n",
    "    while epoch < epochs:\n",
    "        val = target_f(x, y, w)\n",
    "        if min_val - val > toler:\n",
    "            min_w, min_val = w, val\n",
    "        \n",
    "        for xi in in_random_order(data):\n",
    "            gradient_i = gradient_f(xi[0], xi[1], w)\n",
    "            w = w - (alpha *  gradient_i)\n",
    "            \n",
    "        epoch += 1\n",
    "    return min_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "y = np.array([3*i for i in x])\n",
    "\n",
    "print('w: {:.6f}'.format(SGD(target_f, gradient_f, x, y, epochs=100, alpha=1e-2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Play with different values. Try increasing the number of points in x. Do you need to change the number of epochs? And the step size alpha?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Exercise: Stochastic Gradient Descent and Linear Regression\n",
    "\n",
    "The linear regression model assumes a linear relationship between data:\n",
    "\n",
    "$$ y_i = w_1 x_i + w_0 $$\n",
    "\n",
    "Let's generate a more realistic dataset (with noise), where $w_1 = 2$ and $w_0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "from scipy import stats \n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: input data\n",
    "# y: noisy output data\n",
    "\n",
    "x = np.random.uniform(0,1,20)\n",
    "\n",
    "# f = 2x + 0\n",
    "def f(x): return 2*x + 0\n",
    "\n",
    "noise_variance =0.1\n",
    "noise = np.random.randn(x.shape[0])*noise_variance\n",
    "y = f(x) + noise\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.xlabel('$x$', fontsize=15)\n",
    "plt.ylabel('$f(x)$', fontsize=15)\n",
    "plt.plot(x, y, 'o', label='y')\n",
    "plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)')\n",
    "plt.ylim([0,2])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code in order to:\n",
    "+ Compute the value of $w$ by using a estimator based on minimizing the squared error.\n",
    "+ Get from SGD function a vector, `target_value`, representing the value of the target function at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your target function as f_target 1/n Sum (y - wx)**2\n",
    "def target_f(x,y,w):\n",
    "    # your code here\n",
    "    return\n",
    "\n",
    "# Write your gradient function\n",
    "def gradient_f(x,y,w):\n",
    "    # your code here\n",
    "    return\n",
    "\n",
    "def in_random_order(data):\n",
    "    '''\n",
    "    Random data generator\n",
    "    '''\n",
    "    import random\n",
    "    indexes = [i for i,_ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "\n",
    "# Modify the SGD function to return a 'target_value' vector\n",
    "def SGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        y, \n",
    "        toler = 1e-6, \n",
    "        epochs=100, \n",
    "        alpha_0=0.01):\n",
    "    \n",
    "    # Insert your code among the following lines\n",
    "    target_value = []\n",
    "    \n",
    "    data = list(zip(x,y))\n",
    "    w = random.random()\n",
    "    alpha = alpha_0\n",
    "    min_w, min_val = float('inf'), float('inf')\n",
    "    iteration_no_increase = 0\n",
    "    epoch = 0\n",
    "    while epoch < epochs and iteration_no_increase < 100:\n",
    "        val = target_f(x, y, w)\n",
    "        if min_val - val > toler:\n",
    "            min_w, min_val = w, val\n",
    "            alpha = alpha_0\n",
    "            iteration_no_increase = 0\n",
    "        else:\n",
    "            iteration_no_increase += 1\n",
    "            alpha *= 0.95\n",
    "        \n",
    "        target_value.append(val)\n",
    "        epoch += 1\n",
    "    return min_w, target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value of the solution\n",
    "w, target_value = SGD(target_f, gradient_f, x, y, epochs=100, alpha_0=1e-2)\n",
    "print('w: {:.6f}'.format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the solution regression line\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(x, y, 'o', label='t')\n",
    "plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)', alpha=0.5)\n",
    "plt.plot([0, 1], [0*w, 1*w], 'r-', label='fitted line', alpha=0.5, linestyle='--')\n",
    "plt.xlabel('input x')\n",
    "plt.ylabel('target t')\n",
    "plt.title('input vs. target')\n",
    "plt.ylim([0,2])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the evolution of the target function value during iterations.\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(np.arange(len(target_value)), target_value, 'o', alpha = 0.2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid()\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code, general **batch gradient descent** looks something like this:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "    grad = evaluate_gradient(target_f, data, w)\n",
    "    w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "For a pre-defined number of epochs, we first compute the gradient vector of the target function for the whole dataset w.r.t. our parameter vector. \n",
    "\n",
    "**Stochastic gradient descent** (SGD) in contrast performs a parameter update for each training example and label:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for sample in data:\n",
    "        grad = evaluate_gradient(target_f, sample, w)\n",
    "        w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "**Mini-batch gradient descent** finally takes the best of both worlds and performs an update for every mini-batch of $n$ training examples:\n",
    "\n",
    "```python\n",
    "nb_epochs = 100\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for batch in get_batches(data, batch_size=50):\n",
    "    grad = evaluate_gradient(target_f, batch, w)\n",
    "    w = w - learning_rate * grad\n",
    "```\n",
    "\n",
    "Minibatch SGD has the advantage that it works with a slightly less noisy estimate of the gradient. However, as the minibatch size increases, the number of updates done per computation done decreases (eventually it becomes very inefficient, like batch gradient descent). \n",
    "\n",
    "There is an optimal trade-off (in terms of computational efficiency) that may vary depending on the data distribution and the particulars of the class of function considered, as well as how computations are implemented.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*PV-fcUsNlD9EgTIc61h-Ig.png\" alt=\"GD variants\" style=\"float: center; width: 700px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(iterable, \n",
    "                num_elem_batch = 1):\n",
    "    '''\n",
    "    Generator of batches from an iterable that contains data\n",
    "    '''\n",
    "    current_batch = []\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == num_elem_batch:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "x = np.array(range(0, 10))\n",
    "y = np.array(range(10, 20))\n",
    "data = list(zip(x,y))\n",
    "np.random.shuffle(data)\n",
    "for x in get_batches(data, 3):\n",
    "    print(x)\n",
    "\n",
    "print\n",
    "\n",
    "for batch in get_batches(data, 3):\n",
    "    print(np.array(list(zip(*batch))[0]), np.array(list(zip(*batch))[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "from scipy import stats \n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: input data\n",
    "# y: noisy output data\n",
    "\n",
    "x = np.random.uniform(0,1,2000)\n",
    "\n",
    "# f = 2x + 0\n",
    "def f(x): return 2*x + 0\n",
    "\n",
    "noise_variance =0.1\n",
    "noise = np.random.randn(x.shape[0])*noise_variance\n",
    "y = f(x) + noise\n",
    "\n",
    "plt.plot(x, y, 'o', label='y')\n",
    "plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)')\n",
    "plt.xlabel('$x$', fontsize=15)\n",
    "plt.ylabel('$t$', fontsize=15)\n",
    "plt.ylim([0,2])\n",
    "plt.title('inputs (x) vs targets (y)')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_target = 1/n Sum (y - wx)**2\n",
    "def target_f(x,\n",
    "             y,\n",
    "             w):\n",
    "    return np.sum((y - x * w)**2.0) / x.size\n",
    "\n",
    "# gradient_f = 2/n Sum 2wx**2 - 2xy\n",
    "def gradient_f(x,\n",
    "               y,\n",
    "               w):\n",
    "    return 2 * np.sum(2*w*(x**2) - 2*x*y) / x.size\n",
    "\n",
    "def in_random_order(data):\n",
    "    '''\n",
    "    Random data generator\n",
    "    '''\n",
    "    import random\n",
    "    indexes = [i for i,_ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "        \n",
    "def get_batches(iterable, \n",
    "                num_elem_batch = 1):\n",
    "    '''\n",
    "    Generator of batches from an iterable that contains data\n",
    "    '''\n",
    "    current_batch = []\n",
    "    for item in iterable:\n",
    "        current_batch.append(item)\n",
    "        if len(current_batch) == num_elem_batch:\n",
    "            yield current_batch\n",
    "            current_batch = []\n",
    "    if current_batch:\n",
    "        yield current_batch\n",
    "\n",
    "def SGD_MB(target_f, gradient_f, x, y, epochs=100, alpha_0=0.01):\n",
    "    data = list(zip(x,y))\n",
    "    w = random.random()\n",
    "    alpha = alpha_0\n",
    "    min_w, min_val = float('inf'), float('inf')\n",
    "    epoch = 0\n",
    "    while epoch < epochs:\n",
    "        val = target_f(x, y, w)\n",
    "        if val < min_val:\n",
    "            min_w, min_val = w, val\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            alpha *= 0.9\n",
    "        np.random.shuffle(data)\n",
    "        for batch in get_batches(data, num_elem_batch = 100):\n",
    "            x_batch = np.array(list(zip(*batch))[0])\n",
    "            y_batch = np.array(list(zip(*batch))[1])\n",
    "            gradient = gradient_f(x_batch, y_batch, w)\n",
    "            w = w - (alpha *  gradient)\n",
    "        epoch += 1\n",
    "    return min_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = SGD_MB(target_f, gradient_f, x, y)\n",
    "print('w: {:.6f}'.format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o', label='t')\n",
    "plt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)', alpha=0.5)\n",
    "plt.plot([0, 1], [0*w, 1*w], 'r-', label='fitted line', alpha=0.5, linestyle='--')\n",
    "plt.xlabel('input x')\n",
    "plt.ylabel('target t')\n",
    "plt.ylim([0,2])\n",
    "plt.title('input vs. target')\n",
    "plt.grid()\n",
    "plt.legend(loc=2)\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced gradient descend\n",
    "\n",
    "There are a number of problems faced by the vanilla mini-batch GD, as explained by Sebastian Ruder in this [excellent post](http://ruder.io/optimizing-gradient-descent/):\n",
    "\n",
    "* **Choosing a proper learning rate can be difficult**. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.\n",
    "\n",
    "* Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics.\n",
    "\n",
    "* Additionally, **the same learning rate applies to all parameter updates**. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.\n",
    "\n",
    "* Another **key challenge** of minimizing highly non-convex error functions common for neural networks is **avoiding getting trapped in their numerous suboptimal local minima**. Dauphin et al. [19] argue that the difficulty arises in fact not from local minima but from **saddle points**, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\n",
    "\n",
    "Related to this last point, we have: \n",
    "\n",
    "* **Ill-conditioning of the Hessian matrix**: regions where the curvature around one variable is much larger than in other directions (like in a ravine).\n",
    "\n",
    "* **Plateaus**, or flat regions, where there is no gradient and therefore the optimizer gets trapped.\n",
    "\n",
    "The following methods try to prevent some of these problems.\n",
    "\n",
    "\n",
    "<img src=\"http://2.bp.blogspot.com/-q6l20Vs4P_w/VPmIC7sEhnI/AAAAAAAACC4/g3UOUX2r_yA/s1600/s25RsOr%2B-%2BImgur.gif\">\n",
    "\n",
    "(Image credit: Alec Radford) \n",
    "\n",
    "<img src=\"https://i.imgur.com/2dKCQHh.gif?1\">\n",
    "\n",
    "(Image credit: Alec Radford) \n",
    "\n",
    "\n",
    "### Momentum: remember what you have already done\n",
    "\n",
    "SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.\n",
    "\n",
    "<img src=\"http://ruder.io/content/images/2015/12/without_momentum.gif\">\n",
    "<img src=\"http://ruder.io/content/images/2015/12/with_momentum.gif\">\n",
    "\n",
    "Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction of the update vector of the past time step to the current update vector:\n",
    "\n",
    "$$ v_t = m v_{t-1} + \\alpha \\nabla_w f $$\n",
    "$$ w = w - v_t    $$\n",
    "\n",
    "The momentum $m$ is commonly set to $0.9$.\n",
    "\n",
    "\n",
    "### Adagrad: a learning rate for each parameter\n",
    "\n",
    "All previous approaches manipulated the learning rate globally and equally for all parameters. Tuning the learning rates is an expensive process, so much work has gone into devising methods that can adaptively tune the learning rates, and even do so per parameter. \n",
    "\n",
    "Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.\n",
    "\n",
    "$$ c = c + (\\nabla_w f)^2 $$\n",
    "$$ w = w - \\frac{\\alpha}{\\sqrt{c}}\\nabla_w f $$ \n",
    "\n",
    "\n",
    "### RMProp\n",
    "\n",
    "RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in [Lecture 6e of his Coursera Class](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf).\n",
    "\n",
    "RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. In particular, it uses a moving average of squared gradients instead, giving:\n",
    "\n",
    "$$ c = \\beta c + (1 - \\beta)(\\nabla_w f)^2 $$\n",
    "$$ w = w - \\frac{\\alpha}{\\sqrt{c}}\\nabla_w f $$ \n",
    "\n",
    "where $\\beta$ is a decay rate that controls the size of the moving average (typically, $\\beta=0.9$..\n",
    "\n",
    "Take a look at [this video](https://www.coursera.org/lecture/deep-neural-network/rmsprop-BhJlm) by Andrew NG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "from scipy import stats \n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function that I'm going to plot\n",
    "def f(x,y):\n",
    "    return x**2 + 5*y**2\n",
    "\n",
    "x = np.arange(-3.0,3.0,0.1)\n",
    "y = np.arange(-3.0,3.0,0.1)\n",
    "X,Y = np.meshgrid(x, y, indexing='ij') # grid of point\n",
    "Z = f(X, Y) # evaluation of the function on the grid\n",
    "\n",
    "plt.pcolor(X, Y, Z, cmap=plt.cm.gist_earth)\n",
    "plt.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.gcf().set_size_inches((6,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_f(x):\n",
    "    return x[0]**2.0 + 5*x[1]**2.0\n",
    "\n",
    "def part_f(x, \n",
    "           f, \n",
    "           i, \n",
    "           h=1e-6):\n",
    "    w1 = [x_j + (h if j==i else 0) for j, x_j in enumerate(x)]\n",
    "    w2 = [x_j - (h if j==i else 0) for j, x_j in enumerate(x)]\n",
    "    return (f(w1) - f(w2))/(2*h)\n",
    "\n",
    "def gradient_f(x, \n",
    "               f, \n",
    "               h=1e-6):\n",
    "    return np.array([round(part_f(x,f,i,h), 10) for i,_ in enumerate(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        alpha_0=0.01,\n",
    "        toler = 0.000001):\n",
    "    alpha = alpha_0\n",
    "    min_val = float('inf')\n",
    "    steps = 0\n",
    "    iteration_no_increase = 0\n",
    "    trace = []\n",
    "    while iteration_no_increase < 100:\n",
    "        val = target_f(x)\n",
    "        if min_val - val > toler:\n",
    "            min_val = val\n",
    "            alpha = alpha_0\n",
    "            iteration_no_increase = 0\n",
    "        else:\n",
    "            alpha *= 0.95\n",
    "            iteration_no_increase += 1\n",
    "        trace.append(val)\n",
    "        gradient_i = gradient_f(x, target_f)\n",
    "        x = x - (alpha *  gradient_i)\n",
    "        steps += 1\n",
    "    return x, val, steps, trace\n",
    "\n",
    "x = np.array([2,-2])\n",
    "x, val, steps, trace = SGD(target_f, gradient_f, x)\n",
    "print(x)\n",
    "print('Val: {:.6f}, steps: {:.0f}'.format(val, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_M(target_f, \n",
    "        gradient_f, \n",
    "        x, \n",
    "        alpha_0=0.01,\n",
    "        toler = 0.000001,\n",
    "        m = 0.9):\n",
    "    \n",
    "    alpha = alpha_0\n",
    "    min_val = float('inf')\n",
    "    steps = 0\n",
    "    iteration_no_increase = 0\n",
    "    v = 0.0\n",
    "    trace = []\n",
    "    while iteration_no_increase < 100:\n",
    "        val = target_f(x)\n",
    "        if min_val - val > toler:\n",
    "            min_val = val\n",
    "            alpha = alpha_0\n",
    "            iteration_no_increase = 0\n",
    "        else:\n",
    "            alpha *= 0.95\n",
    "            iteration_no_increase += 1\n",
    "        trace.append(val)\n",
    "        gradient_i = gradient_f(x, target_f)\n",
    "        v = m * v + (alpha *  gradient_i)\n",
    "        x = x - v\n",
    "        steps += 1\n",
    "    return x, val, steps, trace\n",
    "\n",
    "x = np.array([2,-2])\n",
    "x, val, steps, trace2 = SGD_M(target_f, gradient_f, x)\n",
    "\n",
    "print('\\n',x)\n",
    "print('Val: {:.6f}, steps: {:.0f}'.format(val, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array(range(len(trace)))\n",
    "x3 = np.array(range(len(trace2)))\n",
    "plt.xlim([0,len(trace)])\n",
    "plt.gcf().set_size_inches((10,3))\n",
    "plt.semilogy(x3, trace2, '--')\n",
    "plt.semilogy(x2, trace, '-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (dl_win)",
   "language": "python",
   "name": "dl_win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
